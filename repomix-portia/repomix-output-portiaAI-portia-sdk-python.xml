This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where comments have been removed, empty lines have been removed, content has been compressed (code blocks are separated by ⋮---- delimiter), security check has been disabled.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Code comments have been removed from supported file types
- Empty lines have been removed from all files
- Content has been compressed - code blocks are separated by ⋮---- delimiter
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.cursor/
  rules/
    sdk-context.mdc
  context.md
.github/
  workflows/
    create-release-pr.yml
    create-release-tag.yml
    daily-tests.yml
    deploy_pypi.yml
    docs.yml
    evals.yml
    licence.yml
    pyright.yml
    ready_to_eval_labeller.yml
    release.yml
    ruff.yml
    security.yml
    testing.yml
    trigger-downstream-updates.yml
  pull_request_template.md
portia/
  builder/
    __init__.py
    conditionals.py
    plan_builder_v2.py
    plan_v2.py
    reference.py
    step_v2.py
  execution_agents/
    utils/
      __init__.py
      final_output_summarizer.py
      step_summarizer.py
    __init__.py
    base_execution_agent.py
    clarification_tool.py
    conditional_evaluation_agent.py
    context.py
    default_execution_agent.py
    execution_utils.py
    memory_extraction.py
    one_shot_agent.py
    output.py
  introspection_agents/
    __init__.py
    default_introspection_agent.py
    introspection_agent.py
  open_source_tools/
    __init__.py
    browser_tool.py
    calculator_tool.py
    crawl_tool.py
    extract_tool.py
    image_understanding_tool.py
    llm_tool.py
    local_file_reader_tool.py
    local_file_writer_tool.py
    map_tool.py
    pdf_reader_tool.py
    registry.py
    search_tool.py
    weather.py
  planning_agents/
    __init__.py
    base_planning_agent.py
    context.py
    default_planning_agent.py
  telemetry/
    __init__.py
    telemetry_service.py
    views.py
  templates/
    __init__.py
    default_planning_agent.xml.jinja
    example_plans.py
    render.py
    tool_description.xml.jinja
  __init__.py
  clarification_handler.py
  clarification.py
  cli_clarification_handler.py
  cli.py
  cloud.py
  common.py
  config.py
  end_user.py
  errors.py
  execution_hooks.py
  gemini_langsmith_wrapper.py
  logger.py
  mcp_session.py
  model.py
  plan_run.py
  plan.py
  portia.py
  prefixed_uuid.py
  storage.py
  token_check.py
  tool_call.py
  tool_decorator.py
  tool_registry.py
  tool_wrapper.py
  tool.py
  version.py
tests/
  integration/
    data/
      war_and_peace_ch1.txt
    open_source_tools/
      test_browser_tool.py
    mcp_server.py
    test_e2e.py
    test_execution_agent.py
    test_mcp_session.py
    test_model.py
    test_plan_v2.py
    test_portia_cloud.py
    test_runner_context.py
    test_summariser.py
  unit/
    builder/
      test_plan_builder_v2.py
      test_plan_v2.py
      test_reference.py
      test_step_v2.py
    execution_agents/
      utils/
        test_final_output_summarizer.py
        test_step_summarizer.py
      test_base_execution_agent.py
      test_clarification_tool.py
      test_context.py
      test_default_execution_agent.py
      test_execution_utils.py
      test_memory_extraction.py
      test_oneshot_agent.py
      test_output.py
    introspection_agents/
      __init__.py
      test_default_introspection_agent.py
    open_source_tools/
      conftest.py
      test_browser_tool.py
      test_calculator_tool.py
      test_crawl_tool.py
      test_extract_tool.py
      test_file_reader_tool.py
      test_file_writer_tool.py
      test_image_understanding_tool.py
      test_llm_tool.py
      test_map_tool.py
      test_pdf_reader_tool.py
      test_search_tool.py
      test_weather_tool.py
    planning_agents/
      test_default_planning_agent.py
    telemetry/
      test_telemetry_service.py
    conftest.py
    test_clarification_handler.py
    test_clarifications.py
    test_cli_clarification_handler.py
    test_cli.py
    test_common.py
    test_config.py
    test_end_user.py
    test_execution_hooks.py
    test_gemini_langsmith_wrapper.py
    test_logging.py
    test_model.py
    test_plan_builder.py
    test_plan_run.py
    test_plan_storage.py
    test_plan.py
    test_portia_async.py
    test_portia.py
    test_storage_async.py
    test_storage.py
    test_token_check.py
    test_tool_async.py
    test_tool_call.py
    test_tool_decorator.py
    test_tool_registry.py
    test_tool_wrapper.py
    test_tool.py
    test_version.py
  conftest.py
  utils.py
.env.example
.gitignore
.pre-commit-config.yaml
CODE_OF_CONDUCT.md
CONTRIBUTING.md
example_builder.py
example.py
LICENSE
pyproject.toml
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".cursor/rules/sdk-context.mdc">
---
description: 
globs: 
alwaysApply: true
---
This is the portia-sdk-python repository, a python package for managing agentic LLM workflows, with access to planning and execution of workflows. This includes tool calling with in built tools, and ability to create your own via classes and decorators, including support for MCP tools. The project uses uv to manage python version and dependencies. Here are some common commands for uv, and should be preferenced over pip or calling python directly.
- `uv sync`: updates python packages in lock file
- `uv run <command>`: runs a command, like `python`, `pytest`
- `uv add <package>`: adds a package to the pyproject.toml, lock file and also installs it to the virtual environment

A number of the functions and classes have async versions, which are typically copies of the sync versions, normally noted with an `a` prefix, example `portia.plan` -> `portia.aplan`, When updating any existing functions, make sure to update the async version as well if they exist.

This project uses ruff to lint, pyright to do static typechecking and pytest for tests. Tests are split between unit and integration under the tests/unit and tests/integration folders. Don't call pytest directly, always call via the seperate folders. 

Whenever making changes, make sure to run, in order
- `uv run ruff format`
- `uv run ruff check --fix`
- `uv run pyright`
- `uv run pytest tests/unit -n auto`
- `uv run pytest tests/integration -n auto`
and then resolve any issues.
</file>

<file path=".cursor/context.md">
# Portia SDK Python

## Project Overview
This is an open-source framework for creating reliable and production-ready multi-agent systems.

## Core Architecture
Portia implements a three-agent architecture to ensure robust and reliable execution:

1. **Planning Agent**: Creates a comprehensive plan for how to achieve a given task, breaking it down into executable steps.
2. **Execution Agent**: Executes individual steps of the plan, focusing on the implementation details and concrete actions required.
3. **Introspection Agent**: Operates between execution steps to check which step is needed next.

The Portia interface has both synchronous and asynchronous methods, typically denoted with an `a` prefix, eg portia.plan() -> portia.aplan(). These should be kept in line with each other, so if you make changes to one, you should make similar changes to the other. 

## Developing

You can run linting in the codebase by running the following commands, but only do this if asked:
* uv run pyright
* uv run ruff check --fix
* uv run pytest {command} -n auto
All python commands should be run with `uv run` and python should never be called directly for anything. 

If this doesn't work, you may need to install uv with pip install uv, making sure to run `uv sync --all-extras --all-groups` after to ensure all dependencies are installed.
</file>

<file path=".github/workflows/create-release-pr.yml">
name: Create Release PR
on:
  workflow_dispatch:
    inputs:
      release_type:
        description: 'Release type (alpha/release)'
        required: true
        type: choice
        options:
          - alpha
          - release
      bump_type:
        description: 'Version bump type (v{major}.{minor}.{patch})'
        required: true
        type: choice
        options:
          - patch
          - minor
          - major
        default: 'patch'
jobs:
  create-release-pr:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Install uv
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
      - name: Get current version
        id: current_version
        run: |
          CURRENT_VERSION=$(uv version --short)
          echo "current_version=$CURRENT_VERSION" >> $GITHUB_OUTPUT
      - name: Bump version
        id: bump_version
        run: |
          BASE_VERSION=$(echo "${{ steps.current_version.outputs.current_version }}" | sed 's/a[0-9]\+$//')
          case "${{ inputs.bump_type }}" in
            patch)
              NEW_VERSION=$(echo $BASE_VERSION | awk -F. '{$NF = $NF + 1;} 1' | sed 's/ /./g')
              ;;
            minor)
              NEW_VERSION=$(echo $BASE_VERSION | awk -F. '{$(NF-1) = $(NF-1) + 1; $NF = 0;} 1' | sed 's/ /./g')
              ;;
            major)
              NEW_VERSION=$(echo $BASE_VERSION | awk -F. '{$1 = $1 + 1; $(NF-1) = 0; $NF = 0;} 1' | sed 's/ /./g')
              ;;
          esac
          if [ "${{ inputs.release_type }}" = "alpha" ]; then
            NEW_VERSION="${NEW_VERSION}a0"
          fi
          echo "new_version=$NEW_VERSION" >> $GITHUB_OUTPUT
      - name: Create release branch and update version
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email '41898282+github-actions[bot]@users.noreply.github.com'
          git checkout -b release/v${{ steps.bump_version.outputs.new_version }}
          uv version ${{ steps.bump_version.outputs.new_version }}
          git add pyproject.toml
          git commit -m "Bump version to v${{ steps.bump_version.outputs.new_version }}"
          git push origin release/v${{ steps.bump_version.outputs.new_version }}
      - name: Create Pull Request
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.DEPLOY_PAT_TOKEN }}
          script: |
            const { data: pr } = await github.rest.pulls.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `Release v${{ steps.bump_version.outputs.new_version }}`,
              body: `Automated PR for version bump to v${{ steps.bump_version.outputs.new_version }}`,
              head: `release/v${{ steps.bump_version.outputs.new_version }}`,
              base: 'main'
            });
            await github.rest.issues.addLabels({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: pr.number,
              labels: ['release']
            });
</file>

<file path=".github/workflows/create-release-tag.yml">
name: Create Release Tag
on:
  pull_request:
    types: [closed]
    branches:
      - main
jobs:
  create-tag:
    if: github.event.pull_request.merged == true && startsWith(github.event.pull_request.head.ref, 'release/v')
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          fetch-tags: true
      - name: Install uv
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
      - name: Get version from pyproject.toml
        id: get_version
        run: |
          VERSION=$(uv version --short)
          echo "version=$VERSION" >> $GITHUB_OUTPUT
      - name: Create tag
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.DEPLOY_PAT_TOKEN }}
          script: |
            const version = 'v${{ steps.get_version.outputs.version }}';
            const sha = context.sha;
            try {
              await github.rest.git.createRef({
                owner: context.repo.owner,
                repo: context.repo.repo,
                ref: `refs/tags/${version}`,
                sha: sha
              });
              console.log(`Successfully created tag ${version}`);
            } catch (error) {
              core.setFailed(`Failed to create tag: ${error.message}`);
            }
      - name: Create latest prod release tag tag
        if: ${{ !contains(steps.get_version.outputs.version, 'a') }}
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.DEPLOY_PAT_TOKEN }}
          script: |
            const sha = context.sha;
            try {
              await github.rest.git.updateRef({
                owner: context.repo.owner,
                repo: context.repo.repo,
                ref: `tags/latest-prod-release`,
                sha: sha,
                force: true
              });
              console.log(`Successfully created tag latest-prod-release`);
            } catch (error) {
              core.setFailed(`Failed to create tag: ${error.message}`);
            }
</file>

<file path=".github/workflows/daily-tests.yml">
name: Daily Integration Tests
on:
  schedule:
    - cron: '0 9 * * *'
  workflow_dispatch:
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true
permissions:
  contents: read
jobs:
  daily-tests:
    timeout-minutes: 20
    runs-on: ubuntu-latest-16core
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: install uv
        run: pipx install uv
      - name: install dependencies
        run: uv sync --all-extras
      - name: Run daily expensive tests
        env:
          PORTIA_API_ENDPOINT: "https://api.porita.dev"
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          MISTRAL_API_KEY: ${{ secrets.MISTRAL_API_KEY }}
          PORTIA_API_KEY: ${{ secrets.PORTIA_API_KEY }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          BROWSERBASE_API_KEY: ${{ secrets.BROWSERBASE_API_KEY }}
          BROWSERBASE_PROJECT_ID: ${{ secrets.BROWSERBASE_PROJECT_ID }}
          TAVILY_API_KEY: ${{ secrets.TAVILY_API_KEY }}
        run: |
          set -o pipefail
          uv run pytest -m daily --log-cli-level=WARNING --junitxml=pytest-daily.xml | tee pytest-daily.txt
      - name: Integration test failure alert
        if: failure() && github.ref == 'refs/heads/main'
        uses: slackapi/slack-github-action@v2.1.0
        env:
          SLACK_BOT_TOKEN: ${{ secrets.SLACK_BOT_TOKEN }}
        with:
          method: chat.postMessage
          token: ${{ secrets.SLACK_BOT_TOKEN }}
          payload: |
            channel: C07V8NK09RC
            text: "Browser tool integration tests failed!"
</file>

<file path=".github/workflows/deploy_pypi.yml">
name: Release to PyPI
on:
  push:
    tags:
      - 'v[0-9]+.[0-9]+.[0-9]+'
      - 'v[0-9]+.[0-9]+.[0-9]+a[0-9]+'
permissions:
  contents: read
  actions: write
jobs:
  release:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.ref }}
      - name: Install uv
        run: curl -LsSf https://astral.sh/uv/install.sh | sh
      - name: Extract version from tag
        id: get_tag_version
        run: |
          TAG_VERSION=${GITHUB_REF
          echo "tag_version=$TAG_VERSION" >> $GITHUB_OUTPUT
      - name: Get version from pyproject.toml
        id: get_project_version
        run: |
          PROJECT_VERSION=$(uv version --short)
          echo "project_version=$PROJECT_VERSION" >> $GITHUB_OUTPUT
      - name: Verify versions match
        id: verify_versions
        run: |
          if [ "${{ steps.get_tag_version.outputs.tag_version }}" != "${{ steps.get_project_version.outputs.project_version }}" ]; then
            echo "Tag version (${{ steps.get_tag_version.outputs.tag_version }}) does not match project version (${{ steps.get_project_version.outputs.project_version }})"
            exit 1
          fi
      - name: Build and publish to PyPI
        id: pypi_publish
        run: |
          uv build
          uv publish --token ${{ secrets.POETRY_PYPI_TOKEN_PYPI }}
      - name: Notify Slack on success
        if: success()
        uses: slackapi/slack-github-action@v1.24.0
        with:
          channel-id: '${{ vars.SLACK_DEV_CHANNEL }}'
          slack-message: "✅ Successfully published SDK version ${{ steps.get_tag_version.outputs.tag_version }} to PyPI! 🎉"
        env:
          SLACK_BOT_TOKEN: ${{ secrets.SLACK_BOT_TOKEN }}
      - name: Notify Slack on failure
        if: failure()
        uses: slackapi/slack-github-action@v1.24.0
        with:
          channel-id: '${{ vars.SLACK_RUN_CHANNEL }}'
          slack-message: "❌ Failed to publish version ${{ steps.get_tag_version.outputs.tag_version }} to PyPI.\nSee: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
        env:
          SLACK_BOT_TOKEN: ${{ secrets.SLACK_BOT_TOKEN }}
</file>

<file path=".github/workflows/docs.yml">
name: Update Docs
on:
  push:
    branches:
      - main
jobs:
  update-docs:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout SDK repo
        uses: actions/checkout@v4
      - name: Generate documentation
        run: |
          pip install pydoc-markdown
          pydoc-markdown
      - name: Checkout Docs repo
        uses: actions/checkout@v4
        with:
          repository: portiaAI/docs
          token: ${{ secrets.DOCS_REPO_PAT }}
          path: docs-repo
      - name: Copy generated docs
        run: |
          rm -rf docs-repo/docs/SDK
          mv docs/SDK docs-repo/docs/SDK
      - name: Commit and push changes
        run: |
          cd docs-repo
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add .
          git commit -m "Update docs from SDK [skip ci]" || echo "No changes to commit"
          git push origin main
</file>

<file path=".github/workflows/evals.yml">
name: Run Evals for portia-sdk-python
on:
  workflow_dispatch:
  push:
    branches:
      - main
  pull_request_target:
    branches:
      - main
    types:
      - synchronize
      - labeled
permissions:
  contents: read
  pull-requests: read
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}-${{ github.event.pull_request.number }}
  cancel-in-progress: true
jobs:
  evals:
    runs-on: ubuntu-latest
    if: >
      github.event_name != 'pull_request' ||
      contains(github.event.pull_request.labels.*.name, 'ready_to_eval')
    steps:
      - name: Checkout portia-sdk-python repo
        uses: actions/checkout@v4
        with:
          path: portia-sdk-python
          repository: portiaAI/portia-sdk-python
          ref: ${{ github.event_name == 'pull_request_target' && github.event.pull_request.head.ref || github.ref_name }}
          token: ${{ secrets.PORTIA_GH_TOKEN }}
      - name: Checkout platform repo
        uses: actions/checkout@v4
        with:
          path: platform
          repository: portiaAI/platform
          token: ${{ secrets.PORTIA_GH_TOKEN }}
      - name: Install UV
        run: pip install uv
      - name: Install dependencies to SteelThread
        working-directory: ./platform/steel_thread
        run: |
          uv add "portia-sdk-python[all] @ file://../../portia-sdk-python"
      - name: Install dependencies to Evals
        working-directory: ./platform/evals
        run: |
          uv add "portia-sdk-python[all] @ file://../../portia-sdk-python"
      - name: Eval query planner
        id: eval_query_planner
        working-directory: ./platform/evals
        env:
          LANGCHAIN_TRACING_V2: "true"
          LANGCHAIN_ENDPOINT: "https://api.smith.langchain.com"
          LANGCHAIN_API_KEY: ${{ secrets.LANGCHAIN_API_KEY }}
          LANGCHAIN_PROJECT: ${{ vars.LANGCHAIN_PROJECT }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          PORTIA_API_KEY: ${{ secrets.PORTIA_EVAL_API_KEY }}
          PORTIA_API_ENDPOINT: ${{ secrets.PORTIA_EVAL_API_ENDPOINT }}
          LLM_REDIS_CACHE_URL: ${{ secrets.LLM_REDIS_CACHE_URL }}
        run: |
          EVAL_OUTPUT=$(uv run cli.py query-planner eval --provider=google --threshold_file=query_planner/thresholds/gemini-2.5-pro/thresholds_local.yaml --reps 1 --metadata "pr=${{ github.event.pull_request.number }},author=${{ github.event.pull_request.user.login || github.actor }},run=${{ github.event_name == 'push' && 'main' || 'pr' }},env=local,repo=sdk" --slice_name main  --max_concurrency 25)
          echo "eval_url=$(echo "$EVAL_OUTPUT" | grep -o '${LANGCHAIN_ENDPOINT}/.*')" >> $GITHUB_OUTPUT
          echo "eval_name=$(echo "$EVAL_OUTPUT" | grep -oP "experiment:\s*'\K[^']+")" >> $GITHUB_OUTPUT
          if echo "$EVAL_OUTPUT" | grep -q "EVAL BREACH"; then
            BREACHES=$(echo "$EVAL_OUTPUT" | grep "EVAL BREACH:" | tr '\n' ' ' | sed 's/"/\\"/g')
            echo "metric_breaches=${BREACHES}" >> $GITHUB_OUTPUT
            echo "has_failing_eval_planner_scores=true" >> $GITHUB_OUTPUT
          fi
      - name: Eval agent (execution)
        id: eval_execution_agent
        working-directory: ./platform/evals
        env:
          LANGCHAIN_TRACING_V2: "true"
          LANGCHAIN_ENDPOINT: "https://api.smith.langchain.com"
          LANGCHAIN_API_KEY: ${{ secrets.LANGCHAIN_API_KEY }}
          LANGCHAIN_PROJECT: ${{ vars.LANGCHAIN_PROJECT }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          PORTIA_API_KEY: ${{ secrets.PORTIA_EVAL_API_KEY }}
          PORTIA_API_ENDPOINT: ${{ secrets.PORTIA_EVAL_API_ENDPOINT }}
          LLM_REDIS_CACHE_URL: ${{ secrets.LLM_REDIS_CACHE_URL }}
        run: |
          EVAL_OUTPUT=$(uv run cli.py agent eval --slice_name=main --provider=google --threshold_file=execution/thresholds/gemini-2.5-flash/thresholds_local.yaml --reps 1 --metadata "pr=${{ github.event.pull_request.number }},author=${{ github.event.pull_request.user.login || github.actor }},run=${{ github.event_name == 'push' && 'main' || 'pr' }},env=local,repo=sdk"  --max_concurrency 25)
          echo $EVAL_OUTPUT
          echo "eval_url=$(echo "$EVAL_OUTPUT" | grep -o 'https://smith.langchain.com/.*')" >> $GITHUB_OUTPUT
          echo "eval_name=$(echo "$EVAL_OUTPUT" | grep -oP "experiment:\s*'\K[^']+")" >> $GITHUB_OUTPUT
          echo "eval_name: $EVAL_OUTPUT"
          if echo "$EVAL_OUTPUT" | grep -q "EVAL BREACH"; then
            BREACHES=$(echo "$EVAL_OUTPUT" | grep "EVAL BREACH:" | tr '\n' ' ' | sed 's/"/\\"/g')
            echo "BREACHES: $BREACHES"
            echo "metric_breaches=${BREACHES}" >> $GITHUB_OUTPUT
            echo "has_failing_eval_agent_scores=true" >> $GITHUB_OUTPUT
          fi
      - name: Eval agent (introspection)
        id: eval_agent_introspection
        working-directory: ./platform/evals
        env:
          LANGCHAIN_TRACING_V2: "true"
          LANGCHAIN_ENDPOINT: "https://api.smith.langchain.com"
          LANGCHAIN_API_KEY: ${{ secrets.LANGCHAIN_API_KEY }}
          LANGCHAIN_PROJECT: ${{ vars.LANGCHAIN_PROJECT }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          PORTIA_API_KEY: ${{ secrets.PORTIA_EVAL_API_KEY }}
          PORTIA_API_ENDPOINT: ${{ secrets.PORTIA_EVAL_API_ENDPOINT }}
          LLM_REDIS_CACHE_URL: ${{ secrets.LLM_REDIS_CACHE_URL }}
        run: |
          EVAL_OUTPUT=$(uv run cli.py introspection eval --slice_name=main --provider=google --threshold_file=introspection/thresholds/gemini-2.5-flash/thresholds_local.yaml --reps 1 --metadata "pr=${{ github.event.pull_request.number }},author=${{ github.event.pull_request.user.login || github.actor }},run=${{ github.event_name == 'push' && 'main' || 'pr' }},env=local,repo=sdk"  --max_concurrency 25)
          echo "eval_url=$(echo "$EVAL_OUTPUT" | grep -o 'https://smith.langchain.com/.*')" >> $GITHUB_OUTPUT
          echo "eval_name=$(echo "$EVAL_OUTPUT" | grep -oP "experiment:\s*'\K[^']+")" >> $GITHUB_OUTPUT
          if echo "$EVAL_OUTPUT" | grep -q "EVAL BREACH"; then
            BREACHES=$(echo "$EVAL_OUTPUT" | grep "EVAL BREACH:" | tr '\n' ' ' | sed 's/"/\\"/g')
            echo "BREACHES: $BREACHES"
            echo "metric_breaches=${BREACHES}" >> $GITHUB_OUTPUT
            echo "has_failing_eval_introspection_scores=true" >> $GITHUB_OUTPUT
          fi
      - name: Summary results
        id: summary_results
        working-directory: ./platform/evals
        env:
          LANGCHAIN_TRACING_V2: "true"
          LANGCHAIN_ENDPOINT: "https://api.smith.langchain.com"
          LANGCHAIN_API_KEY: ${{ secrets.LANGCHAIN_API_KEY }}
          LANGCHAIN_PROJECT: ${{ vars.LANGCHAIN_PROJECT }}
          PORTIA_API_KEY: ${{ secrets.PORTIA_EVAL_API_KEY }}
          PORTIA_API_ENDPOINT: ${{ secrets.PORTIA_EVAL_API_ENDPOINT }}
          EXECUTION_AGENT_EXPERIMENT_ID: ${{ steps.eval_execution_agent.outputs.eval_name }}
          QUERY_PLANNER_EXPERIMENT_ID: ${{ steps.eval_query_planner.outputs.eval_name }}
          INTROSPECTION_AGENT_EXPERIMENT_ID: ${{ steps.eval_agent_introspection.outputs.eval_name }}
        run: |
          echo "EXECUTION_AGENT_EXPERIMENT_ID: $EXECUTION_AGENT_EXPERIMENT_ID"
          echo "QUERY_PLANNER_EXPERIMENT_ID: $QUERY_PLANNER_EXPERIMENT_ID"
          echo "INTROSPECTION_AGENT_EXPERIMENT_ID: $INTROSPECTION_AGENT_EXPERIMENT_ID"
          uv run jupyter nbconvert --to markdown --execute analysis/github_analysis.ipynb --output notebook_output.md --no-input
          sed -i '/<style[^>]*>/,/<\/style>/d' analysis/notebook_output.md
          cat analysis/notebook_output.md >> $GITHUB_STEP_SUMMARY
      - name: Check for evaluation failures
        run: |
          CONTAINS_THRESHOLD_BREACH=false
          if [[ "${{ steps.eval_query_planner.outputs.has_failing_eval_planner_scores }}" == "true" ]]; then
            echo "Query planner eval failed or has breaches."
            echo "Breaches: ${{ steps.eval_query_planner.outputs.metric_breaches }}"
            CONTAINS_THRESHOLD_BREACH=true
          fi
          if [[ "${{ steps.eval_execution_agent.outputs.has_failing_eval_agent_scores }}" == "true" ]]; then
            echo "Execution agent failed or has breaches."
            echo "Breaches: ${{ steps.eval_execution_agent.outputs.metric_breaches }}"
            CONTAINS_THRESHOLD_BREACH=true
          fi
          if [[ "${{ steps.eval_agent_introspection.outputs.has_failing_eval_introspection_scores }}" == "true" ]]; then
            echo "Introspection agent failed or has breaches."
            echo "Breaches: ${{ steps.eval_agent_introspection.outputs.metric_breaches }}"
            CONTAINS_THRESHOLD_BREACH=true
          fi
          if [[ "$CONTAINS_THRESHOLD_BREACH" == "true" ]]; then
            echo "One or more evaluations failed."
            exit 1
          fi
      - name: Setup Redis CLI
        if: failure()
        uses: shogo82148/actions-setup-redis@v1
      - name: Clear LLM cache
        if: failure()
        working-directory: ./platform
        run: |
          echo "LLM_REDIS_CACHE_URL=${{ secrets.LLM_REDIS_CACHE_URL }}" > .env
          ./scripts/clear_llm_cache.sh
</file>

<file path=".github/workflows/licence.yml">
name: License Check
on:
  push:
    branches: [main]
  pull_request:
jobs:
  license-check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Install uv
        run: pipx install uv
      - name: Install dependencies
        run: |
          uv sync
          uv pip install licensecheck pipdeptree
      - name: Check licenses
        run: |
          echo "Checking for disallowed licenses..."
          echo "::group::License Report"
          uv run python -m licensecheck --zero | tee license_output.txt
          EXIT_CODE=${PIPESTATUS[0]}
          if [ $EXIT_CODE -ne 0 ]; then
            echo "❌ The following packages have incompatible licenses:"
            INCOMPATIBLE=$(cat license_output.txt | grep "^│ ✖" | sed 's/│ ✖/❌/g')
            echo "$INCOMPATIBLE"
            if [ ! -z "$INCOMPATIBLE" ]; then
              echo -e "\n📦 Reverse dependency information:"
              echo "$INCOMPATIBLE" | while read -r line; do
                PKG=$(echo "$line" | awk '{print $3}')
                echo -e "\nDependency tree for $PKG:"
                uv run pipdeptree --reverse --packages "$PKG"
              done
            fi
            echo "::endgroup::"
            echo "::error::License check failed! Please ensure all dependencies use permissive licenses (MIT, Apache-2.0, BSD, ISC)."
            exit 1
          fi
          echo "::endgroup::"
          echo "✅ All dependency licenses are compliant!"
</file>

<file path=".github/workflows/pyright.yml">
name: Run Pyright
on:
  pull_request:
    branches:
      - "*"
jobs:
  pyright:
    name: Static Type Checking with Pyright
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: "16"
      - name: Install Pyright
        run: npm install -g pyright
      - name: Install uv
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.local/bin" >> $GITHUB_PATH
      - name: Install dependencies
        run: uv sync --all-extras
      - name: Run Pyright
        run: uv run pyright
</file>

<file path=".github/workflows/ready_to_eval_labeller.yml">
name: Ready to eval Labeller
on:
  pull_request:
    types: [review_requested]
jobs:
  add-label:
    runs-on: ubuntu-latest
    steps:
      - name: Add 'ready_to_eval' label
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.DEPLOY_PAT_TOKEN }}
          script: |
            github.rest.issues.addLabels({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.payload.pull_request.number,
              labels: ['ready_to_eval']
            })
</file>

<file path=".github/workflows/release.yml">
name: Version Tag Creation
on:
  workflow_dispatch:
    inputs:
      branch:
        description: 'Branch to create tag from'
        required: true
        default: 'production'
        type: choice
        options:
          - main
          - production
permissions:
  contents: write
  actions: write
jobs:
  create-tag:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          ref: ${{ github.event.inputs.branch }}
      - name: Install uv
        run: curl -LsSf https://astral.sh/uv/install.sh | sh
      - name: Fetch tags
        run: git fetch --tags
      - name: Get version from pyproject.toml
        id: get_version
        run: |
          VERSION=$(uv version --short)
          echo "Version: $VERSION"
          echo "VERSION=$VERSION" >> $GITHUB_ENV
      - name: Check if version is already tagged
        run: |
          if git rev-parse "$VERSION" >/dev/null 2>&1; then
            echo "Tag $VERSION already exists. Skipping tag creation."
            echo "tag_exists=true" >> $GITHUB_ENV
          else
            echo "Tag $VERSION does not exist. Creating new tag."
          fi
      - name: Create Git tag
        if: env.tag_exists != 'true'
        run: |
          git tag $VERSION
      - name: Push changes
        if: env.tag_exists != 'true'
        run: |
          git push origin $VERSION
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Publish to PyPI
        if: env.tag_exists != 'true'
        run: |
          export POETRY_PYPI_TOKEN_PYPI=${{secrets.POETRY_PYPI_TOKEN_PYPI}}
          uv build
          uv publish --token ${{ secrets.POETRY_PYPI_TOKEN_PYPI }}
</file>

<file path=".github/workflows/ruff.yml">
name: Formatting (ruff)
on: [pull_request]
permissions:
  contents: read
  pull-requests: read
jobs:
  ruff:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: astral-sh/ruff-action@v1
        with:
          args: check
          version: "0.8.6"
</file>

<file path=".github/workflows/security.yml">
name: Python Testing
on:
  pull_request:
    types:
      - opened
      - synchronize
      - labeled
permissions:
  contents: read
  pull-requests: read
jobs:
  security:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Install Bandit
        run: pip install bandit
      - name: Run Bandit
        run: |
          bandit -r . -x tests --severity-level medium -o bandit-report.txt
      - name: Upload Bandit Report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: bandit-report
          path: bandit-report.txt
</file>

<file path=".github/workflows/testing.yml">
name: Python Testing
on:
  pull_request_target:
    types:
      - opened
      - synchronize
      - labeled
concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number }}
  cancel-in-progress: true
permissions:
  contents: read
  pull-requests: read
jobs:
  testing:
    timeout-minutes: 20
    runs-on: ubuntu-latest-16core
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha }}
      - name: install uv
        run: pipx install uv
      - uses: pydantic/ollama-action@v3
        with:
          model: qwen2.5:0.5b
      - name: install dependencies
        run: uv sync --all-extras
      - name: unit_tests + coverage
        env:
          PORTIA_API_ENDPOINT: "https://api.porita.dev"
        run: |
          set -o pipefail
          uv run pytest tests/unit -n auto --cov
      - name: integration_tests + coverage
        env:
          PORTIA_API_ENDPOINT: "https://api.porita.dev"
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          MISTRAL_API_KEY: ${{ secrets.MISTRAL_API_KEY }}
          PORTIA_API_KEY: ${{ secrets.PORTIA_API_KEY }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          BROWSERBASE_API_KEY: ${{ secrets.BROWSERBASE_API_KEY }}
          BROWSERBASE_PROJECT_ID: ${{ secrets.BROWSERBASE_PROJECT_ID }}
          TAVILY_API_KEY: ${{ secrets.TAVILY_API_KEY }}
        run: |
          set -o pipefail
          uv run pytest tests/integration -m "not daily" -n auto --cov --cov-append --cov-fail-under 100 --log-cli-level=WARNING --junitxml=pytest.xml | tee pytest-coverage.txt
</file>

<file path=".github/workflows/trigger-downstream-updates.yml">
name: Trigger Downstream Updates
on:
  workflow_dispatch:
  push:
    branches:
      - main
  pull_request:
    types: [closed]
    branches:
      - main
jobs:
  trigger-docs-update:
    if: github.event_name == 'push' || (github.event_name == 'pull_request' && github.event.pull_request.merged == true) || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    steps:
      - name: Trigger Docs Repository Dispatch
        uses: peter-evans/repository-dispatch@v3
        with:
          token: ${{ secrets.DEPLOY_PAT_TOKEN }}
          repository: portiaAI/docs
          event-type: bump-sdk-version
      - name: Trigger Portia Platform Repository Dispatch
        uses: peter-evans/repository-dispatch@v3
        with:
          token: ${{ secrets.DEPLOY_PAT_TOKEN }}
          repository: portiaAI/platform
          event-type: bump-sdk-version
</file>

<file path=".github/pull_request_template.md">
# Description

Please include a summary of the change. Please also include relevant motivation and context. List any dependencies that are required for this change.

Ticket Link: N/A 

## Type of change

(select all that apply)

- [ ] Bug fix 
- [ ] New feature 
- [ ] Breaking change 
- [ ] Refactor
- [ ] Requires sync with platform release
- [ ] Documentation update

## Screenshots

(If applicable, add screenshots to help explain your changes)

## Changelog

(If applicable, add a changelog [entry](https://keepachangelog.com/en/))
</file>

<file path="portia/builder/__init__.py">

</file>

<file path="portia/builder/conditionals.py">
class ConditionalBlock(BaseModel)
⋮----
clause_step_indexes: list[int] = Field(default_factory=list)
parent_conditional_block: "ConditionalBlock | None" = Field(
class ConditionalBlockClauseType(StrEnum)
⋮----
NEW_CONDITIONAL_BLOCK = "NEW_CONDITIONAL_BLOCK"
ALTERNATE_CLAUSE = "ALTERNATE_CLAUSE"
END_CONDITION_BLOCK = "END_CONDITION_BLOCK"
class ConditionalStepResult(BaseModel)
⋮----
type: ConditionalBlockClauseType
conditional_result: bool
next_clause_step_index: int
end_condition_block_step_index: int
</file>

<file path="portia/builder/plan_builder_v2.py">
class PlanBuilderError(ValueError)
class PlanBuilderV2
⋮----
def __init__(self, label: str = "Run the plan built with the Plan Builder") -> None
⋮----
@property
    def _current_conditional_block(self) -> ConditionalBlock | None
⋮----
parent_block = self._current_conditional_block
conditional_block = ConditionalBlock(
⋮----
def else_(self) -> PlanBuilderV2
def endif(self) -> PlanBuilderV2
⋮----
def build(self) -> PlanV2
</file>

<file path="portia/builder/plan_v2.py">
class PlanV2(BaseModel)
⋮----
id: PlanUUID = Field(default_factory=PlanUUID, description="The ID of the plan.")
steps: list[StepV2] = Field(description="The steps to be executed in the plan.")
plan_inputs: list[PlanInput] = Field(
summarize: bool = Field(default=False, description="Whether to summarize the plan output.")
final_output_schema: type[BaseModel] | None = Field(
label: str = Field(
⋮----
@model_validator(mode="after")
    def validate_plan(self) -> PlanV2
⋮----
step_names = [step.step_name for step in self.steps]
⋮----
duplicates = [name for name in step_names if step_names.count(name) > 1]
unique_duplicates = list(set(duplicates))
⋮----
input_names = [plan_input.name for plan_input in self.plan_inputs]
⋮----
duplicates = [name for name in input_names if input_names.count(name) > 1]
⋮----
def to_legacy_plan(self, plan_context: PlanContext) -> Plan
def step_output_name(self, step: int | str | StepV2) -> str
⋮----
step_num = self.steps.index(step)
⋮----
step_num = self.idx_by_name(step)
⋮----
step_num = step
⋮----
def idx_by_name(self, name: str) -> int
</file>

<file path="portia/builder/reference.py">
def default_step_name(step_index: int) -> str
class Reference(BaseModel, ABC)
⋮----
model_config = ConfigDict(extra="allow")
⋮----
@abstractmethod
    def get_legacy_name(self, plan: PlanV2) -> str
⋮----
@abstractmethod
    def get_value(self, run_data: RunContext) -> ReferenceValue | None
class StepOutput(Reference)
⋮----
step: str | int = Field(
def __init__(self, step: str | int) -> None
⋮----
@override
    def get_legacy_name(self, plan: PlanV2) -> str
def __str__(self) -> str
⋮----
@override
    def get_value(self, run_data: RunContext) -> ReferenceValue | None
⋮----
step_index = run_data.plan.idx_by_name(self.step)
val = run_data.step_output_values[step_index]
⋮----
class Input(Reference)
⋮----
name: str = Field(description="The name of the input.")
def __init__(self, name: str) -> None
⋮----
plan_input = next(
⋮----
value = run_data.plan_run.plan_run_inputs.get(self.name)
⋮----
class ReferenceValue(BaseModel)
⋮----
value: Output = Field(description="The referenced value.")
description: str = Field(description="Description of the referenced value.", default="")
</file>

<file path="portia/builder/step_v2.py">
class StepV2(BaseModel, ABC)
⋮----
model_config = ConfigDict(arbitrary_types_allowed=True)
step_name: str = Field(description="The name of the step.")
conditional_block: ConditionalBlock | None = Field(
⋮----
@abstractmethod
    async def run(self, run_data: RunContext) -> Any
⋮----
@abstractmethod
    def describe(self) -> str
⋮----
@abstractmethod
    def to_legacy_step(self, plan: PlanV2) -> Step
⋮----
matches = re.findall(r"\{\{\s*(StepOutput|Input)\s*\(\s*([\w\s]+)\s*\)\s*\}\}", _input)
# If there are matches, replace each {{ StepOutput(var_name) }}
# or {{ Input(var_name) }} with its resolved value.
⋮----
result = _input
⋮----
var_name = var_name.strip()  # noqa: PLW2901
⋮----
var_name = int(var_name)  # noqa: PLW2901
ref = StepOutput(var_name) if ref_type == "StepOutput" else Input(var_name)  # type: ignore reportArgumentType
resolved = self._resolve_input_reference(ref, run_data)
resolved_val = (
pattern = (
result = re.sub(pattern, str(resolved_val), result, count=1)
⋮----
def _get_value_for_input(self, _input: Any, run_data: RunContext) -> Any | None:  # noqa: ANN401
⋮----
resolved_input = self._resolve_input_reference(_input, run_data)
⋮----
_input: Any,  # noqa: ANN401
⋮----
) -> Any | ReferenceValue | None:  # noqa: ANN401
⋮----
name = _input.get_legacy_name(plan)
# Ensure name starts with a $ so that it is clear it is a reference
# This is done so it appears nicely in the UI
⋮----
name = f"${name}"
⋮----
def _inputs_to_legacy_plan_variables(self, inputs: list[Any], plan: PlanV2) -> list[Variable]
def _get_legacy_condition(self, plan: PlanV2) -> str | None
⋮----
step_names = [s.step_name for s in plan.steps]
current_step_index = step_names.index(self.step_name)
def get_conditional_for_nested_block(block: ConditionalBlock) -> str | None
⋮----
active_clause_step_index = next(
⋮----
# First clause step index where the current step index is greater
# than the clause step index e.g. for clause step indexes [1, 8, 12]
# and current step index 2, the active clause step index is 1
⋮----
# The step is the `if_` or the `endif` step, so no new condition is needed
# as this will always be evaluated at this 'depth' of the plan branching.
⋮----
# All previous clause conditions must be false for this step to get run
previous_clause_step_indexes = itertools.takewhile(
condition_str = " and ".join(
⋮----
# The step is a non-conditional step within a block, so we need to make the
# active clause condition was true.
condition_str = f"{plan.step_output_name(active_clause_step_index)} is true" + (
⋮----
legacy_condition_strings = []
current_block = self.conditional_block
⋮----
legacy_condition_string = get_conditional_for_nested_block(current_block)
⋮----
current_block = current_block.parent_conditional_block
⋮----
class LLMStep(StepV2)
⋮----
task: str = Field(description="The task to perform.")
inputs: list[Any | Reference] = Field(
output_schema: type[BaseModel] | None = Field(
⋮----
@override
    def describe(self) -> str
⋮----
output_info = f" -> {self.output_schema.__name__}" if self.output_schema else ""
⋮----
@override
@traceable(name="LLM Step - Run")
    async def run(self, run_data: RunContext) -> str | BaseModel:  # pyright: ignore[reportIncompatibleMethodOverride] - needed due to Langsmith decorator
⋮----
llm_tool = LLMTool(structured_output_schema=self.output_schema)
tool_ctx = ToolRunContext(
task_data = [
⋮----
def _format_value(self, _input: Any, run_data: RunContext) -> Any | None:  # noqa: ANN401
⋮----
@override
    def to_legacy_step(self, plan: PlanV2) -> Step
class InvokeToolStep(StepV2)
⋮----
tool: str | Tool = Field(
args: dict[str, Any | Reference] = Field(
⋮----
def _tool_name(self) -> str
⋮----
@override
@traceable(name="Invoke Tool Step - Run")
    async def run(self, run_data: RunContext) -> Any:  # pyright: ignore[reportIncompatibleMethodOverride] - needed due to Langsmith decorator
⋮----
tool = run_data.portia.get_tool(self.tool, run_data.plan_run)
⋮----
tool = self.tool
⋮----
args = {k: self._get_value_for_input(v, run_data) for k, v in self.args.items()}
# TODO(RH): Move to async tool run when we can  # noqa: FIX002, TD003
output = tool.run(tool_ctx, **args)
⋮----
model = run_data.portia.config.get_default_model()
output = await model.aget_structured_response(
⋮----
inputs_desc = ", ".join(
⋮----
class FunctionStep(StepV2)
⋮----
function: Callable[..., Any] = Field(description=("The function to call."))
⋮----
fn_name = getattr(self.function, "__name__", str(self.function))
⋮----
@override
@traceable(name="Function Step - Run")
    async def run(self, run_data: RunContext) -> Any:  # pyright: ignore[reportIncompatibleMethodOverride] - needed due to Langsmith decorator
⋮----
output = self.function(**args)
⋮----
class SingleToolAgentStep(StepV2)
⋮----
tool: str = Field(description="The tool to use.")
⋮----
@override
@traceable(name="Single Tool Agent Step - Run")
    async def run(self, run_data: RunContext) -> None:  # pyright: ignore[reportIncompatibleMethodOverride] - needed due to Langsmith decorator
⋮----
agent = run_data.portia.get_agent_for_step(
output_obj = await agent.execute_async()
⋮----
class ConditionalStep(StepV2)
⋮----
condition: Callable[..., bool] | str = Field(
args: dict[str, Reference | Any] = Field(
clause_index_in_block: int = Field(description="The index of the clause in the condition block")
block_clause_type: ConditionalBlockClauseType
⋮----
@field_validator("conditional_block", mode="after")
@classmethod
    def validate_conditional_block(cls, v: ConditionalBlock | None) -> ConditionalBlock
⋮----
@property
    def block(self) -> ConditionalBlock
⋮----
@override
@traceable(name="Conditional Step - Run")
    async def run(self, run_data: RunContext) -> Any
⋮----
condition_str = self._get_value_for_input(self.condition, run_data)
agent = ConditionalEvaluationAgent(run_data.portia.config)
conditional_result = await agent.execute(condition_str, args)
⋮----
conditional_result = self.condition(**args)
next_clause_step_index = (
⋮----
cond_str = self.condition
⋮----
cond_str = (
</file>

<file path="portia/execution_agents/utils/__init__.py">

</file>

<file path="portia/execution_agents/utils/final_output_summarizer.py">
class FinalOutputSummarizer
⋮----
summarizer_only_prompt = (
summarizer_and_structured_output_prompt = (
def __init__(self, config: Config, agent_memory: AgentMemory) -> None
def _build_tasks_and_outputs_context(self, plan: Plan, plan_run: PlanRun) -> str
⋮----
context = []
⋮----
outputs = plan_run.outputs.step_outputs
⋮----
output_value = self.get_output_value(outputs[step.output])
⋮----
def get_output_value(self, output: Output) -> str | None
⋮----
value = output.full_value(self.agent_memory)
⋮----
def create_summary(self, plan: Plan, plan_run: PlanRun) -> str | BaseModel | None
⋮----
model = self.config.get_summarizer_model()
context = self._build_tasks_and_outputs_context(plan, plan_run)
⋮----
class SchemaWithSummary(plan_run.structured_output_schema)
⋮----
fo_summary: str = Field(description="The summary of the plan output")
⋮----
response = model.get_response(
</file>

<file path="portia/execution_agents/utils/step_summarizer.py">
class SummarizerOutputModel(BaseModel)
⋮----
so_summary: str
class StepSummarizer
⋮----
summarizer_prompt = ChatPromptTemplate.from_messages(
⋮----
def invoke(self, state: MessagesState) -> dict[str, Any]
⋮----
messages = state["messages"]
last_message = messages[-1] if len(messages) > 0 else None
⋮----
result = self.model.get_structured_response(
⋮----
coerced_output = structured_output_schema.model_validate(result.model_dump())
⋮----
response: Message = self.model.get_response(
summary = response.content
⋮----
def _parse_tool_output(self, messages: list[AnyMessage]) -> str
⋮----
tool_messages = {msg.tool_call_id: msg for msg in messages if isinstance(msg, ToolMessage)}
last_ai_message_with_tool_calls = next(
tool_outputs = []
⋮----
tool_output_message = tool_messages[tool_call["id"]]
output = (
⋮----
tool_output = "\nOUTPUT_SEPARATOR\n".join(tool_outputs)
⋮----
tool_output = (
⋮----
def _parse_messages(self, tool_output: str) -> list[Message]
def _setup_summarizer(self, messages: list[AnyMessage]) -> tuple[str, list[Message]]
⋮----
last_message = messages[-1]
⋮----
tool_output = self._parse_tool_output(messages)
parsed_messages = self._parse_messages(tool_output)
⋮----
schema = self.step.structured_output_schema or self.tool.structured_output_schema
⋮----
class SummarizerOutput(SummarizerOutputModel, schema)
⋮----
async def ainvoke(self, state: MessagesState) -> dict[str, Any]
⋮----
result = await self.model.aget_structured_response(
⋮----
response: Message = await self.model.aget_response(
</file>

<file path="portia/execution_agents/__init__.py">

</file>

<file path="portia/execution_agents/base_execution_agent.py">
class BaseExecutionAgent
⋮----
@property
    def step(self) -> Step
⋮----
@abstractmethod
    def execute_sync(self) -> Output
async def execute_async(self) -> Output
def get_system_context(self, ctx: ToolRunContext, step_inputs: list[StepInput]) -> str
⋮----
messages = state["messages"]
last_message = messages[-1]
errors = [msg for msg in messages if is_soft_tool_error(msg)]
⋮----
clarification = self.execution_hooks.after_tool_call(
⋮----
structured_output_schema = self.step.structured_output_schema or (
⋮----
# If the value is larger than the threshold value, always summarise them as they are
# too big to store the full value locally
⋮----
# If the tool has a structured output schema attached and hasn't already been
</file>

<file path="portia/execution_agents/clarification_tool.py">
class ClarificationToolSchema(BaseModel)
⋮----
argument_name: str = Field(
class ClarificationTool(Tool[str])
⋮----
id: str = "clarification_tool"
name: str = "Clarification tool"
description: str = (
args_schema: type[BaseModel] = ClarificationToolSchema
output_schema: tuple[str, str] = ("str", "Model dump of the clarification to raise")
step: int
def run(self, ctx: ToolRunContext, argument_name: str) -> str
</file>

<file path="portia/execution_agents/conditional_evaluation_agent.py">
class BooleanResponse(BaseModel)
⋮----
explanation: str = Field(description="Explanation for the response value")
response: bool = Field(description="Whether the conditional statement is True")
class ConditionalEvaluationAgent
⋮----
def __init__(self, config: Config) -> None
⋮----
@traceable(name="Conditional Evaluation Agent - Execute")
    async def execute(self, conditional: str, arguments: dict[str, Any]) -> bool
⋮----
model = self.config.get_introspection_model()
rendered_args = "\n".join(
resp = await model.aget_structured_response(
</file>

<file path="portia/execution_agents/context.py">
def generate_main_system_context() -> list[str]
class StepInput(BaseModel)
⋮----
name: str
value: Serializable | None
description: str
⋮----
input_context = ["Inputs: the original inputs provided by the planning_agent"]
used_outputs = set()
⋮----
unused_output_keys = set(previous_outputs.keys()) - used_outputs
⋮----
output_val = (str(previous_outputs[output_key].get_value()) or "")[:10000]
⋮----
def generate_clarification_context(clarifications: ClarificationListType, step: int) -> list[str]
⋮----
clarification_context = []
# want to use the value provided, and clarifications for other steps which may be useful
# (e.g. consider a plan with 10 steps, each needing the same clarification, we don't want
current_step_clarifications = []
other_step_clarifications = []
⋮----
def generate_context_from_run_context(context: ToolRunContext) -> list[str]
⋮----
execution_context = ["Metadata: This section contains general context about this execution."]
⋮----
previous_outputs = plan_run.outputs.step_outputs
clarifications = plan_run.outputs.clarifications
system_context = generate_main_system_context()
⋮----
context = ["Additional context: You MUST use this information to complete your task."]
input_context = generate_input_context(step_inputs, previous_outputs)
⋮----
clarification_context = generate_clarification_context(
⋮----
execution_context = generate_context_from_run_context(ctx)
</file>

<file path="portia/execution_agents/default_execution_agent.py">
class ExecutionState(MessagesState)
⋮----
step_inputs: list[StepInput]
class ToolArgument(BaseModel)
⋮----
model_config = ConfigDict(arbitrary_types_allowed=True)
name: str = Field(description="Name of the argument, as requested by the tool.")
explanation: str = Field(
value: Any | None = Field(
valid: bool = Field(
class ToolInputs(BaseModel)
⋮----
args: list[ToolArgument] = Field(description="Arguments for the tool.")
class VerifiedToolArgument(BaseModel)
⋮----
# produce invalid JSON.
made_up: bool = Field(
explanation: str | None = Field(
schema_invalid: bool = Field(
class VerifiedToolInputs(BaseModel)
⋮----
args: list[VerifiedToolArgument] = Field(description="Arguments for the tool.")
class ParserModel
⋮----
"Do not assume a default value that meets the type expectation or is a common testing value. "  # noqa: E501
⋮----
"class ToolArgument:\n"
"  name: str
"  explanation: str
"For large arguments, include why you did or didn't use templating.\n"
"  value: Any | None  # Value of the argument from the goal or context.\n"
"  valid: bool  # Whether the value is valid for the argument.\n\n",
⋮----
def invoke(self, state: ExecutionState) -> dict[str, Any]
⋮----
formatted_messages = self.arg_parser_prompt.format_messages(
errors = []
tool_inputs: ToolInputs | None = None
⋮----
response = self.model.get_structured_response(
tool_inputs = ToolInputs.model_validate(response)
⋮----
test_args = {}
⋮----
# also test the ToolInputs that have come back
# actually work for the schema of the tool
# if not we can retry
⋮----
# Previously we would raise an error here, but this restricts the agent from
# being able to raise clarifications for the tool arguments marked as invalid.
# Missing tool arguments are often represented as None, which isn't a compatible
⋮----
class VerifierModel:
⋮----
messages = state["messages"]
tool_args = messages[-1].content
formatted_messages = self.arg_verifier_prompt.format_messages(
⋮----
response = VerifiedToolInputs.model_validate(response)
response = self._validate_args_against_schema(response, state["step_inputs"])
⋮----
arg_dict = {arg.name: arg.value for arg in tool_inputs.args}
⋮----
# Extract the arg names from the pydantic error to mark them as schema_invalid = True.
# At this point we know the arguments are invalid, so we can trigger a clarification
# request.
invalid_arg_names = set()
⋮----
# Gemini often returns lists as '[1,2,3]', but downstream LLMs can handle this.
⋮----
# Mark any made up arguments that are None and optional as not made up.
# We don't need to raise a clarification for these
⋮----
class ToolCallingModel
⋮----
tool_calling_prompt = ChatPromptTemplate.from_messages(
⋮----
verified_args = self.agent.verified_args
⋮----
matching_clarification = self.agent.get_last_resolved_clarification(arg.name)
⋮----
model = self.model.to_langchain().bind_tools(self.tools)
⋮----
past_errors = [msg for msg in messages if "ToolSoftError" in msg.content]
⋮----
response = model.invoke(
result = template_in_required_inputs(response, state["step_inputs"])
⋮----
clarification = self.agent.execution_hooks.before_tool_call(
⋮----
class DefaultExecutionAgent(BaseExecutionAgent)
⋮----
last_message = messages[-1]
arguments = VerifiedToolInputs.model_validate_json(str(last_message.content))
⋮----
matching_clarification = self.get_last_resolved_clarification(arg.name)
⋮----
matching_clarification = None
⋮----
matching_clarification = clarification
⋮----
def execute_sync(self) -> Output
⋮----
tool_run_ctx = ToolRunContext(
model = self.config.get_execution_model()
tools = [
tool_node = ToolNode(tools)
graph = StateGraph(ExecutionState)
⋮----
app = graph.compile()
invocation_result = app.invoke({"messages": [], "step_inputs": []})
</file>

<file path="portia/execution_agents/execution_utils.py">
class AgentNode(str, Enum)
⋮----
TOOL_AGENT = "tool_agent"
SUMMARIZER = "summarizer"
TOOLS = "tools"
ARGUMENT_VERIFIER = "argument_verifier"
ARGUMENT_PARSER = "argument_parser"
MEMORY_EXTRACTION = "memory_extraction"
MAX_RETRIES = 4
def is_clarification(artifact: Any) -> bool
⋮----
messages = state["messages"]
⋮----
def get_arg_value_with_templating(step_inputs: list[StepInput], arg: Any) -> Any
def _template_inputs_into_arg_value(arg_value: str, step_inputs: list[StepInput]) -> str
⋮----
template_args = {}
⋮----
input_name = step_input.name
# jinja can't handle inputs that start with $, so remove any leading $
input_name = input_name.lstrip("$")
arg_value = arg_value.replace(step_input.name, input_name)
⋮----
untemplated_var_matches = re.findall(r"\{\{(\$[^\}]*)\}\}", arg_value)
⋮----
extra_vars = ", ".join(list(untemplated_var_matches))
⋮----
output_values: list[Output] = []
tool_soft_error = None
tool_hard_error = None
⋮----
tool_soft_error = str(message.content)
⋮----
tool_hard_error = str(message.content)
⋮----
clarification = InputClarification.model_validate_json(message.content)
⋮----
output = output_values[0]
⋮----
# If there is a structured output schema, then it is stored in the last tool call's value
final_value = output_values[-1].get_value()
⋮----
values = []
summaries = []
⋮----
output_value = output.get_value()
⋮----
final_summary = output_values[-1].get_summary() or ", ".join(summaries)
⋮----
def is_soft_tool_error(message: BaseMessage) -> bool
</file>

<file path="portia/execution_agents/memory_extraction.py">
class MemoryExtractionStep
⋮----
def invoke(self, _: dict[str, Any]) -> dict[str, Any]
⋮----
potential_inputs = self.agent.plan_run.get_potential_step_inputs()
step_inputs = [
⋮----
expected_inputs = {input_.name for input_ in self.agent.step.inputs}
known_inputs = {input_.name for input_ in step_inputs}
⋮----
def _truncate_inputs(self, inputs: list[StepInput]) -> None
</file>

<file path="portia/execution_agents/one_shot_agent.py">
class ExecutionState(MessagesState)
⋮----
step_inputs: list[StepInput]
class OneShotToolCallingModel
⋮----
arg_parser_prompt = ChatPromptTemplate.from_messages(
⋮----
def invoke(self, state: ExecutionState) -> dict[str, Any]
⋮----
response = model.invoke(formatted_messages)
result = template_in_required_inputs(response, state["step_inputs"])
⋮----
model = self.model.to_langchain().bind_tools(self.tools)
messages = state["messages"]
past_errors = [str(msg) for msg in messages if is_soft_tool_error(msg)]
clarification_tool = ClarificationTool(step=self.agent.plan_run.current_step_index)
formatted_messages = self.arg_parser_prompt.format_messages(
⋮----
def _handle_execution_hooks(self, response: BaseMessage) -> dict[str, list] | None
⋮----
clarification = self.agent.execution_hooks.before_tool_call(
⋮----
async def ainvoke(self, state: ExecutionState) -> dict[str, Any]
⋮----
response = await model.ainvoke(formatted_messages)
⋮----
class OneShotAgent(BaseExecutionAgent)
⋮----
def execute_sync(self) -> Output
⋮----
app = self._setup_graph(sync=True).compile()
invocation_result = app.invoke({"messages": [], "step_inputs": []})
⋮----
async def execute_async(self) -> Output
⋮----
app = self._setup_graph(sync=False).compile()
invocation_result = await app.ainvoke({"messages": [], "step_inputs": []})
⋮----
def _setup_graph(self, sync: bool) -> StateGraph
⋮----
tool_run_ctx = ToolRunContext(
model = self.config.get_execution_model()
tools = [
clarification_tool = ClarificationTool(step=self.plan_run.current_step_index)
⋮----
tool_node = ToolNode(tools)
graph = StateGraph(ExecutionState)
</file>

<file path="portia/execution_agents/output.py">
class BaseOutput(BaseModel)
⋮----
@abstractmethod
    def get_value(self) -> Serializable | None
⋮----
@abstractmethod
    def serialize_value(self) -> str
⋮----
@abstractmethod
    def full_value(self, agent_memory: AgentMemory) -> Serializable | None
⋮----
@abstractmethod
    def get_summary(self) -> str | None
class LocalDataValue(BaseOutput)
⋮----
model_config = ConfigDict(extra="forbid")
value: Serializable | None = Field(
summary: str | None = Field(
def get_value(self) -> Serializable | None
def serialize_value(self) -> str
def full_value(self, agent_memory: AgentMemory) -> Serializable | None
def get_summary(self) -> str | None
⋮----
@field_serializer("value")
    def serialize_value_field(self, value: Serializable | None) -> str
class AgentMemoryValue(BaseOutput)
⋮----
output_name: str
plan_run_id: PlanRunUUID
summary: str = Field(
⋮----
def get_summary(self) -> str
Output = LocalDataValue | AgentMemoryValue
⋮----
class LocalOutput(LocalDataValue)
⋮----
class AgentMemoryOutput(AgentMemoryValue)
</file>

<file path="portia/introspection_agents/__init__.py">

</file>

<file path="portia/introspection_agents/default_introspection_agent.py">
class DefaultIntrospectionAgent(BaseIntrospectionAgent)
⋮----
def __init__(self, config: Config, agent_memory: AgentMemory) -> None
⋮----
introspection_condition = plan.steps[plan_run.current_step_index].condition
memory_outputs = [
⋮----
def _get_plan_steps_pretty(self, plan: Plan) -> str
</file>

<file path="portia/introspection_agents/introspection_agent.py">
SKIPPED_OUTPUT = "Tool execution skipped"
COMPLETED_OUTPUT = "Tool execution skipped and completed the plan run"
class PreStepIntrospectionOutcome(PortiaEnum)
⋮----
CONTINUE = "CONTINUE"
SKIP = "SKIP"
COMPLETE = "COMPLETE"
class PreStepIntrospection(BaseModel)
⋮----
outcome: PreStepIntrospectionOutcome = Field(
reason: str = Field(
class BaseIntrospectionAgent(ABC)
⋮----
def __init__(self, config: Config, agent_memory: AgentMemory) -> None
</file>

<file path="portia/open_source_tools/__init__.py">

</file>

<file path="portia/open_source_tools/browser_tool.py">
NotSet: Any = PydanticUndefined
BROWSERBASE_AVAILABLE = validate_extras_dependencies("tools-browser-browserbase", raise_error=False)
T = TypeVar("T", bound=str | BaseModel)
class BrowserToolForUrlSchema(BaseModel)
⋮----
task: str = Field(
task_data: list[Any] | str | None = Field(
class BrowserToolSchema(BaseModel)
⋮----
url: str = Field(
⋮----
class BrowserTaskOutput(BaseModel, Generic[T])
⋮----
task_output: T | None = Field(
human_login_required: bool = Field(
login_url: str | None = Field(
user_login_guidance: str | None = Field(
class BrowserInfrastructureOption(Enum)
⋮----
LOCAL = "local"
REMOTE = "remote"
class BrowserTool(Tool[str | BaseModel])
⋮----
model_config = ConfigDict(arbitrary_types_allowed=True)
id: str = Field(init_var=True, default="browser_tool")
name: str = Field(init_var=True, default="Browser Tool")
description: str = Field(
args_schema: type[BaseModel] = Field(init_var=True, default=BrowserToolSchema)
output_schema: tuple[str, str] = ("str", "The Browser tool's response to the user query.")
model: GenerativeModel | None | str = Field(
infrastructure_option: BrowserInfrastructureOption = Field(
custom_infrastructure_provider: BrowserInfrastructureProvider | None = Field(default=None)
structured_output_schema: type[BaseModel] | None = Field(
⋮----
@cached_property
    def infrastructure_provider(self) -> BrowserInfrastructureProvider
⋮----
@staticmethod
    def process_task_data(task_data: list[Any] | str | None) -> str
⋮----
model = ctx.config.get_generative_model(self.model) or ctx.config.get_default_model()
llm = model.to_langchain()
async def run_browser_tasks() -> str | BaseModel | ActionClarification
⋮----
output_schema = self.structured_output_schema if self.structured_output_schema else str
output_model = BrowserTaskOutput[output_schema]
⋮----
agent = Agent(
result = await agent.run()
⋮----
task_to_complete = (
task_result = await run_agent_task(task_to_complete, output_model)
⋮----
loop = asyncio.get_event_loop()
⋮----
loop = asyncio.new_event_loop()
⋮----
class BrowserToolForUrl(BrowserTool)
⋮----
domain_parts = str(HttpUrl(url).host).split(".")
formatted_domain = "_".join(domain_parts)
⋮----
id = f"browser_tool_for_url_{formatted_domain}"
⋮----
name = f"Browser Tool for {formatted_domain}"
⋮----
description = (
⋮----
class BrowserInfrastructureProvider(ABC)
⋮----
@abstractmethod
    def setup_browser(self, ctx: ToolRunContext) -> Browser
⋮----
@abstractmethod
    def construct_auth_clarification_url(self, ctx: ToolRunContext, sign_in_url: str) -> HttpUrl
⋮----
@abstractmethod
    def step_complete(self, ctx: ToolRunContext) -> None
class BrowserInfrastructureProviderLocal(BrowserInfrastructureProvider)
⋮----
def setup_browser(self, ctx: ToolRunContext) -> Browser
⋮----
def get_chrome_instance_path(self) -> str
⋮----
chrome_path_from_env = os.environ.get("PORTIA_BROWSER_LOCAL_CHROME_EXEC")
⋮----
def step_complete(self, ctx: ToolRunContext) -> None
def get_extra_chromium_args(self) -> list[str] | None
⋮----
extra_chromium_args_from_env = os.environ.get("PORTIA_BROWSER_LOCAL_EXTRA_CHROMIUM_ARGS")
⋮----
class BrowserInfrastructureProviderBrowserBase(BrowserInfrastructureProvider)
⋮----
def __init__(self, api_key: str | None = None, project_id: str | None = None) -> None
⋮----
api_key = api_key or os.environ.get("BROWSERBASE_API_KEY")
⋮----
def step_complete(self, ctx: ToolRunContext) -> None
⋮----
session_id = ctx.end_user.get_additional_data("bb_session_id")
⋮----
def get_context_id(self, ctx: ToolRunContext, bb: Browserbase) -> str
⋮----
fresh_context = bb.contexts.create(project_id=self.project_id)
⋮----
def create_session(self, bb_context_id: str) -> SessionCreateResponse
def get_or_create_session(self, context: ToolRunContext, bb: Browserbase) -> str
⋮----
context_id = context.end_user.get_additional_data(
⋮----
session_id = context.end_user.get_additional_data("bb_session_id")
session_connect_url = context.end_user.get_additional_data("bb_session_connect_url")
⋮----
session = self.create_session(context_id)
session_connect_url = session.connect_url
session_id = session.id
⋮----
live_view_link = self.bb.sessions.debug(session_id)
⋮----
def setup_browser(self, ctx: ToolRunContext) -> Browser
⋮----
session_connect_url = self.get_or_create_session(ctx, self.bb)
⋮----
def _is_first_browser_tool_call(self, plan_run: PlanRun, plan: Plan) -> bool
</file>

<file path="portia/open_source_tools/calculator_tool.py">
allowed_operators = {
def safe_eval(node: Any) -> Any
⋮----
left = safe_eval(node.left)
right = safe_eval(node.right)
op_type = type(node.op)
⋮----
operand = safe_eval(node.operand)
⋮----
def safe_evaluate(expression: str) -> float
⋮----
parsed = ast.parse(expression.strip(), mode="eval")
result = safe_eval(parsed)
⋮----
class CalculatorToolSchema(BaseModel)
⋮----
math_question: str = Field(
class CalculatorTool(Tool[float])
⋮----
id: str = "calculator_tool"
name: str = "Calculator Tool"
description: str = (
args_schema: type[BaseModel] = CalculatorToolSchema
output_schema: tuple[str, str] = ("str", "A string dump of the computed result")
def run(self, _: ToolRunContext, math_question: str) -> float
⋮----
expression = self.math_expression(math_question)
⋮----
def math_expression(self, prompt: str) -> str
⋮----
prompt = prompt.lower()
prompt = prompt.replace("added to", "+").replace("plus", "+").replace("and", "+")
prompt = prompt.replace("minus", "-")
prompt = prompt.replace("times", "*")
prompt = prompt.replace("what is ", "").replace("?", "")
prompt = prompt.replace("x", "*")
⋮----
parts = prompt.split("subtracted from")
⋮----
prompt = parts[1].strip() + " - " + parts[0].strip()
⋮----
match = re.search(r"subtract\s+(.+)\s+from\s+(.+)", prompt)
⋮----
prompt = f"{match.group(2)} - {match.group(1)}"
⋮----
prompt = prompt.replace("subtract", "-")
⋮----
parts = prompt.split("divided by")
⋮----
prompt = parts[0].strip() + " / " + parts[1].strip()
⋮----
match = re.search(r"divide\s+(.+)\s+by\s+(.+)", prompt)
⋮----
prompt = f"{match.group(1)} / {match.group(2)}"
⋮----
prompt = prompt.replace("divide", "/")
⋮----
match = re.search(r"multiply\s+(.+)\s+by\s+(.+)", prompt)
⋮----
prompt = f"{match.group(1)} * {match.group(2)}"
⋮----
parts = prompt.split("multiplied by")
⋮----
prompt = parts[0].strip() + " * " + parts[1].strip()
⋮----
prompt = prompt.replace("multiply", "*")
</file>

<file path="portia/open_source_tools/crawl_tool.py">
DEFAULT_MAX_DEPTH = 1
DEFAULT_MAX_BREADTH = 20
DEFAULT_LIMIT = 50
class CrawlToolSchema(BaseModel)
⋮----
url: str = Field(
instructions: str | None = Field(
max_depth: int = Field(
max_breadth: int = Field(
limit: int = Field(
select_paths: list[str] | None = Field(
select_domains: list[str] | None = Field(
exclude_paths: list[str] | None = Field(
exclude_domains: list[str] | None = Field(
allow_external: bool = Field(
class CrawlTool(Tool[str])
⋮----
id: str = "crawl_tool"
name: str = "Crawl Tool"
description: str = (
args_schema: type[BaseModel] = CrawlToolSchema
output_schema: tuple[str, str] = ("str", "str: crawled content and discovered pages")
⋮----
api_key = os.getenv("TAVILY_API_KEY")
⋮----
payload = self._build_payload(
⋮----
payload: dict[str, Any] = {"url": url}
⋮----
def _make_api_request(self, api_key: str, payload: dict[str, Any]) -> str
⋮----
api_url = "https://api.tavily.com/crawl"
headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
⋮----
response = httpx.post(api_url, headers=headers, json=payload, timeout=60.0)
⋮----
json_response = response.json()
⋮----
def _format_results(self, results: list[Any]) -> str
⋮----
formatted_results = []
⋮----
url_info = f"URL: {result.get('url', 'N/A')}"
content_preview = result.get("raw_content", "")
⋮----
def _raise_crawl_error(self, json_response: dict[str, Any]) -> NoReturn
def _handle_http_error(self, e: httpx.HTTPStatusError) -> NoReturn
⋮----
error_detail = f"HTTP {e.response.status_code}"
⋮----
error_body = e.response.json()
</file>

<file path="portia/open_source_tools/extract_tool.py">
class ExtractToolSchema(BaseModel)
⋮----
urls: list[str] = Field(..., description="List of URLs to extract content from")
include_images: bool = Field(
include_favicon: bool = Field(
extract_depth: str = Field(
format: str = Field(default="markdown", description="Output format: 'markdown' or 'text'")
class ExtractTool(Tool[str])
⋮----
id: str = "extract_tool"
name: str = "Extract Tool"
description: str = (
args_schema: type[BaseModel] = ExtractToolSchema
output_schema: tuple[str, str] = ("str", "str: extracted content from URLs")
⋮----
api_key = os.getenv("TAVILY_API_KEY")
⋮----
url = "https://api.tavily.com/extract"
payload = {
headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
response = httpx.post(url, headers=headers, json=payload, timeout=60.0)
⋮----
json_response = response.json()
</file>

<file path="portia/open_source_tools/image_understanding_tool.py">
class ImageUnderstandingToolSchema(BaseModel)
⋮----
task: str = Field(
image_url: str | None = Field(
image_file: str | None = Field(
⋮----
@model_validator(mode="after")
    def check_image_url_or_file(self) -> Self
⋮----
has_image_url = self.image_url is not None
has_image_file = self.image_file is not None
⋮----
class ImageUnderstandingTool(Tool[str])
⋮----
id: str = "image_understanding_tool"
name: str = "Image Understanding Tool"
description: str = (
args_schema: type[BaseModel] = ImageUnderstandingToolSchema
output_schema: tuple[str, str] = (
prompt: str = """
tool_context: str = ""
model: GenerativeModel | None | str = Field(
def run(self, ctx: ToolRunContext, **kwargs: Any) -> str
⋮----
model = ctx.config.get_generative_model(self.model) or ctx.config.get_default_model()
tool_schema = ImageUnderstandingToolSchema(**kwargs)
context = (
⋮----
content = (
⋮----
image_url = tool_schema.image_url
⋮----
image_data = base64.b64encode(image_file.read()).decode("utf-8")
mime_type = mimetypes.guess_type(tool_schema.image_file)[0]
image_url = f"data:{mime_type};base64,{image_data}"
⋮----
messages = [
response = model.to_langchain().invoke(messages)
</file>

<file path="portia/open_source_tools/llm_tool.py">
class LLMToolSchema(BaseModel)
⋮----
task: str = Field(
task_data: list[Any] | str | None = Field(
class LLMTool(Tool[str | BaseModel])
⋮----
LLM_TOOL_ID: ClassVar[str] = "llm_tool"
id: str = LLM_TOOL_ID
name: str = "LLM Tool"
description: str = (
args_schema: type[BaseModel] = LLMToolSchema
output_schema: tuple[str, str] = (
prompt: str = """
tool_context: str = ""
model: GenerativeModel | str | None = Field(
structured_output_schema: type[BaseModel] | None = Field(
⋮----
@staticmethod
    def process_task_data(task_data: list[Any] | str | None) -> str
⋮----
model = ctx.config.get_generative_model(self.model) or ctx.config.get_default_model()
messages = self._get_messages(task, task_data)
⋮----
response = model.get_response(messages)
⋮----
response = await model.aget_response(messages)
⋮----
def _get_messages(self, task: str, task_data: list[Any] | str | None = None) -> list[Message]
⋮----
context = (
task_data_str = self.process_task_data(task_data)
task_str = task
⋮----
content = task_str if not len(context.split("\n")) > 1 else f"{context}\n\n{task_str}"
</file>

<file path="portia/open_source_tools/local_file_reader_tool.py">
class FileReaderToolSchema(BaseModel)
⋮----
filename: str = Field(
class FileReaderTool(Tool[str])
⋮----
id: str = "file_reader_tool"
name: str = "File reader tool"
description: str = "Finds and reads content from a local file on Disk"
args_schema: type[BaseModel] = FileReaderToolSchema
output_schema: tuple[str, str] = ("str", "A string dump or JSON of the file content")
def run(self, ctx: ToolRunContext, filename: str) -> str | Clarification
⋮----
file_path = Path(filename)
suffix = file_path.suffix.lower()
⋮----
alt_file_paths = self.find_file(file_path)
⋮----
def find_file(self, file_path: Path) -> list[str]
⋮----
search_path = file_path.parent
filename = file_path.name
</file>

<file path="portia/open_source_tools/local_file_writer_tool.py">
class FileWriterToolSchema(BaseModel)
⋮----
filename: str = Field(
content: str = Field(
class FileWriterTool(Tool[str])
⋮----
id: str = "file_writer_tool"
name: str = "File writer tool"
description: str = "Writes content to a file locally"
args_schema: type[BaseModel] = FileWriterToolSchema
output_schema: tuple[str, str] = ("str", "A string indicating where the content was written to")
def run(self, _: ToolRunContext, filename: str, content: str) -> str
⋮----
filepath = Path(filename)
</file>

<file path="portia/open_source_tools/map_tool.py">
class MapToolSchema(BaseModel)
⋮----
url: str = Field(..., description="The root URL to begin the mapping (e.g., 'docs.tavily.com')")
max_depth: int = Field(
max_breadth: int = Field(
limit: int = Field(
instructions: str | None = Field(
select_paths: list[str] | None = Field(
select_domains: list[str] | None = Field(
exclude_paths: list[str] | None = Field(
exclude_domains: list[str] | None = Field(
allow_external: bool = Field(
categories: list[str] | None = Field(
class MapTool(Tool[str])
⋮----
id: str = "map_tool"
name: str = "Map Tool"
description: str = (
args_schema: type[BaseModel] = MapToolSchema
output_schema: tuple[str, str] = ("str", "str: list of discovered URLs on the website")
⋮----
api_key = os.getenv("TAVILY_API_KEY")
⋮----
payload = self._build_payload(
⋮----
payload = {
⋮----
optional_keys = [
⋮----
def _make_api_request(self, api_key: str, payload: dict) -> str
⋮----
api_url = "https://api.tavily.com/map"
headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
response = httpx.post(api_url, headers=headers, json=payload, timeout=60.0)
⋮----
json_response = response.json()
</file>

<file path="portia/open_source_tools/pdf_reader_tool.py">
class PDFReaderToolSchema(BaseModel)
⋮----
file_path: str = Field(
class PDFReaderTool(Tool[str])
⋮----
id: str = "pdf_reader_tool"
name: str = "PDF Reader Tool"
description: str = "Read a PDF file and extract its text content using Mistral OCR"
args_schema: type[BaseModel] = PDFReaderToolSchema
output_schema: tuple[str, str] = (
def run(self, _: ToolRunContext, file_path: str) -> str
⋮----
api_key = os.getenv("MISTRAL_API_KEY")
⋮----
client = Mistral(api_key=api_key)
⋮----
uploaded_pdf = client.files.upload(
signed_url = client.files.get_signed_url(file_id=uploaded_pdf.id)
ocr_response = client.ocr.process(
</file>

<file path="portia/open_source_tools/registry.py">
example_tool_registry = ToolRegistry(
open_source_tool_registry = ToolRegistry(
</file>

<file path="portia/open_source_tools/search_tool.py">
MAX_RESULTS = 3
class SearchToolSchema(BaseModel)
⋮----
search_query: str = Field(
class SearchTool(Tool[str])
⋮----
id: str = "search_tool"
name: str = "Search Tool"
description: str = (
args_schema: type[BaseModel] = SearchToolSchema
output_schema: tuple[str, str] = ("str", "str: output of the search results")
should_summarize: bool = True
api_url: str = "https://api.tavily.com/search"
def run(self, _: ToolRunContext, search_query: str) -> str
⋮----
response = httpx.post(self.api_url, headers=headers, json=payload)
⋮----
async def arun(self, _: ToolRunContext, search_query: str) -> str
⋮----
response = await client.post(self.api_url, headers=headers, json=payload)
⋮----
def _check_valid_api_key(self) -> str
⋮----
api_key = os.getenv("TAVILY_API_KEY")
⋮----
def _build_payload(self, search_query: str) -> dict
def _build_headers(self, api_key: str) -> dict
def _prep_request(self, search_query: str) -> tuple[dict, dict]
⋮----
api_key = self._check_valid_api_key()
payload = self._build_payload(search_query)
headers = self._build_headers(api_key)
⋮----
def _parse_response(self, response: httpx.Response) -> str
⋮----
json_response = response.json()
⋮----
results = json_response["results"]
</file>

<file path="portia/open_source_tools/weather.py">
class WeatherToolSchema(BaseModel)
⋮----
city: str = Field(..., description="The city to get the weather for")
class WeatherTool(Tool[str])
⋮----
id: str = "weather_tool"
name: str = "Weather Tool"
description: str = "Get the weather for a given city"
args_schema: type[BaseModel] = WeatherToolSchema
output_schema: tuple[str, str] = ("str", "String output of the weather with temp and city")
def run(self, _: ToolRunContext, city: str) -> str
⋮----
url = self._prep_run(city)
response = httpx.get(url)
⋮----
async def arun(self, _: ToolRunContext, city: str) -> str
⋮----
response = await client.get(url)
⋮----
def _prep_run(self, city: str) -> str
⋮----
api_key = os.getenv("OPENWEATHERMAP_API_KEY")
⋮----
def _parse_weather_data(self, response: httpx.Response, city: str) -> str
⋮----
data = response.json()
⋮----
weather = data["weather"][0]["description"]
⋮----
temp = data["main"]["temp"]
</file>

<file path="portia/planning_agents/__init__.py">
__all__ = ["BasePlanningAgent", "DefaultPlanningAgent"]
</file>

<file path="portia/planning_agents/base_planning_agent.py">
logger = logging.getLogger(__name__)
class BasePlanningAgent(ABC)
⋮----
def __init__(self, config: Config) -> None
⋮----
class StepsOrError(BaseModel)
⋮----
model_config = ConfigDict(extra="forbid")
steps: list[Step]
error: str | None = Field(
</file>

<file path="portia/planning_agents/context.py">
system_context = default_query_system_context()
non_default_examples_provided = True
⋮----
examples = DEFAULT_EXAMPLE_PLANS
non_default_examples_provided = False
tools_with_descriptions = get_tool_descriptions_for_tools(tool_list=tool_list)
plan_input_dicts = None
⋮----
plan_input_dicts = [
⋮----
def default_query_system_context() -> list[str]
def get_tool_descriptions_for_tools(tool_list: list[Tool]) -> list[dict[str, str]]
</file>

<file path="portia/planning_agents/default_planning_agent.py">
logger = logging.getLogger(__name__)
DEFAULT_PLANNING_PROMPT = """
class DefaultPlanningAgent(BasePlanningAgent)
⋮----
previous_errors = []
⋮----
prompt = render_prompt_insert_defaults(
response = self.model.get_structured_response(
steps_or_error = self._process_response(response, tool_list, plan_inputs, i)
⋮----
# Check for errors in the response
⋮----
# We don't retry LLM errors as we have no new useful information to provide
⋮----
tool_error = self._validate_tools_in_response(response.steps, tool_list)
⋮----
input_error = self._validate_inputs_in_response(response.steps, plan_inputs)
⋮----
# Add LLMTool to the steps that don't have a tool_id
⋮----
response = await self.model.aget_structured_response(
⋮----
def _validate_tools_in_response(self, steps: list[Step], tool_list: list[Tool]) -> str | None
⋮----
tool_ids = [tool.id for tool in tool_list]
missing_tools = [
⋮----
plan_inputs_names = {input_.name for input_ in (plan_inputs or [])}
step_outputs = set()
</file>

<file path="portia/telemetry/__init__.py">

</file>

<file path="portia/telemetry/telemetry_service.py">
logger = logging.getLogger(__name__)
def xdg_cache_home() -> Path
⋮----
default = Path.home() / ".portia"
env_var = os.getenv("XDG_CACHE_HOME")
⋮----
def get_project_id_key() -> str
⋮----
endpoint = os.getenv("PORTIA_API_ENDPOINT")
⋮----
class BaseProductTelemetry(ABC)
⋮----
@abstractmethod
    def capture(self, event: BaseTelemetryEvent) -> None
⋮----
@singleton
class ProductTelemetry(BaseProductTelemetry)
⋮----
USER_ID_PATH = str(xdg_cache_home() / "portia" / "telemetry_user_id")
PROJECT_API_KEY = get_project_id_key()
HOST = "https://eu.i.posthog.com"
UNKNOWN_USER_ID = "UNKNOWN"
_curr_user_id = None
def __init__(self) -> None
⋮----
telemetry_disabled = os.getenv("ANONYMIZED_TELEMETRY", "true").lower() == "false"
⋮----
posthog_logger = logging.getLogger("posthog")  # pragma: no cover
posthog_logger.disabled = True  # pragma: no cover
⋮----
def capture(self, event: BaseTelemetryEvent) -> None
⋮----
logger.debug(f"Telemetry event: {event.name} {event.properties}")  # noqa: G004
⋮----
logger.exception(f"Failed to send telemetry event {event.name}")  # noqa: G004
⋮----
@property
    def user_id(self) -> str
⋮----
# File access may fail due to permissions or other reasons. We don't want to
⋮----
new_user_id = str(uuid.uuid4())
</file>

<file path="portia/telemetry/views.py">
@dataclass
class BaseTelemetryEvent(ABC)
⋮----
@property
@abstractmethod
    def name(self) -> str
⋮----
@property
    def properties(self) -> dict[str, Any]
⋮----
@dataclass
class PortiaFunctionCallTelemetryEvent(BaseTelemetryEvent)
⋮----
function_name: str
function_call_details: dict[str, Any]
name: str = "portia_function_call"
⋮----
@dataclass
class ToolCallTelemetryEvent(BaseTelemetryEvent)
⋮----
tool_id: str | None
name: str = "tool_call"
</file>

<file path="portia/templates/__init__.py">
__all__ = ["render_template"]
</file>

<file path="portia/templates/default_planning_agent.xml.jinja">
Use the following information to construct a plan in response to the following user query: {{query}}

<Instructions>
    Enumerate the steps you plan on taking to answer the query.
    IMPORTANT: Ensure you use the tool id exactly as it is defined in <Tools> below (including whether it has a portia: prefix or not)
    Each step should:
     - include a description, plus any outputs from previous steps or plan inputs that will be required for this step should be specified as inputs for this step.
     - you need to ensure that all the information for carrying out the step will be provided in the combination of the task description and the inputs.
     - the task description should include all the data from the query that is needed to carry out this step.
     - DO NOT specify information in the 'task' field for a step if that is already being passed in by an input to the step
     - MAKE SURE EVERY step input is referencing variables from previous steps or the provided plan inputs. IT SHOULD NOT contain any other data and plan inputs should ONLY be used if they are provided.
     - IMPORTANT: ONLY USE plan inputs if they are provided - DO NOT make any up. YOU MUST RETURN ERROR if you do not have ALL required plan inputs to generate a plan.
     - Make sure all IDs and URLs from the query are included in the task description as they are, and do not calculate/assume any data yourself.
     - give a name to the variable for the output of the step if it is successful.
     - DO NOT mention the tool you will use in the task description, but MAKE SURE to use it in the tool_id field.
     - Additional information about the caller can be provided in the EndUser section. You can use this if the query refers to the end user, i.e. Send me an email.
     - IMPORTANT: MAKE SURE to not provide examples or assumptions in the task description.
     - IMPORTANT: Do not generate structure output schema field in the step yourself.
    IMPORTANT: If you can't come up with a plan provide a descriptive error instead - DO NOT create a plan with zero steps. When returning an error, return only the JSON object with the error message filled in plus an empty list of steps - DO NOT include text (explaining the error or otherwise) outside the JSON object.
    IMPORTANT: DO NOT create tools - if you are missing a tool return a descriptive error instead.
    IMPORTANT: Values for plan inputs will be provided when the plan is executed, so make sure to use them in your steps appropriately.
</Instructions>

{% if previous_errors %}
<PreviousErrors>{% for error in previous_errors %}
    <PreviousError>
        Encountered following error in previous run: {{error}}
    </PreviousError>{% endfor %}
</PreviousErrors>
{% endif %}

{% if system_context %}<SystemContext>{% for context in system_context %}
    {{context}}{% endfor %}
</SystemContext>{% endif %}

<Examples>{% for example in examples %}
    <Example>
        <Request>
            <Tools>
                {{example.plan_context.tool_ids | safe }}
            </Tools>
            <Query>
                {{example.plan_context.query}}
            </Query>
        </Request>
        <Response>
            [{% for step in example.steps %}
                {{step.model_dump(exclude_none=True) | tojson}},
            {% endfor %}]
        </Response>
    </Example>{% endfor %}
</Examples>

<Tools>{% for tool in tools %}
    <Tool id={{tool.id}}>
        {{tool.description | safe}}

        Tool arguments:
            {{tool.args | safe}}
        Output schema:
            {{tool.output_schema | safe}}
    </Tool>{% endfor %}
</Tools>
{% if end_user.email or end_user.name or end_user.phone_number or end_user.additional_data %}
<EndUser>
    {% if end_user.email %}<EndUser.Email>{{ end_user.email }}</EndUser.Email>{% endif %}
    {% if end_user.name %}<EndUser.Name>{{ end_user.name }}</EndUser.Name>{% endif %}
    {% if end_user.phone_number %}<EndUser.PhoneNumber>{{ end_user.phone_number }}</EndUser.PhoneNumber>{% endif %}
    {% if end_user.additional_data %}<EndUser.AdditionalData>{{ end_user.additional_data }}</EndUser.AdditionalData>{% endif %}
</EndUser>
{% endif %}{% if plan_inputs %}
<PlanInputs>{% for input in plan_inputs %}
    <PlanInput name="{{input.name}}">
        {{input.description}}
    </PlanInput>{% endfor %}
</PlanInputs>
{% endif %}
<Request>
    <Tools>
        {{tools | map(attribute='id') | list}}
    </Tools>
    <Query>
        {{query}}
    </Query>
</Request>
{% if non_default_examples_provided %}
Use the example plans as a scaffold to build this plan, following the same structure and steps. 
You must use the example plans as a template if at all applicable and they are similar to the query.
Be biased towards using the same tools and tool groups as the example plans if they are similar.
{% endif %}
</file>

<file path="portia/templates/example_plans.py">
DEFAULT_EXAMPLE_PLANS: list[Plan] = [
</file>

<file path="portia/templates/render.py">
def render_template(file_name: str, **kwargs: Any) -> str
⋮----
source = importlib.resources.files(templates).joinpath(file_name)
⋮----
env = Environment(loader=FileSystemLoader(template_path.parent), autoescape=True)
template = env.get_template(file_name)
</file>

<file path="portia/templates/tool_description.xml.jinja">
---
{{tool.overview_description | safe}}

Args:{% for arg in tool.args %}
- {{arg.name}} ({{arg.type}}) {% if arg.examples %}Sample inputs: {{arg.examples}}{% endif %} {% if arg.enum %}Enum: {{arg.enum}}{% endif %} {% if arg.default %}Default Value: {{arg.default}}{% endif %}{% endfor %}

Returns: {% if tool.output_description %}{{tool.output_description}}{% endif %}
---
</file>

<file path="portia/__init__.py">
__all__ = [
</file>

<file path="portia/clarification_handler.py">
class ClarificationHandler(ABC)
</file>

<file path="portia/clarification.py">
class ClarificationCategory(PortiaEnum)
⋮----
ACTION = "Action"
INPUT = "Input"
MULTIPLE_CHOICE = "Multiple Choice"
VALUE_CONFIRMATION = "Value Confirmation"
USER_VERIFICATION = "User Verification"
CUSTOM = "Custom"
class Clarification(BaseModel, ABC)
⋮----
id: ClarificationUUID = Field(
plan_run_id: PlanRunUUID | None = Field(
category: ClarificationCategory = Field(
response: Serializable | None = Field(
step: int | None = Field(default=None, description="The step this clarification is linked to.")
user_guidance: str = Field(
resolved: bool = Field(
source: str | None = Field(
class ActionClarification(Clarification)
⋮----
action_url: HttpUrl
require_confirmation: bool = Field(
⋮----
@field_serializer("action_url")
    def serialize_action_url(self, action_url: HttpUrl) -> str
class InputClarification(Clarification)
⋮----
argument_name: str = Field(
⋮----
class MultipleChoiceClarification(Clarification)
⋮----
options: list[Serializable]
⋮----
@model_validator(mode="after")
    def validate_response(self) -> Self
class ValueConfirmationClarification(Clarification)
class UserVerificationClarification(Clarification)
⋮----
@field_validator("response")
@classmethod
    def validate_response(cls, v: Any) -> Any
⋮----
@property
    def user_confirmed(self) -> bool
class CustomClarification(Clarification)
⋮----
name: str = Field(
data: dict[str, Any] = Field(
ClarificationType = (
ClarificationListType = list[ClarificationType]
</file>

<file path="portia/cli_clarification_handler.py">
class CLIClarificationHandler(ClarificationHandler)
⋮----
user_input = click.prompt(
⋮----
choices = click.Choice(clarification.options)
⋮----
result = click.confirm(text=click.style(clarification.user_guidance, fg=87), default=False)
⋮----
user_input = click.prompt(click.style("\nPlease enter a value", fg=87))
</file>

<file path="portia/cli.py">
DEFAULT_FILE_PATH = ".portia"
PORTIA_API_KEY = "portia_api_key"
class EnvLocation(Enum)
⋮----
ENV_FILE = "ENV_FILE"
ENV_VARS = "ENV_VARS"
class CLIConfig(BaseModel)
⋮----
env_location: EnvLocation = Field(
end_user_id: str = Field(
confirm: bool = Field(
tool_id: str | None = Field(
⋮----
option_name = field.replace("_", "-")
⋮----
field_type = _annotation_to_click_type(info.annotation)
⋮----
optional_kwargs = {}
default = info.default if info.default_factory is None else info.default_factory()
⋮----
field_help = info.description or f"Set the value for {option_name}"
⋮----
args = get_args(annotation)
⋮----
def common_options(f: Callable[..., Any]) -> Callable[..., Any]
⋮----
@wraps(f)
    def wrapper(*args: Any, **kwargs: Any) -> Any
⋮----
class CLIExecutionHooks(ExecutionHooks)
⋮----
clarification_handler: ClarificationHandler | None = CLIClarificationHandler()
⋮----
@click.group(context_settings={"max_content_width": 240})
def cli() -> None
⋮----
@click.command()
def version() -> None
⋮----
registry = DefaultToolRegistry(config)
portia = Portia(
plan = portia.plan(query, end_user=cli_config.end_user_id)
⋮----
plan_run = portia.run_plan(plan, end_user=cli_config.end_user_id)
⋮----
portia = Portia(config=config)
output = portia.plan(query, end_user=cli_config.end_user_id)
⋮----
cli_config = CLIConfig(**kwargs)
⋮----
config = Config.from_default(**kwargs)
</file>

<file path="portia/cloud.py">
class PortiaCloudClient
⋮----
config: Config
def __init__(self, config: Config) -> None
⋮----
headers = {}
⋮----
headers = {
⋮----
api_key = config.must_get_api_key("portia_api_key").get_secret_value()
⋮----
api_key = self.config.must_get_api_key("portia_api_key").get_secret_value()
</file>

<file path="portia/common.py">
Serializable = Any
SERIALIZABLE_TYPE_VAR = TypeVar("SERIALIZABLE_TYPE_VAR", bound=Serializable)
class PortiaEnum(str, Enum)
⋮----
@classmethod
    def enumerate(cls) -> tuple[tuple[str, str], ...]
def _serialize_for_json(value: Any) -> Any
def combine_args_kwargs(*args: Any, **kwargs: Any) -> Any
⋮----
args_dict = {f"{i}": _serialize_for_json(arg) for i, arg in enumerate(args)}
kwargs_dict = {k: _serialize_for_json(v) for k, v in kwargs.items()}
⋮----
EXTRAS_GROUPS_DEPENDENCIES = {
def validate_extras_dependencies(extra_group: str, *, raise_error: bool = True) -> bool
⋮----
def are_packages_installed(packages: list[str]) -> bool
⋮----
T = TypeVar("T")
def singleton(cls: type[T]) -> Callable[..., T]
⋮----
instance: list[T | None] = [None]
def wrapper(*args: Any, **kwargs: Any) -> T
def reset() -> None
</file>

<file path="portia/config.py">
T = TypeVar("T")
class StorageClass(Enum)
⋮----
MEMORY = "MEMORY"
DISK = "DISK"
CLOUD = "CLOUD"
class Model(NamedTuple)
⋮----
provider: LLMProvider
model_name: str
class LLMModel(Enum)
⋮----
@classmethod
    def _missing_(cls, value: object) -> LLMModel
GPT_4_O = Model(provider=LLMProvider.OPENAI, model_name="gpt-4o")
GPT_4_1 = Model(provider=LLMProvider.OPENAI, model_name="gpt-4.1")
GPT_4_O_MINI = Model(provider=LLMProvider.OPENAI, model_name="gpt-4o-mini")
GPT_3_5_TURBO = Model(provider=LLMProvider.OPENAI, model_name="gpt-3.5-turbo")
O_3_MINI = Model(provider=LLMProvider.OPENAI, model_name="o3-mini")
CLAUDE_3_5_SONNET = Model(provider=LLMProvider.ANTHROPIC, model_name="claude-3-5-sonnet-latest")
CLAUDE_3_5_HAIKU = Model(provider=LLMProvider.ANTHROPIC, model_name="claude-3-5-haiku-latest")
CLAUDE_3_OPUS = Model(provider=LLMProvider.ANTHROPIC, model_name="claude-3-opus-latest")
CLAUDE_3_7_SONNET = Model(provider=LLMProvider.ANTHROPIC, model_name="claude-3-7-sonnet-latest")
MISTRAL_LARGE = Model(provider=LLMProvider.MISTRALAI, model_name="mistral-large-latest")
GEMINI_2_5_FLASH = Model(
GEMINI_2_5_PRO = Model(
GEMINI_2_0_FLASH = Model(
GEMINI_2_0_FLASH_LITE = Model(
GEMINI_1_5_FLASH = Model(
AZURE_GPT_4_O = Model(provider=LLMProvider.AZURE_OPENAI, model_name="gpt-4o")
AZURE_GPT_4_O_MINI = Model(provider=LLMProvider.AZURE_OPENAI, model_name="gpt-4o-mini")
AZURE_GPT_4_1 = Model(provider=LLMProvider.AZURE_OPENAI, model_name="gpt-4.1")
AZURE_O_3_MINI = Model(provider=LLMProvider.AZURE_OPENAI, model_name="o3-mini")
⋮----
@property
    def api_name(self) -> str
def provider(self) -> LLMProvider
def to_model_string(self) -> str
class _AllModelsSupportedWithDeprecation(Container)
⋮----
def __contains__(self, item: object) -> bool
ALL_MODELS_SUPPORTED_WITH_DEPRECATION = _AllModelsSupportedWithDeprecation()
SUPPORTED_OPENAI_MODELS = ALL_MODELS_SUPPORTED_WITH_DEPRECATION
SUPPORTED_ANTHROPIC_MODELS = ALL_MODELS_SUPPORTED_WITH_DEPRECATION
SUPPORTED_MISTRALAI_MODELS = ALL_MODELS_SUPPORTED_WITH_DEPRECATION
SUPPORTED_GOOGLE_GENERATIVE_AI_MODELS = ALL_MODELS_SUPPORTED_WITH_DEPRECATION
SUPPORTED_AZURE_OPENAI_MODELS = ALL_MODELS_SUPPORTED_WITH_DEPRECATION
class ExecutionAgentType(Enum)
⋮----
ONE_SHOT = "ONE_SHOT"
DEFAULT = "DEFAULT"
class PlanningAgentType(Enum)
class LogLevel(Enum)
⋮----
DEBUG = "DEBUG"
INFO = "INFO"
WARNING = "WARNING"
ERROR = "ERROR"
CRITICAL = "CRITICAL"
FEATURE_FLAG_AGENT_MEMORY_ENABLED = "feature_flag_agent_memory_enabled"
E = TypeVar("E", bound=Enum)
def parse_str_to_enum(value: str | E, enum_type: type[E]) -> E
PLANNING_MODEL_KEY = "planning_model_name"
EXECUTION_MODEL_KEY = "execution_model_name"
INTROSPECTION_MODEL_KEY = "introspection_model_name"
SUMMARISER_MODEL_KEY = "summariser_model_name"
DEFAULT_MODEL_KEY = "default_model_name"
MODEL_EXTRA_KWARGS = {
class GenerativeModelsConfig(BaseModel)
⋮----
model_config = ConfigDict(arbitrary_types_allowed=True)
default_model: GenerativeModel | str | None = None
planning_model: GenerativeModel | str | None = None
execution_model: GenerativeModel | str | None = None
introspection_model: GenerativeModel | str | None = None
summarizer_model: GenerativeModel | str | None = None
⋮----
@model_validator(mode="before")
@classmethod
    def parse_models(cls, data: dict[str, Any]) -> dict[str, Any]
⋮----
new_data = {}
⋮----
# need to cache for longer than a day.
CACHE_TTL_SECONDS = 60 * 60 * 24
class Config(BaseModel)
⋮----
# Portia Cloud Options
portia_api_endpoint: str = Field(
portia_dashboard_url: str = Field(
portia_api_key: SecretStr | None = Field(
# LLM API Keys
openai_api_key: SecretStr = Field(
anthropic_api_key: SecretStr = Field(
mistralai_api_key: SecretStr = Field(
google_api_key: SecretStr = Field(
azure_openai_api_key: SecretStr = Field(
azure_openai_endpoint: str = Field(
ollama_base_url: str = Field(
aws_access_key_id: str = Field(
aws_secret_access_key: str = Field(
aws_default_region: str = Field(
aws_credentials_profile_name: str | None = Field(
llm_redis_cache_url: str | None = Field(
llm_provider: LLMProvider | None = Field(
models: GenerativeModelsConfig = Field(
feature_flags: dict[str, bool] = Field(
argument_clarifications_enabled: bool = Field(
⋮----
@model_validator(mode="after")
    def parse_feature_flags(self) -> Self
⋮----
# Fill here with any default feature flags.
# e.g. CONDITIONAL_FLAG: True,
⋮----
@model_validator(mode="after")
    def setup_cache(self) -> Self
⋮----
cache = RedisCache(self.llm_redis_cache_url, ttl=CACHE_TTL_SECONDS, prefix="llm:")
⋮----
logger().warning(  # pragma: no cover
"Not using cache as cache group is not installed. "  # pragma: no cover
"Install portia-sdk-python[caching] to use caching."  # pragma: no cover
)  # pragma: no cover
⋮----
# Storage Options
storage_class: StorageClass = Field(
⋮----
@field_validator("storage_class", mode="before")
@classmethod
    def parse_storage_class(cls, value: str | StorageClass) -> StorageClass
storage_dir: str | None = Field(
# Logging Options
# default_log_level controls the minimal log level, i.e. setting to DEBUG will print all logs
# where as setting it to ERROR will only display ERROR and above.
default_log_level: LogLevel = Field(
⋮----
@field_validator("default_log_level", mode="before")
@classmethod
    def parse_default_log_level(cls, value: str | LogLevel) -> LogLevel
# default_log_sink controls where default logs are sent. By default this is STDOUT (sys.stdout)
# but can also be set to STDERR (sys.stderr)
# or to a file by setting this to a file path ("./logs.txt")
default_log_sink: str = Field(
# json_log_serialize sets whether logs are JSON serialized before sending to the log sink.
json_log_serialize: bool = Field(
# Agent Options
execution_agent_type: ExecutionAgentType = Field(
⋮----
@field_validator("execution_agent_type", mode="before")
@classmethod
    def parse_execution_agent_type(cls, value: str | ExecutionAgentType) -> ExecutionAgentType
# PlanningAgent Options
planning_agent_type: PlanningAgentType = Field(
⋮----
@field_validator("planning_agent_type", mode="before")
@classmethod
    def parse_planning_agent_type(cls, value: str | PlanningAgentType) -> PlanningAgentType
large_output_threshold_tokens: int = Field(
def exceeds_output_threshold(self, value: str | list[str | dict]) -> bool
def get_agent_default_model(  # noqa: C901, PLR0911, PLR0912
⋮----
@model_validator(mode="after")
    def fill_default_models(self) -> Self
⋮----
@model_validator(mode="after")
    def check_config(self) -> Self
⋮----
# Portia API Key must be provided if using cloud storage
⋮----
# Check that all models passed as strings are instantiable, i.e. they have the
# right API keys and other required configuration.
⋮----
@classmethod
    def from_default(cls, **kwargs) -> Config:  # noqa: ANN003
def has_api_key(self, name: str) -> bool
def must_get_api_key(self, name: str) -> SecretStr
def must_get(self, name: str, expected_type: type[T]) -> T
⋮----
value = getattr(self, name)
⋮----
# ensure non-empty values
⋮----
def get_default_model(self) -> GenerativeModel
⋮----
model = self.get_generative_model(self.models.default_model)
⋮----
# Default model is required, but not provided.
⋮----
def get_planning_model(self) -> GenerativeModel
def get_execution_model(self) -> GenerativeModel
def get_introspection_model(self) -> GenerativeModel
def get_summarizer_model(self) -> GenerativeModel
⋮----
def _parse_model_string(self, model_string: str) -> GenerativeModel
⋮----
parts = model_string.strip().split("/", maxsplit=1)
provider = parts[0]
model_name = parts[1]
llm_provider = LLMProvider(provider)
⋮----
def _construct_model_from_name(  # noqa: PLR0911
⋮----
def llm_provider_default_from_api_keys(**kwargs) -> LLMProvider | None:  # noqa: ANN003, PLR0911
def default_config(**kwargs) -> Config:  # noqa: ANN003
⋮----
llm_provider_from_api_keys = llm_provider_default_from_api_keys(**kwargs)
⋮----
llm_provider = parse_str_to_enum(
⋮----
llm_provider = llm_provider_from_api_keys
⋮----
llm_provider = None
# Handle deprecated llm_model_name keyword argument
⋮----
legacy_model_kwargs = {}
⋮----
models = kwargs.pop("models", {})
⋮----
models = models.model_dump(exclude_unset=True)
duplicate_model_keys = kwargs.keys() & models.keys()
⋮----
def filter_none(mapping: dict[str, Any]) -> dict[str, Any]
kwargs_models = {
models = GenerativeModelsConfig(
default_storage_class = (
</file>

<file path="portia/end_user.py">
class EndUser(BaseModel)
⋮----
external_id: str = Field(description="The external ID of the end user.")
name: str = Field(default="", description="The name of the end user.")
email: str = Field(default="", description="The email address of the end user.")
phone_number: str = Field(default="", description="The phone number of the end user.")
additional_data: dict[str, str | None] = Field(
def set_additional_data(self, key_name: str, key_value: str) -> None
def remove_additional_data(self, key_name: str) -> None
def get_additional_data(self, key_name: str) -> str | None
</file>

<file path="portia/errors.py">
class PortiaBaseError(Exception)
class SkipExecutionError(PortiaBaseError)
⋮----
should_return: bool
def __init__(self, reason: str, should_return: bool = False) -> None
class ConfigNotFoundError(PortiaBaseError)
⋮----
def __init__(self, value: str) -> None
class InvalidConfigError(PortiaBaseError)
⋮----
def __init__(self, value: str, issue: str) -> None
class PlanError(PortiaBaseError)
⋮----
def __init__(self, error_string: str) -> None
class PlanNotFoundError(PortiaBaseError)
⋮----
def __init__(self, plan_id: PlanUUID) -> None
class PlanRunNotFoundError(PortiaBaseError)
⋮----
def __init__(self, plan_run_id: PlanRunUUID | str | None) -> None
class ToolNotFoundError(PortiaBaseError)
⋮----
def __init__(self, tool_id: str) -> None
class DuplicateToolError(PortiaBaseError)
class InvalidToolDescriptionError(PortiaBaseError)
class ToolRetryError(PortiaBaseError)
⋮----
def __init__(self, tool_id: str, error_string: str) -> None
class ToolFailedError(PortiaBaseError)
class InvalidPlanRunStateError(PortiaBaseError)
class InvalidAgentError(PortiaBaseError)
⋮----
def __init__(self, state: str) -> None
class InvalidAgentOutputError(PortiaBaseError)
⋮----
def __init__(self, content: str) -> None
class ToolHardError(PortiaBaseError)
⋮----
def __init__(self, cause: Exception | str) -> None
class ToolSoftError(PortiaBaseError)
class StorageError(PortiaBaseError)
</file>

<file path="portia/execution_hooks.py">
class BeforeStepExecutionOutcome(PortiaEnum)
⋮----
CONTINUE = "CONTINUE"
SKIP = "SKIP"
class ExecutionHooks(BaseModel)
⋮----
model_config = ConfigDict(arbitrary_types_allowed=True)
clarification_handler: ClarificationHandler | None = None
before_step_execution: Callable[[Plan, PlanRun, Step], BeforeStepExecutionOutcome] | None = None
after_step_execution: Callable[[Plan, PlanRun, Step, Output], None] | None = None
before_plan_run: Callable[[Plan, PlanRun], None] | None = None
after_plan_run: Callable[[Plan, PlanRun, Output], None] | None = None
before_tool_call: (
after_tool_call: Callable[[Tool, Any, PlanRun, Step], Clarification | None] | None = None
⋮----
tool_ids = [tool.id]
⋮----
tool_ids = [tool]
⋮----
tool_ids = [t.id if isinstance(t, Tool) else t for t in tool]
⋮----
previous_clarification = plan_run.get_clarification_for_step(
serialised_args = (
⋮----
def log_step_outputs(plan: Plan, plan_run: PlanRun, step: Step, output: Output) -> None
</file>

<file path="portia/gemini_langsmith_wrapper.py">
def _get_ls_params(model_name: str, _: dict) -> dict[str, str]
⋮----
def _extract_parts(content_item: types.ContentUnion | types.ContentUnionDict) -> list[str]
⋮----
result = []
⋮----
contents = inputs["contents"]
⋮----
contents = [contents]
first = contents[0]
parts = _extract_parts(first)
⋮----
def wrap_gemini(client: genai.Client) -> genai.Client
⋮----
original_generate_content = client.models.generate_content
⋮----
decorator = run_helpers.traceable(
</file>

<file path="portia/logger.py">
FUNCTION_COLOR_MAP = {
class LoggerInterface(Protocol)
⋮----
def debug(self, msg: str, *args, **kwargs) -> None: ...
def info(self, msg: str, *args, **kwargs) -> None: ...
def warning(self, msg: str, *args, **kwargs) -> None: ...
def error(self, msg: str, *args, **kwargs) -> None: ...
def critical(self, msg: str, *args, **kwargs) -> None: ...
def exception(self, msg: str, *args, **kwargs) -> None: ...
class Formatter
⋮----
def __init__(self) -> None
def format(self, record: Any) -> str
⋮----
msg = record["message"]
⋮----
msg = self._sanitize_message_(msg)
function_color = self._get_function_color_(record)
result = (
⋮----
formatted_stack_trace = "".join(traceback.format_exception(record["exception"].value))
formatted_stack_trace = self._sanitize_message_(formatted_stack_trace, truncate=False)
⋮----
def _sanitize_message_(self, msg: str, truncate: bool = True) -> str
⋮----
msg = re.sub(r"(?<!\{)\{(?!\{)", "{{", msg)
msg = re.sub(r"(?<!\})\}(?!\})", "}}", msg)
msg = msg.replace("<", r"\<").replace(">", r"\>")
⋮----
def _get_function_color_(self, record: Any) -> str
def _truncated_message_(self, msg: str) -> str
⋮----
lines = msg.split("\n")
⋮----
keep_lines = self.max_lines - 1
head_lines = keep_lines // 2
tail_lines = keep_lines - head_lines
truncated_lines = lines[:head_lines]
⋮----
msg = "\n".join(truncated_lines)
⋮----
class SafeLogger(LoggerInterface)
⋮----
def __init__(self, child_logger: LoggerInterface) -> None
def debug(self, msg: str, *args: Any, **kwargs: Any) -> None
def info(self, msg: str, *args: Any, **kwargs: Any) -> None
def warning(self, msg: str, *args: Any, **kwargs: Any) -> None
def error(self, msg: str, *args: Any, **kwargs: Any) -> None
def exception(self, msg: str, *args: Any, **kwargs: Any) -> None
def critical(self, msg: str, *args: Any, **kwargs: Any) -> None
class LoggerManager
⋮----
def __init__(self, custom_logger: LoggerInterface | None = None) -> None
⋮----
@property
    def logger(self) -> LoggerInterface
def set_logger(self, custom_logger: LoggerInterface) -> None
def configure_from_config(self, config: Config) -> None
⋮----
log_sink = config.default_log_sink
⋮----
log_sink = sys.stdout
⋮----
log_sink = sys.stderr
⋮----
logger_manager = LoggerManager()
def logger() -> LoggerInterface
</file>

<file path="portia/mcp_session.py">
class SseMcpClientConfig(BaseModel)
⋮----
server_name: str
url: str
headers: dict[str, Any] | None = None
timeout: float = 5
sse_read_timeout: float = 60 * 5
tool_call_timeout_seconds: float | None = None
class StdioMcpClientConfig(BaseModel)
⋮----
command: str
args: list[str] = Field(default_factory=list)
env: dict[str, str] | None = None
encoding: str = "utf-8"
encoding_error_handler: Literal["strict", "ignore", "replace"] = "strict"
⋮----
@classmethod
    def from_raw(cls, config: str | dict[str, Any]) -> StdioMcpClientConfig
⋮----
json_config = json.loads(config)
⋮----
json_config = config
⋮----
server_name = next(iter(json_config["mcpServers"].keys()))
server_config = json_config["mcpServers"][server_name]
⋮----
server_name = next(iter(json_config["servers"].keys()))
server_config = json_config["servers"][server_name]
⋮----
class StreamableHttpMcpClientConfig(BaseModel)
⋮----
model_config = ConfigDict(arbitrary_types_allowed=True)
⋮----
timeout: float = 30
⋮----
terminate_on_close: bool = True
auth: httpx.Auth | None = None
⋮----
McpClientConfig = SseMcpClientConfig | StdioMcpClientConfig | StreamableHttpMcpClientConfig
⋮----
@asynccontextmanager
async def get_mcp_session(mcp_client_config: McpClientConfig) -> AsyncIterator[ClientSession]
</file>

<file path="portia/model.py">
_llm_cache: ContextVar[BaseCache | None] = ContextVar("llm_cache", default=None)
class Message(BaseModel)
⋮----
role: Literal["user", "assistant", "system"]
content: str
⋮----
@classmethod
    def from_langchain(cls, message: BaseMessage) -> Message
def to_langchain(self) -> BaseMessage
class LLMProvider(Enum)
⋮----
OPENAI = "openai"
ANTHROPIC = "anthropic"
MISTRALAI = "mistralai"
GOOGLE = "google"
AMAZON = "amazon"
AZURE_OPENAI = "azure-openai"
CUSTOM = "custom"
OLLAMA = "ollama"
GOOGLE_GENERATIVE_AI = "google"
BaseModelT = TypeVar("BaseModelT", bound=BaseModel)
class GenerativeModel(ABC)
⋮----
provider: LLMProvider
def __init__(self, model_name: str) -> None
⋮----
@abstractmethod
    def get_response(self, messages: list[Message]) -> Message
⋮----
@abstractmethod
    async def aget_response(self, messages: list[Message]) -> Message
⋮----
def get_context_window_size(self) -> int
def __str__(self) -> str
def __repr__(self) -> str
⋮----
@abstractmethod
    def to_langchain(self) -> BaseChatModel
class LangChainGenerativeModel(GenerativeModel)
⋮----
provider: LLMProvider = LLMProvider.CUSTOM
⋮----
def to_langchain(self) -> BaseChatModel
def get_response(self, messages: list[Message]) -> Message
⋮----
langchain_messages = [msg.to_langchain() for msg in messages]
response = self._client.invoke(langchain_messages)
⋮----
structured_client = self._client.with_structured_output(schema, **kwargs)
response = structured_client.invoke(langchain_messages)
⋮----
async def aget_response(self, messages: list[Message]) -> Message
⋮----
response = await self._client.ainvoke(langchain_messages)
⋮----
response = await structured_client.ainvoke(langchain_messages)
⋮----
cache = _llm_cache.get()
⋮----
cache_data = {
data_hash = hashlib.md5(
llm_string = f"{provider}:{model}:{data_hash}"
prompt = json.dumps(messages)
⋮----
cached = cache.lookup(prompt, llm_string)
⋮----
response = client.chat.completions.create(
⋮----
cached = await cache.alookup(prompt, llm_string)
⋮----
response = await client.chat.completions.create(
⋮----
@classmethod
    def set_cache(cls, cache: BaseCache) -> None
class OpenAIGenerativeModel(LangChainGenerativeModel)
⋮----
provider: LLMProvider = LLMProvider.OPENAI
⋮----
temperature = 1 if model_name.lower() in ("o3-mini", "o4-mini", "gpt-5") else temperature
client = ChatOpenAI(
⋮----
instructor_messages = [map_message_to_instructor(msg) for msg in messages]
⋮----
class AzureOpenAIGenerativeModel(LangChainGenerativeModel)
⋮----
provider: LLMProvider = LLMProvider.AZURE_OPENAI
⋮----
temperature = 1 if model_name.lower() in ("o3-mini", "o4-mini") else temperature
client = AzureChatOpenAI(
⋮----
class AnthropicGenerativeModel(LangChainGenerativeModel)
⋮----
provider: LLMProvider = LLMProvider.ANTHROPIC
_output_instructor_threshold = 512
⋮----
client = ChatAnthropic(
⋮----
content = ", ".join(
⋮----
content = response.content
⋮----
structured_client = self._client.with_structured_output(schema, include_raw=True, **kwargs)
raw_response = structured_client.invoke(langchain_messages)
⋮----
response = raw_response["parsed"]
⋮----
raw_response = await structured_client.ainvoke(langchain_messages)
⋮----
class MistralAIGenerativeModel(LangChainGenerativeModel)
⋮----
provider: LLMProvider = LLMProvider.MISTRALAI
⋮----
client = ChatMistralAI(
⋮----
async def aget_response(self, messages: list[Message]) -> Message
⋮----
structured_client = self._client.with_structured_output(
⋮----
def set_amazon_logging_level(level: int) -> None
class AmazonBedrockGenerativeModel(LangChainGenerativeModel)
⋮----
provider: LLMProvider = LLMProvider.AMAZON
⋮----
client = ChatBedrock(
⋮----
bedrock_client = boto3.client(
⋮----
class GoogleGenAiGenerativeModel(LangChainGenerativeModel)
⋮----
provider: LLMProvider = LLMProvider.GOOGLE
⋮----
genai_client = genai.Client(api_key=api_key.get_secret_value())
client = ChatGoogleGenerativeAI(
⋮----
wrapped_gemini_client = wrap_gemini(genai_client)
⋮----
class OllamaGenerativeModel(LangChainGenerativeModel)
⋮----
provider_name: str = "ollama"
⋮----
client = instructor.from_openai(
instructor_messages = [map_message_to_instructor(message) for message in messages]
⋮----
def map_message_to_instructor(message: Message) -> ChatCompletionMessageParam
</file>

<file path="portia/plan_run.py">
class PlanRunState(PortiaEnum)
⋮----
NOT_STARTED = "NOT_STARTED"
IN_PROGRESS = "IN_PROGRESS"
NEED_CLARIFICATION = "NEED_CLARIFICATION"
READY_TO_RESUME = "READY_TO_RESUME"
COMPLETE = "COMPLETE"
FAILED = "FAILED"
class PlanRunOutputs(BaseModel)
⋮----
model_config = ConfigDict(extra="forbid")
clarifications: ClarificationListType = Field(
step_outputs: dict[str, Output] = Field(
final_output: Output | None = Field(
class PlanRun(BaseModel)
⋮----
id: PlanRunUUID = Field(
plan_id: PlanUUID = Field(
current_step_index: int = Field(
state: PlanRunState = Field(
end_user_id: str = Field(
outputs: PlanRunOutputs = Field(
plan_run_inputs: dict[str, LocalDataValue] = Field(
structured_output_schema: type[BaseModel] | None = Field(
def get_outstanding_clarifications(self) -> ClarificationListType
def get_clarifications_for_step(self, step: int | None = None) -> ClarificationListType
⋮----
step = self.current_step_index
⋮----
def get_potential_step_inputs(self) -> dict[str, Output]
def __str__(self) -> str
class ReadOnlyPlanRun(PlanRun)
⋮----
model_config = ConfigDict(frozen=True, extra="forbid")
⋮----
@classmethod
    def from_plan_run(cls, plan_run: PlanRun) -> ReadOnlyPlanRun
</file>

<file path="portia/plan.py">
class PlanBuilder
⋮----
query: str
steps: list[Step]
plan_inputs: list[PlanInput]
structured_output_schema: type[BaseModel] | None
⋮----
inputs = []
⋮----
output = f"$output_{len(self.steps)}"
⋮----
step_index = self._get_step_index_or_raise(step_index)
⋮----
description = ""
⋮----
def build(self) -> Plan
⋮----
tool_ids = list({step.tool_id for step in self.steps if step.tool_id is not None})
⋮----
def _get_step_index_or_raise(self, step_index: int | None) -> int
⋮----
step_index = len(self.steps) - 1
⋮----
class Variable(BaseModel)
⋮----
model_config = ConfigDict(extra="ignore")
name: str = Field(
description: str = Field(
def pretty_print(self) -> str
class PlanInput(BaseModel)
⋮----
description: str | None = Field(
value: Serializable | None = Field(
⋮----
class Step(BaseModel)
⋮----
model_config = ConfigDict(extra="allow")
task: str = Field(
inputs: list[Variable] = Field(
tool_id: str | None = Field(
output: str = Field(
condition: str | None = Field(
structured_output_schema: type[BaseModel] | None = Field(
⋮----
message = (
⋮----
class ReadOnlyStep(Step)
⋮----
model_config = ConfigDict(frozen=True, extra="forbid")
⋮----
@classmethod
    def from_step(cls, step: Step) -> ReadOnlyStep
class PlanContext(BaseModel)
⋮----
model_config = ConfigDict(extra="forbid")
query: str = Field(description="The original query given by the user.")
tool_ids: list[str] = Field(
⋮----
@field_serializer("tool_ids")
    def serialize_tool_ids(self, tool_ids: list[str]) -> list[str]
class Plan(BaseModel)
⋮----
id: PlanUUID = Field(
plan_context: PlanContext = Field(description="The context for when the plan was created.")
steps: list[Step] = Field(description="The set of steps to solve the query.")
plan_inputs: list[PlanInput] = Field(
⋮----
def __str__(self) -> str
⋮----
@classmethod
    def from_response(cls, response_json: dict) -> Plan
⋮----
portia_tools = [tool for tool in self.plan_context.tool_ids if tool.startswith("portia:")]
other_tools = [
tools_summary = f"{len(portia_tools)} portia tools, {len(other_tools)} other tools"
inputs_section = ""
⋮----
inputs_section = (
⋮----
@model_validator(mode="after")
    def validate_plan(self) -> Self
⋮----
outputs = [step.output + (step.condition or "") for step in self.steps]
⋮----
input_names = [input_.name for input_ in self.plan_inputs]
⋮----
class ReadOnlyPlan(Plan)
⋮----
@classmethod
    def from_plan(cls, plan: Plan) -> ReadOnlyPlan
</file>

<file path="portia/portia.py">
class RunContext(BaseModel)
⋮----
model_config = ConfigDict(arbitrary_types_allowed=True)
plan: PlanV2 = Field(description="The Portia plan being executed.")
legacy_plan: Plan = Field(description="The legacy plan representation.")
plan_run: PlanRun = Field(description="The current plan run instance.")
end_user: EndUser = Field(description="The end user executing the plan.")
step_output_values: list[ReferenceValue] = Field(
portia: Portia = Field(description="The Portia client instance.")
class Portia
⋮----
def initialize_end_user(self, end_user: str | EndUser | None = None) -> EndUser
⋮----
default_external_id = "portia:default_user"
⋮----
end_user = default_external_id
end_user_instance = self.storage.get_end_user(external_id=end_user)
⋮----
end_user_instance = EndUser(external_id=end_user or default_external_id)
⋮----
end_user = EndUser(external_id=default_external_id)
⋮----
async def ainitialize_end_user(self, end_user: str | EndUser | None = None) -> EndUser
⋮----
end_user_instance = await self.storage.aget_end_user(external_id=end_user)
⋮----
coerced_plan_run_inputs = self._coerce_plan_run_inputs(plan_run_inputs)
plan = self._plan(
end_user = self.initialize_end_user(end_user)
plan_run = self._create_plan_run(plan, end_user, coerced_plan_run_inputs)
⋮----
plan = await self._aplan(
end_user = await self.ainitialize_end_user(end_user)
plan_run = await self._acreate_plan_run(plan, end_user, coerced_plan_run_inputs)
⋮----
to_return = []
⋮----
resolved_plans = []
⋮----
resolved_plan = self._resolve_single_example_plan(example_plan)
⋮----
resolved_plan = await self._aresolve_single_example_plan(example_plan)
⋮----
def _resolve_single_example_plan(self, example_plan: Plan | PlanUUID | str) -> Plan
async def _aresolve_single_example_plan(self, example_plan: Plan | PlanUUID | str) -> Plan
def _load_plan_by_uuid(self, plan_uuid: PlanUUID) -> Plan
async def _aload_plan_by_uuid(self, plan_uuid: PlanUUID) -> Plan
def _resolve_string_example_plan(self, example_plan: str) -> Plan
⋮----
plan_uuid = PlanUUID.from_string(example_plan)
⋮----
async def _aresolve_string_example_plan(self, example_plan: str) -> Plan
⋮----
tools = [
⋮----
tools = self.tool_registry.match_tools(query)
resolved_example_plans = self._resolve_example_plans(example_plans)
⋮----
planning_agent = self._get_planning_agent()
coerced_plan_inputs = self._coerce_plan_inputs(plan_inputs)
outcome = planning_agent.generate_steps_or_error(
⋮----
plan = Plan(
⋮----
resolved_example_plans = await self._aresolve_example_plans(example_plans)
⋮----
outcome = await planning_agent.agenerate_steps_or_error(
⋮----
plan_run = self._get_plan_run_from_plan(
⋮----
plan_run = await self._aget_plan_run_from_plan(
⋮----
plan_id = (
structured_output_schema = (
⋮----
plan = self.storage.get_plan(plan_id)
⋮----
# ensure we have the plan in storage.
# we won't if for example the user used PlanBuilder instead of dynamic planning.
⋮----
plan = await self.storage.aget_plan(plan_id)
⋮----
parsed_id = (
plan_run = self.storage.get_plan_run(parsed_id)
plan = self.storage.get_plan(plan_id=plan_run.plan_id)
⋮----
def _check_initial_readiness(self, plan: Plan, plan_run: PlanRun) -> tuple[bool, PlanRun]
⋮----
outstanding_clarifications = plan_run.get_outstanding_clarifications()
ready_clarifications = self._check_remaining_tool_readiness(plan, plan_run)
⋮----
plan_run = self._raise_clarifications(clarifications_to_raise, plan_run)
plan_run = self._handle_clarifications(plan_run)
⋮----
plan_run = await self.storage.aget_plan_run(parsed_id)
plan = await self.storage.aget_plan(plan_id=plan_run.plan_id)
⋮----
missing_inputs = [
⋮----
input_values_by_name = {input_obj.name: input_obj for input_obj in plan_run_inputs}
⋮----
plan_run = self._execute_plan_run(plan, plan_run)
⋮----
plan_run = await self._aexecute_plan_run(plan, plan_run)
⋮----
def _handle_clarifications(self, plan_run: PlanRun) -> PlanRun
⋮----
# has been raised
⋮----
clarifications = plan_run.get_outstanding_clarifications()
⋮----
# If clarifications are handled synchronously,
# we'll go through this immediately.
# we'll wait for the plan run to be ready.
plan_run = self.wait_for_ready(plan_run)
⋮----
matched_clarification = next(
⋮----
start_time = time.time()
tries = 0
⋮----
plan = self.storage.get_plan(plan_run.plan_id)
⋮----
plan_run = self.storage.get_plan_run(plan_run.id)
current_step_clarifications = plan_run.get_clarifications_for_step()
⋮----
# wait a couple of seconds as we're long polling
⋮----
ready_clarifications = self._check_remaining_tool_readiness(
⋮----
def _set_plan_run_state(self, plan_run: PlanRun, state: PlanRunState) -> None
⋮----
plan_run = PlanRun(
⋮----
def _execute_plan_run(self, plan: Plan, plan_run: PlanRun) -> PlanRun
⋮----
last_executed_step_output = self._get_last_executed_step_output(plan, plan_run)
introspection_agent = self._get_introspection_agent()
⋮----
step = plan.steps[index]
⋮----
last_executed_step_output = self._execute_step(
⋮----
error_output = LocalDataValue(value=str(e))
⋮----
async def _aexecute_plan_run(self, plan: Plan, plan_run: PlanRun) -> PlanRun
⋮----
last_executed_step_output = await self._aexecute_step(
⋮----
combined_clarifications = self._handle_new_clarifications(
⋮----
agent = self.get_agent_for_step(
⋮----
def _handle_before_step_execution_hook(self, plan: Plan, plan_run: PlanRun, step: Step) -> None
⋮----
outcome = self.execution_hooks.before_step_execution(
⋮----
tool_id = step.tool_id or ""
step_tool = self.tool_registry.get_tool(tool_id) if tool_id in self.tool_registry else None
⋮----
combined_clarifications = self._check_remaining_tool_readiness(
⋮----
combined_clarifications = new_clarifications + ready_clarifications
⋮----
def _log_execute_start(self, plan_run: PlanRun, plan: Plan) -> None
⋮----
dashboard_url = self.config.must_get("portia_dashboard_url", str)
dashboard_message = (
⋮----
error_output = LocalDataValue(value=str(error))
⋮----
def _log_final_output(self, plan_run: PlanRun, plan: Plan) -> None
⋮----
summary = plan_run.outputs.final_output.get_summary()
⋮----
summary = str(plan_run.outputs.final_output.get_value())
⋮----
summary = (
⋮----
def _get_last_executed_step_output(self, plan: Plan, plan_run: PlanRun) -> Output | None
⋮----
pre_step_outcome = introspection_agent.pre_step_introspection(
⋮----
pre_step_outcome = await introspection_agent.apre_step_introspection(
⋮----
def _should_introspect(self, plan: Plan, plan_run: PlanRun) -> bool
⋮----
step = plan.steps[plan_run.current_step_index]
⋮----
log_message = (
⋮----
output = LocalDataValue(
⋮----
def _get_planning_agent(self) -> BasePlanningAgent
⋮----
cls: type[BasePlanningAgent]
⋮----
cls = DefaultPlanningAgent
⋮----
final_output = LocalDataValue(
⋮----
summarizer = FinalOutputSummarizer(config=self.config, agent_memory=self.storage)
output = summarizer.create_summary(
⋮----
unsumarrized_output = plan_run.structured_output_schema(**output.model_dump())
⋮----
output_value = step_output.get_value()
⋮----
new_clarifications = (
⋮----
existing_clarification_ids = [clar.id for clar in plan_run.outputs.clarifications]
new_clarifications = [
⋮----
def get_tool(self, tool_id: str | None, plan_run: PlanRun) -> Tool | None
⋮----
child_tool = self.tool_registry.get_tool(tool_id)
⋮----
child_tool = LLMTool()
⋮----
raise  # pragma: no cover
⋮----
tool = self.get_tool(step.tool_id, plan_run)
cls: type[BaseExecutionAgent]
⋮----
cls = OneShotAgent
⋮----
cls = DefaultExecutionAgent
cls = OneShotAgent if isinstance(tool, LLMTool) else cls
⋮----
unauthenticated_client = PortiaCloudClient.new_client(
portia_registry = PortiaToolRegistry(
cloud_registry = self.tool_registry + portia_registry
tools = cloud_registry.match_tools(query)
⋮----
replan_outcome = planning_agent.generate_steps_or_error(
⋮----
tools_used = ", ".join([str(step.tool_id) for step in replan_outcome.steps])
⋮----
def _get_introspection_agent(self) -> BaseIntrospectionAgent
def _set_step_output(self, output: Output, plan_run: PlanRun, step: Step) -> Output
def _persist_step_state(self, plan_run: PlanRun, step: Step) -> Output
⋮----
step_output = plan_run.outputs.step_outputs[step.output]
⋮----
step_output = self.storage.save_plan_run_output(step.output, step_output, plan_run.id)
⋮----
tools_remaining = set()
portia_cloud_tool_ids_remaining = set()
ready_clarifications = []
check_from_index = start_index if start_index is not None else plan_run.current_step_index
tool_run_context = ToolRunContext(
⋮----
step = plan.steps[step_index]
# TODO(RH): Tidy up this check to work with local functions  # noqa: FIX002, TD003
⋮----
continue  # pragma: no cover - Should not happen if tool_id is set - defensive check
⋮----
ready_response = tool.ready(tool_run_context)
⋮----
portia_tools_ready_response = PortiaRemoteTool.batch_ready_check(
⋮----
legacy_plan = plan.to_legacy_plan(
⋮----
plan_run = await self._aget_plan_run_from_plan(legacy_plan, end_user, plan_run_inputs)
⋮----
end_user = self.storage.get_end_user(plan_run.end_user_id)
⋮----
run_data = RunContext(
⋮----
plan_run = await self._execute_builder_plan(plan, run_data)
⋮----
async def _execute_builder_plan(self, plan: PlanV2, run_data: RunContext) -> PlanRun:  # noqa: C901, PLR0912, PLR0915
⋮----
output_value = self._get_last_executed_step_output(run_data.legacy_plan, run_data.plan_run)
branch_stack: list[ConditionalStepResult] = []
⋮----
result = await step.run(run_data)
except Exception as e:  # noqa: BLE001
⋮----
jump_to_step_index: int | None = None
⋮----
jump_to_step_index = result.next_clause_step_index
⋮----
stack_state = branch_stack[-1]
⋮----
# One of the branches has already run, so we jump to exit
jump_to_step_index = stack_state.end_condition_block_step_index
⋮----
# Overwrite the stack state with the new result
⋮----
output_value = LocalDataValue(value=result)
# This may persist the output to memory - store the memory value if it does
output_value = self._set_step_output(
output = ReferenceValue(
⋮----
# No after_plan_run call here as the plan run will be resumed later
⋮----
except Exception as e:  # noqa: BLE001 - We want to capture all exceptions from the hook here
⋮----
error_value = LocalDataValue(value=str(e))
⋮----
error_output = ReferenceValue(
⋮----
# Skip the after_step_execution hook as we have already run it
⋮----
# Don't increment current step beyond the last step
⋮----
@staticmethod
    def _log_models(config: Config) -> None
⋮----
getter = getattr(config, f"get_{model}")
</file>

<file path="portia/prefixed_uuid.py">
PLAN_UUID_PREFIX = "plan"
PLAN_RUN_UUID_PREFIX = "prun"
CLARIFICATION_UUID_PREFIX = "clar"
class PrefixedUUID(BaseModel)
⋮----
prefix: ClassVar[str] = ""
uuid: UUID = Field(default_factory=uuid4)
def __str__(self) -> str
⋮----
@model_serializer
    def serialize_model(self) -> str
⋮----
@classmethod
    def from_string(cls, prefixed_uuid: str) -> Self
⋮----
@model_validator(mode="before")
@classmethod
    def validate_model(cls, v: str | dict) -> dict
def __hash__(self) -> int
class PlanUUID(PrefixedUUID)
⋮----
prefix: ClassVar[str] = PLAN_UUID_PREFIX
class PlanRunUUID(PrefixedUUID)
⋮----
prefix: ClassVar[str] = PLAN_RUN_UUID_PREFIX
class ClarificationUUID(PrefixedUUID)
⋮----
prefix: ClassVar[str] = CLARIFICATION_UUID_PREFIX
</file>

<file path="portia/storage.py">
T = TypeVar("T", bound=BaseModel)
MAX_OUTPUT_LOG_LENGTH = 1000
class PlanStorage(ABC)
⋮----
@abstractmethod
    def save_plan(self, plan: Plan) -> None
⋮----
@abstractmethod
    def get_plan(self, plan_id: PlanUUID) -> Plan
⋮----
@abstractmethod
    def get_plan_by_query(self, query: str) -> Plan
⋮----
@abstractmethod
    def plan_exists(self, plan_id: PlanUUID) -> bool
def get_similar_plans(self, query: str, threshold: float = 0.5, limit: int = 10) -> list[Plan]
async def asave_plan(self, plan: Plan) -> None
async def aget_plan(self, plan_id: PlanUUID) -> Plan
async def aget_plan_by_query(self, query: str) -> Plan
async def aplan_exists(self, plan_id: PlanUUID) -> bool
⋮----
class PlanRunListResponse(BaseModel)
⋮----
results: list[PlanRun]
count: int
total_pages: int
current_page: int
class RunStorage(ABC)
⋮----
@abstractmethod
    def save_plan_run(self, plan_run: PlanRun) -> None
⋮----
@abstractmethod
    def get_plan_run(self, plan_run_id: PlanRunUUID) -> PlanRun
⋮----
async def asave_plan_run(self, plan_run: PlanRun) -> None
async def aget_plan_run(self, plan_run_id: PlanRunUUID) -> PlanRun
⋮----
class AdditionalStorage(ABC)
⋮----
@abstractmethod
    def save_tool_call(self, tool_call: ToolCallRecord) -> None
⋮----
@abstractmethod
    def save_end_user(self, end_user: EndUser) -> EndUser
⋮----
@abstractmethod
    def get_end_user(self, external_id: str) -> EndUser | None
async def asave_tool_call(self, tool_call: ToolCallRecord) -> None
async def asave_end_user(self, end_user: EndUser) -> EndUser
async def aget_end_user(self, external_id: str) -> EndUser | None
class Storage(PlanStorage, RunStorage, AdditionalStorage)
class AgentMemory(ABC)
⋮----
@abstractmethod
    def get_plan_run_output(self, output_name: str, plan_run_id: PlanRunUUID) -> LocalDataValue
⋮----
MAX_STORAGE_OBJECT_BYTES = 32_000_000
def _check_size(obj_name: str, obj: object) -> None
def log_tool_call(tool_call: ToolCallRecord) -> None
⋮----
output = tool_call.output
⋮----
output = (
⋮----
class InMemoryStorage(PlanStorage, RunStorage, AdditionalStorage, AgentMemory)
⋮----
plans: dict[PlanUUID, Plan]
runs: dict[PlanRunUUID, PlanRun]
outputs: defaultdict[PlanRunUUID, dict[str, LocalDataValue]]
end_users: dict[str, EndUser]
def __init__(self) -> None
def save_plan(self, plan: Plan) -> None
def get_plan(self, plan_id: PlanUUID) -> Plan
def get_plan_by_query(self, query: str) -> Plan
⋮----
plan: Plan | None = None
⋮----
def plan_exists(self, plan_id: PlanUUID) -> bool
def save_plan_run(self, plan_run: PlanRun) -> None
def get_plan_run(self, plan_run_id: PlanRunUUID) -> PlanRun
⋮----
results = list(self.runs.values())
⋮----
results = [plan_run for plan_run in self.runs.values() if plan_run.state == run_state]
⋮----
def get_plan_run_output(self, output_name: str, plan_run_id: PlanRunUUID) -> LocalDataValue
def save_tool_call(self, tool_call: ToolCallRecord) -> None
def save_end_user(self, end_user: EndUser) -> EndUser
⋮----
existing_end_user = self.get_end_user(end_user.external_id)
⋮----
def get_end_user(self, external_id: str) -> EndUser | None
class DiskFileStorage(PlanStorage, RunStorage, AdditionalStorage, AgentMemory)
⋮----
def __init__(self, storage_dir: str | None) -> None
def _ensure_storage(self, file_path: str | None = None) -> None
def _write(self, file_path: str, content: BaseModel) -> None
def _read(self, file_name: str, model: type[T]) -> T
⋮----
f = file.read()
⋮----
plan_files = [
⋮----
plan = self._read(f.name, Plan)
⋮----
plan_runs = []
directory_path = Path(self.storage_dir)
⋮----
plan_run = self._read(f.name, PlanRun)
⋮----
filename = f"{plan_run_id}/{output_name}.json"
⋮----
file_name = f"{plan_run_id}/{output_name}.json"
⋮----
class PortiaCloudStorage(Storage, AgentMemory)
⋮----
DEFAULT_MAX_CACHE_SIZE = 20
⋮----
def _ensure_cache_dir(self, file_path: str | None = None) -> None
def _ensure_cache_size(self) -> None
⋮----
json_files = list(Path(self.cache_dir).glob("**/*.json"))
⋮----
oldest_file = min(json_files, key=lambda f: f.stat().st_mtime)
⋮----
def _write_to_cache(self, file_path: str, content: BaseModel) -> None
def _read_from_cache(self, file_name: str, model: type[T]) -> T
def check_response(self, response: httpx.Response) -> None
⋮----
error_str = str(response.content)
⋮----
response = self.client.post(
⋮----
response = await client.post(
⋮----
response = self.client.get(
⋮----
response_json = response.json()
⋮----
response = await client.get(
⋮----
plans = self.get_similar_plans(query, threshold=1.0, limit=1)
⋮----
plans = await self.aget_similar_plans(query, threshold=1.0, limit=1)
⋮----
response = self.client.put(
⋮----
response = await client.put(
⋮----
query = {}
⋮----
except Exception as e:  # noqa: BLE001
⋮----
# Don't raise an error if the response is not successful, just log it
⋮----
response = self.form_client.put(
⋮----
cache_file_path = f"{plan_run_id}/{output_name}.json"
⋮----
output_response = self.client.get(
⋮----
output_json = output_response.json()
summary = output_json["summary"]
value_url = output_json["url"]
value_response = self.client.get(value_url)
⋮----
output = LocalDataValue(
⋮----
output_response = await client.get(
⋮----
value_response = await client.get(value_url)
⋮----
def get_similar_plans(self, query: str, threshold: float = 0.5, limit: int = 5) -> list[Plan]
⋮----
results = response.json()
⋮----
def get_end_user(self, external_id: str) -> EndUser
</file>

<file path="portia/token_check.py">
AVERAGE_CHARS_PER_TOKEN = 5
def estimate_tokens(text: str) -> int
⋮----
value_str = str(value) if value is not None else ""
estimated_tokens = estimate_tokens(value_str)
context_window_size = model.get_context_window_size()
threshold_tokens = int(context_window_size * threshold_percentage)
</file>

<file path="portia/tool_call.py">
class ToolCallStatus(PortiaEnum)
⋮----
IN_PROGRESS = "IN_PROGRESS"
SUCCESS = "SUCCESS"
NEED_CLARIFICATION = "NEED_CLARIFICATION"
FAILED = "FAILED"
class ToolCallRecord(BaseModel)
⋮----
model_config = ConfigDict(extra="forbid")
tool_name: str
plan_run_id: PlanRunUUID
step: int
end_user_id: str | None
status: ToolCallStatus
input: Any
output: Any
latency_seconds: float
def serialize_input(self) -> Any
def serialize_output(self) -> Any
def _serialize_value(self, value: Any) -> Any
</file>

<file path="portia/tool_decorator.py">
P = inspect.Parameter
T = TypeVar("T")
def tool(fn: Callable[..., T]) -> type[Tool[T]]
⋮----
func_name = fn.__name__
description = (fn.__doc__ or "").strip()
tool_id = func_name
tool_name = _snake_to_title_case(func_name)
sig = inspect.signature(fn)
type_hints = get_type_hints(fn)
args_schema = _create_args_schema(sig, func_name, fn)
output_schema = _create_output_schema(type_hints, func_name)
def make_run_method() -> Callable
⋮----
def run(self: Tool[T], ctx: ToolRunContext, **kwargs: Any) -> T
⋮----
func_params = set(sig.parameters.keys())
⋮----
# Call the original function with filtered kwargs
filtered_kwargs = {k: v for k, v in kwargs.items() if k in func_params}
⋮----
class FunctionTool(Tool[T]):  # type: ignore[misc]
⋮----
def __init__(self, **data: Any) -> None
⋮----
# Set all the class attributes from the closure
⋮----
run = make_run_method()
# Set class name for better debugging
⋮----
def _validate_function(fn: Callable) -> None
⋮----
# Check that function has type hints for return type
⋮----
def _snake_to_title_case(snake_str: str) -> str
def _create_args_schema(sig: inspect.Signature, func_name: str, func: Callable) -> type[BaseModel]
⋮----
fields = {}
# Try to get type hints with extras to preserve Annotated metadata
⋮----
type_hints_with_extras = get_type_hints(func, include_extras=True)
⋮----
type_hints_with_extras = {}
⋮----
# Skip context parameters
⋮----
# First try to get annotation with extras (for Annotated support)
param_annotation = type_hints_with_extras.get(param_name)
⋮----
# Fall back to raw annotation from signature
⋮----
param_annotation = (
# Extract type and field info from annotation
⋮----
# Create the schema class
schema_class_name = f"{_snake_to_title_case(func_name).replace(' ', '')}Schema"
⋮----
default = ... if param.default == inspect.Parameter.empty else param.default
origin = get_origin(param_annotation)
⋮----
args = get_args(param_annotation)
⋮----
param_type = Any
description = _form_param_description(param_name, func_name)
field_info = cast("FieldInfo", Field(default=default, description=description))
⋮----
param_type = args[0]
annotation_metadata = args[1]
field_info: FieldInfo
⋮----
field_info = cast("FieldInfo", Field(default=default, description=annotation_metadata))
⋮----
field_info = cast("FieldInfo", annotation_metadata)
⋮----
param_type = param_annotation
⋮----
def _create_output_schema(type_hints: dict[str, type | object], func_name: str) -> tuple[str, str]
⋮----
return_type = type_hints.get("return", Any)
type_str = return_type.__name__ if hasattr(return_type, "__name__") else str(return_type)
description = f"Output from {func_name} function"
⋮----
def _form_param_description(param_name: str, func_name: str) -> str
</file>

<file path="portia/tool_registry.py">
class ToolRegistry
⋮----
def __init__(self, tools: dict[str, Tool] | Sequence[Tool] | None = None) -> None
def with_tool(self, tool: Tool, *, overwrite: bool = False) -> None
def replace_tool(self, tool: Tool) -> None
def get_tool(self, tool_id: str) -> Tool
def get_tools(self) -> list[Tool]
⋮----
def filter_tools(self, predicate: Callable[[Tool], bool]) -> ToolRegistry
⋮----
tool = self.get_tool(tool_id=tool_id)
new_description = (
⋮----
def __add__(self, other: ToolRegistry | list[Tool]) -> ToolRegistry
def __radd__(self, other: ToolRegistry | list[Tool]) -> ToolRegistry
def __iter__(self) -> Iterator[Tool]
def __len__(self) -> int
def __contains__(self, tool_id: str) -> bool
def _add(self, other: ToolRegistry | list[Tool]) -> ToolRegistry
⋮----
other_registry = other if isinstance(other, ToolRegistry) else ToolRegistry(other)
self_tools = self.get_tools()
other_tools = other_registry.get_tools()
tools = {}
⋮----
class InMemoryToolRegistry(ToolRegistry)
⋮----
@classmethod
    def from_local_tools(cls, tools: Sequence[Tool]) -> InMemoryToolRegistry
class PortiaToolRegistry(ToolRegistry)
⋮----
EXCLUDED_BY_DEFAULT_TOOL_REGEXS: frozenset[str] = frozenset()
⋮----
client = PortiaCloudClient.new_client(config)
⋮----
def with_default_tool_filter(self) -> PortiaToolRegistry
⋮----
def default_tool_filter(tool: Tool) -> bool
⋮----
@classmethod
    def _load_tools(cls, client: httpx.Client) -> dict[str, Tool]
⋮----
response = client.get(
⋮----
response_tools = response.json()
⋮----
response_tools = response.json().get("tools", [])
⋮----
tool = PortiaRemoteTool(
⋮----
class McpToolRegistry(ToolRegistry)
⋮----
config = SseMcpClientConfig(
tools = cls._load_tools(config, read_timeout=tool_list_read_timeout)
⋮----
tools = await cls._load_tools_async(config, read_timeout=tool_list_read_timeout)
⋮----
config = StdioMcpClientConfig(
⋮----
parsed_config = StdioMcpClientConfig.from_raw(config)
⋮----
tools = cls._load_tools(parsed_config, read_timeout=tool_list_read_timeout)
⋮----
config = StreamableHttpMcpClientConfig(
⋮----
T = TypeVar("T")
def _run_async_in_new_loop(coro: Coroutine[Any, Any, T]) -> T
⋮----
result_container: dict[str, T | Exception] = {}
def runner() -> None
⋮----
loop = asyncio.new_event_loop()
⋮----
thread = threading.Thread(target=runner)
⋮----
loop = asyncio.get_event_loop()
⋮----
async def _inner() -> list[PortiaMcpTool]
⋮----
tools = await session.list_tools()
⋮----
tool_name_snake_case = re.sub(r"[^a-zA-Z0-9]+", "_", mcp_tool.name)
description = (
⋮----
class DefaultToolRegistry(ToolRegistry)
⋮----
def __init__(self, config: Config) -> None
⋮----
tools = [
⋮----
class GeneratedBaseModel(BaseModel)
⋮----
_fields_must_omit_none_on_serialize: ClassVar[list[str]] = []
def __init_subclass__(cls) -> None
⋮----
@model_serializer(mode="wrap")
    def serialize(self, handler: SerializerFunctionWrapHandler) -> dict[str, Any]
⋮----
ser = handler(self)
⋮----
@classmethod
    def extend_exclude_unset_fields(cls, fields: list[str]) -> None
BaseModelT = TypeVar("BaseModelT", bound=BaseModel)
⋮----
schema_without_refs = replace_refs(json_schema, proxies=False)
properties = schema_without_refs.get("properties", {})
required = set(schema_without_refs.get("required", []))
non_nullable_omissible_fields = [
additional_properties: bool | dict[str, Any] = schema_without_refs.get(
extra_allowed = additional_properties is True or isinstance(additional_properties, dict)
config_dict: ConfigDict | None = None
pydantic_validator: Any = None
⋮----
config_dict = ConfigDict(extra="allow")
⋮----
extras_schema = generate_pydantic_model_from_json_schema(
validator = partial(
pydantic_validator = model_validator(mode="after")(validator)
fields = dict(
model = create_model(
⋮----
def _is_nullable_field(field_name: str, field: dict[str, Any]) -> bool
⋮----
python_type = _map_pydantic_type(field_name, field)
⋮----
default_from_schema = field.get("default")
field_type = _map_pydantic_type(field_name, field)
⋮----
field_type = field_type | None
field_kwargs: dict[str, Any] = {
⋮----
def _map_pydantic_type(field_name: str, field: dict[str, Any]) -> type | Any
⋮----
types = [
⋮----
item_type = _map_pydantic_type(field_name, field.get("items", {}))
</file>

<file path="portia/tool_wrapper.py">
MAX_OUTPUT_LOG_LENGTH = 1000
class ToolCallWrapper(Tool)
⋮----
model_config = ConfigDict(arbitrary_types_allowed=True)
_child_tool: Tool
_storage: AdditionalStorage
_plan_run: PlanRun
def __init__(self, child_tool: Tool, storage: AdditionalStorage, plan_run: PlanRun) -> None
def ready(self, ctx: ToolRunContext) -> ReadyResponse
def run(self, ctx: ToolRunContext, *args: Any, **kwargs: Any) -> Any | Clarification
⋮----
record = ToolCallRecord(
input_str = repr(record.input)
truncated_input = (
⋮----
start_time = datetime.now(tz=UTC)
⋮----
output = self._child_tool.run(ctx, *args, **kwargs)
</file>

<file path="portia/tool.py">
MAX_TOOL_DESCRIPTION_LENGTH = 16384
class ToolRunContext(BaseModel)
⋮----
model_config = ConfigDict(extra="forbid")
end_user: EndUser
plan_run: PlanRun
plan: Plan
config: Config
clarifications: ClarificationListType
class _ArgsSchemaPlaceholder(BaseModel)
class ReadyResponse(BaseModel)
⋮----
ready: bool
⋮----
class Tool(BaseModel, Generic[SERIALIZABLE_TYPE_VAR])
⋮----
model_config = ConfigDict(extra="forbid", arbitrary_types_allowed=True)
id: str = Field(description="ID of the tool")
name: str = Field(description="Name of the tool")
description: str = Field(description="Purpose of the tool and usage")
args_schema: type[BaseModel] = Field(default_factory=lambda _: _ArgsSchemaPlaceholder)
output_schema: tuple[str, str] = Field(
should_summarize: bool = Field(
structured_output_schema: type[BaseModel] | None = Field(
def ready(self, ctx: ToolRunContext) -> ReadyResponse
⋮----
output = self.run(ctx, *args, **kwargs)
⋮----
output = await self.arun(ctx, *args, **kwargs)
⋮----
def _parse_output(self, output: SERIALIZABLE_TYPE_VAR | Clarification) -> Output
⋮----
clarifications = output if isinstance(output, list) else [output]
⋮----
# to the default output schema, letting the llm step summarizer handle it
⋮----
return LocalDataValue(value=output)  # type: ignore  # noqa: PGH003
⋮----
intermediate_output = self._run(ctx, *args, **kwargs)
return (intermediate_output.get_value(), intermediate_output)  # type: ignore  # noqa: PGH003
⋮----
intermediate_output = await self._arun(ctx, *args, **kwargs)
⋮----
def _generate_tool_description(self) -> str
⋮----
args = []
args_name_description_dict = []
out_type = self.output_schema[0]
out_description = self.output_schema[1]
schema = self.args_json_schema()
⋮----
arg_dict = {
⋮----
description = self.description.replace("\n", " ")
overview = f"{self.name.replace(' ', '_')}({', '.join(args)})"
⋮----
template_dict = {
⋮----
@model_validator(mode="after")
    def check_description_length(self) -> Self
⋮----
description_length = len(self._generate_tool_description())
⋮----
@model_validator(mode="after")
    def check_run_method_signature(self) -> Self
⋮----
sig = inspect.signature(self.__class__.run, eval_str=True)
⋮----
# Dont fail if the types cannot be extracted. This can happen with eval_str=True
# if the class is not defined in a global scope. Since this validator is only
# for warnings we can just exit here.
⋮----
params = list(sig.parameters.values())
⋮----
params = params[1:]
⋮----
ctx_param = params[0]
⋮----
param_map = {
⋮----
pydantic_field = self.args_schema.model_fields.get(arg_name)
⋮----
def to_langchain(self, ctx: ToolRunContext, sync: bool = True) -> StructuredTool
def to_langchain_with_artifact(self, ctx: ToolRunContext, sync: bool = True) -> StructuredTool
def args_json_schema(self) -> dict[str, Any]
⋮----
return replace_refs(self.args_schema.model_json_schema())  # type: ignore  # noqa: PGH003
def __str__(self) -> str
⋮----
@field_serializer("args_schema")
    def serialize_args_schema(self, value: type[BaseModel]) -> str
def pretty(self) -> str
⋮----
title = f"| {self.name} ({self.id}) |"
⋮----
class PortiaRemoteTool(Tool, Generic[SERIALIZABLE_TYPE_VAR])
⋮----
client: httpx.Client
model_config = ConfigDict(arbitrary_types_allowed=True)
def parse_response(self, ctx: ToolRunContext, response: dict[str, Any]) -> Output
⋮----
output = LocalDataValue.model_validate(response["output"])
output_value = output.get_value()
⋮----
clarification = output_value[0]
⋮----
response = self.client.post(
⋮----
response_json = response.json()
⋮----
ready = ReadyResponse.model_validate(response_json)
⋮----
output = self.parse_response(ctx, response.json())
⋮----
client = PortiaCloudClient.new_client(config)
⋮----
batch_ready_response = client.post(
⋮----
class PortiaMcpTool(Tool[str])
⋮----
mcp_client_config: McpClientConfig
def run(self, _: ToolRunContext, **kwargs: Any) -> str
async def arun(self, _: ToolRunContext, **kwargs: Any) -> str
async def call_remote_mcp_tool(self, name: str, arguments: dict | None = None) -> str
⋮----
task = asyncio.create_task(self._call_mcp_tool(name, arguments))
⋮----
def _handle_mcp_tool_result(self, task: asyncio.Task[str]) -> str
async def _call_mcp_tool(self, name: str, arguments: dict | None = None) -> str
⋮----
tool_result = await session.call_tool(
⋮----
ExceptionT = TypeVar("ExceptionT", bound=BaseException)
⋮----
result = []
</file>

<file path="portia/version.py">
def get_version() -> str
⋮----
current_dir = Path(__file__).parent.parent
pyproject_path = current_dir / "pyproject.toml"
</file>

<file path="tests/integration/data/war_and_peace_ch1.txt">
Well, Prince, so Genoa and Lucca are now just family estates of the Buonapartes. But I warn you, if you don't tell me that this means war, if you still try to defend the infamies and horrors perpetrated by that Antichrist—I really believe he is Antichrist—I will have nothing more to do with you and you are no longer my friend, no longer my 'faithful slave,' as you call yourself! But how do you do? I see I have frightened you—sit down and tell me all the news.\" It was in July, 1805, and the speaker was the well-known Anna Pávlovna Schérer, maid of honor and favorite of the Empress Márya Fëdorovna. With these words she greeted Prince Vasíli Kurágin, a man of high rank and importance, who was the first to arrive at her reception. Anna Pávlovna had had a cough for some days. She was, as she said, suffering from la grippe; grippe being then a new word in St. Petersburg, used only by the elite. All her invitations without exception, written in French, and delivered by a scarlet-liveried footman that morning, ran as follows: \"If you have nothing better to do, Count (or Prince), and if the prospect of spending an evening with a poor invalid is not too terrible, I shall be very charmed to see you tonight between 7 and 10—Annette Schérer.\"\"Heavens! what a virulent attack!\" replied the prince, not in the least disconcerted by this reception. He had just entered, wearing an embroidered court uniform, knee breeches, and shoes, and had stars on his breast and a serene expression on his flat face. He spoke in that refined French in which our grandfathers not only spoke but thought, and with the gentle, patronizing intonation natural to a man of importance who had grown old in society and at court. He went up to Anna Pávlovna, kissed her hand, presenting to her his bald, scented, and shining head, and complacently seated himself on the sofa. \"First of all, dear friend, tell me how you are. Set your friend's mind at rest,\" said he without altering his tone, beneath the politeness and affected sympathy of which indifference and even irony could be discerned. \"Can one be well while suffering morally? Can one be calm in times like these if one has any feeling?\" said Anna Pávlovna. \"You are staying the whole evening, I hope?\"\"And the fete at the English ambassador's? Today is Wednesday. I must put in an appearance there,\" said the prince. \"My daughter is coming for me to take me there.\"\"I thought today's fete had been canceled. I confess all these festivities and fireworks are becoming wearisome.\"\"If they had known that you wished it, the entertainment would have been put off,\" said the prince, who, like a wound-up clock, by force of habit said things he did not even wish to be believed. \"Don't tease! Well, and what has been decided about Novosíltsev's dispatch? You know everything.\"\"What can one say about it?\" replied the prince in a cold, listless tone. \"What has been decided? They have decided that Buonaparte has burnt his boats, and I believe that we are ready to burn ours.\" Prince Vasíli always spoke languidly, like an actor repeating a stale part. Anna Pávlovna Schérer on the contrary, despite her forty years, overflowed with animation and impulsiveness. To be an enthusiast had become her social vocation and, sometimes even when she did not feel like it, she became enthusiastic in order not to disappoint the expectations of those who knew her. The subdued smile which, though it did not suit her faded features, always played round her lips expressed, as in a spoiled child, a continual consciousness of her charming defect, which she neither wished, nor could, nor considered it necessary, to correct. In the midst of a conversation on political matters Anna Pávlovna burst out: \"Oh, don't speak to me of Austria. Perhaps I don't understand things, but Austria never has wished, and does not wish, for war. She is betraying us! Russia alone must save Europe. Our gracious sovereign recognizes his high vocation and will be true to it. That is the one thing I have faith in! Our good and wonderful sovereign has to perform the noblest role on earth, and he is so virtuous and noble that God will not forsake him. He will fulfill his vocation and crush the hydra of revolution, which has become more terrible than ever in the person of this murderer and villain! We alone must avenge the blood of the just one.... Whom, I ask you, can we rely on?... England with her commercial spirit will not and cannot understand the Emperor Alexander's loftiness of soul. She has refused to evacuate Malta. She wanted to find, and still seeks, some secret motive in our actions. What answer did Novosíltsev get? None. The English have not understood and cannot understand the self-abnegation of our Emperor who wants nothing for himself, but only desires the good of mankind. And what have they promised? Nothing! And what little they have promised they will not perform! Prussia has always declared that Buonaparte is invincible, and that all Europe is powerless before him.... And I don't believe a word that Hardenburg says, or Haugwitz either. This famous Prussian neutrality is just a trap. I have faith only in God and the lofty destiny of our adored monarch. He will save Europe!\" She suddenly paused, smiling at her own impetuosity. \"I think,\" said the prince with a smile, \"that if you had been sent instead of our dear Wintzingerode you would have captured the King of Prussia's consent by assault. You are so eloquent. Will you give me a cup of tea?\"\"In a moment. À propos,\" she added, becoming calm again, \"I am expecting two very interesting men tonight, le Vicomte de Mortemart, who is connected with the Montmorencys through the Rohans, one of the best French families. He is one of the genuine émigrés, the good ones. And also the Abbé Morio. Do you know that profound thinker? He has been received by the Emperor. Had you heard?\"\"I shall be delighted to meet them,\" said the prince. \"But tell me,\" he added with studied carelessness as if it had only just occurred to him, though the question he was about to ask was the chief motive of his visit, \"is it true that the Dowager Empress wants Baron Funke to be appointed first secretary at Vienna? The baron by all accounts is a poor creature.\" Prince Vasíli wished to obtain this post for his son, but others were trying through the Dowager Empress Márya Fëdorovna to secure it for the baron. Anna Pávlovna almost closed her eyes to indicate that neither she nor anyone else had a right to criticize what the Empress desired or was pleased with. \"Baron Funke has been recommended to the Dowager Empress by her sister,\" was all she said, in a dry and mournful tone. As she named the Empress, Anna Pávlovna's face suddenly assumed an expression of profound and sincere devotion and respect mingled with sadness, and this occurred every time she mentioned her illustrious patroness. She added that Her Majesty had deigned to show Baron Funke beaucoup d'estime, and again her face clouded over with sadness. The prince was silent and looked indifferent. But, with the womanly and courtierlike quickness and tact habitual to her, Anna Pávlovna wished both to rebuke him (for daring to speak as he had done of a man recommended to the Empress) and at the same time to console him, so she said: \"Now about your family. Do you know that since your daughter came out everyone has been enraptured by her? They say she is amazingly beautiful.\" The prince bowed to signify his respect and gratitude. \"I often think,\" she continued after a short pause, drawing nearer to the prince and smiling amiably at him as if to show that political and social topics were ended and the time had come for intimate conversation—\"I often think how unfairly sometimes the joys of life are distributed. Why has fate given you two such splendid children? I don't speak of Anatole, your youngest. I don't like him,\" she added in a tone admitting of no rejoinder and raising her eyebrows. \"Two such charming children. And really you appreciate them less than anyone, and so you don't deserve to have them.\" And she smiled her ecstatic smile. \"I can't help it,\" said the prince. \"Lavater would have said I lack the bump of paternity.\"\"Don't joke; I mean to have a serious talk with you. Do you know I am dissatisfied with your younger son? Between ourselves\" (and her face assumed its melancholy expression), \"he was mentioned at Her Majesty's and you were pitied....\" The prince answered nothing, but she looked at him significantly, awaiting a reply. He frowned. \"What would you have me do?\" he said at last. \"You know I did all a father could for their education, and they have both turned out fools. Hippolyte is at least a quiet fool, but Anatole is an active one. That is the only difference between them.\" He said this smiling in a way more natural and animated than usual, so that the wrinkles round his mouth very clearly revealed something unexpectedly coarse and unpleasant. \"And why are children born to such men as you? If you were not a father there would be nothing I could reproach you with,\" said Anna Pávlovna, looking up pensively. \"I am your faithful slave and to you alone I can confess that my children are the bane of my life. It is the cross I have to bear. That is how I explain it to myself. It can't be helped!\" He said no more, but expressed his resignation to cruel fate by a gesture. Anna Pávlovna meditated. \"Have you never thought of marrying your prodigal son Anatole?\" she asked. \"They say old maids have a mania for matchmaking, and though I don't feel that weakness in myself as yet, I know a little person who is very unhappy with her father. She is a relation of yours, Princess Mary Bolkónskaya.\" Prince Vasíli did not reply, though, with the quickness of memory and perception befitting a man of the world, he indicated by a movement of the head that he was considering this information. \"Do you know,\" he said at last, evidently unable to check the sad current of his thoughts, \"that Anatole is costing me forty thousand rubles a year? And,\" he went on after a pause, \"what will it be in five years, if he goes on like this?\" Presently he added: \"That's what we fathers have to put up with.... Is this princess of yours rich?\"\"Her father is very rich and stingy. He lives in the country. He is the well-known Prince Bolkónski who had to retire from the army under the late Emperor, and was nicknamed 'the King of Prussia.' He is very clever but eccentric, and a bore. The poor girl is very unhappy. She has a brother; I think you know him, he married Lise Meinen lately. He is an aide-de-camp of Kutúzov's and will be here tonight.\"\"Listen, dear Annette,\" said the prince, suddenly taking Anna Pávlovna's hand and for some reason drawing it downwards. \"Arrange that affair for me and I shall always be your most devoted slave-slafe with an f, as a village elder of mine writes in his reports. She is rich and of good family and that's all I want.\" And with the familiarity and easy grace peculiar to him, he raised the maid of honor's hand to his lips, kissed it, and swung it to and fro as he lay back in his armchair, looking in another direction. \"Attendez,\" said Anna Pávlovna, reflecting, \"I'll speak to Lise, young Bolkónski's wife, this very evening, and perhaps the thing can be arranged. It shall be on your family's behalf that I'll start my apprenticeship as old maid",
</file>

<file path="tests/integration/open_source_tools/test_browser_tool.py">
STORAGE = [
⋮----
config = Config.from_default(
tool_registry = ToolRegistry(
portia = Portia(config=config, tools=tool_registry)
query = (
plan_run = portia.run(query)
⋮----
@pytest.mark.daily
@pytest.mark.flaky(reruns=3)
def test_portia_run_query_multi_step() -> None
⋮----
portia = Portia(
⋮----
@pytest.mark.daily
@pytest.mark.flaky(reruns=3)
def test_portia_multi_step_from_plan() -> None
⋮----
plan = (
plan_run = portia.run_plan(plan)
⋮----
step_outputs = plan_run.outputs.step_outputs
</file>

<file path="tests/integration/mcp_server.py">
logger = getLogger(__name__)
server = FastMCP("server", port=11385, log_level="DEBUG")
⋮----
@server.tool()
def add_one(input_number: float) -> str
</file>

<file path="tests/integration/test_e2e.py">
CORE_PROVIDERS = [
PLANNING_PROVIDERS = [
PROVIDER_MODELS = [
AGENTS = [
STORAGE = [
⋮----
config = Config.from_default(
addition_tool = AdditionTool()
⋮----
tool_registry = ToolRegistry([addition_tool])
portia = Portia(config=config, tools=tool_registry)
query = "Add 1 + 2"
plan_run = await portia.arun(query)
⋮----
plan_run = portia.run(query)
⋮----
tool_registry = ToolRegistry([AdditionTool()])
⋮----
plan = portia.plan(query)
⋮----
plan = await portia.aplan(query)
⋮----
class AdditionNumbers(BaseModel)
⋮----
num_a: int = Field(description="First number to add")
num_b: int = Field(description="Second number to add")
class AdditionTool(Tool)
⋮----
id: str = "add_tool"
name: str = "Addition Tool"
description: str = "Adds two numbers together"
args_schema: type[BaseModel] = AdditionNumbers
output_schema: tuple[str, str] = ("int", "The sum of the two numbers")
def run(self, _: ToolRunContext, num_a: int, num_b: int) -> int
⋮----
query = "Add two numbers"
plan = await portia.aplan(
⋮----
test_clarification_handler = TestClarificationHandler()
tool_registry = ToolRegistry([ClarificationTool()])
portia = Portia(
clarification_step = Step(
plan = Plan(
⋮----
plan_run = portia.run_plan(plan)
⋮----
plan_run = await portia.arun_plan(plan)
⋮----
def test_portia_run_query_with_clarifications_no_handler() -> None
⋮----
plan_run = portia.resolve_clarification(
⋮----
@pytest.mark.asyncio
async def test_portia_arun_query_with_clarifications_no_handler() -> None
⋮----
tool_registry = ToolRegistry([ErrorTool()])
⋮----
final_output = plan_run.outputs.final_output.get_value()
⋮----
class MyAdditionTool(AdditionTool)
⋮----
def run(self, _: ToolRunContext, a: int, b: int) -> int
tool_registry = ToolRegistry([MyAdditionTool()])
⋮----
retries: int = 0
def run(self, ctx: ToolRunContext, a: int, b: int) -> int | Clarification
⋮----
step_one = Step(
step_two = Step(
⋮----
resolved = False
⋮----
class ActionClarificationHandler(ClarificationHandler)
⋮----
def on_sleep_called(_: float) -> None
⋮----
resolved = True
⋮----
test_clarification_handler = ActionClarificationHandler()
⋮----
@pytest.mark.flaky(reruns=3)
def test_portia_run_query_with_conditional_steps() -> None
⋮----
config = Config.from_default(storage_class=StorageClass.MEMORY)
portia = Portia(config=config, tools=example_tool_registry)
query = (
⋮----
@pytest.mark.flaky(reruns=3)
@pytest.mark.asyncio
async def test_portia_arun_query_with_conditional_steps() -> None
def test_portia_run_query_with_example_registry_and_hooks() -> None
⋮----
execution_hooks = ExecutionHooks(
config = Config.from_default()
portia = Portia(config=config, tools=open_source_tool_registry, execution_hooks=execution_hooks)
query = """Add 1 + 2 together and then write me a haiku about the answer.
⋮----
def test_portia_run_query_requiring_cloud_tools_not_authenticated() -> None
⋮----
portia = Portia(config=config)
⋮----
@pytest.mark.asyncio
async def test_portia_arun_query_requiring_cloud_tools_not_authenticated() -> None
⋮----
portia = Portia(config=config, tools=open_source_tool_registry)
query = """First calculate 25 * 3, then write a haiku about the result,
⋮----
def test_plan_inputs() -> None
⋮----
numbers_input = PlanInput(
plan_inputs = [numbers_input]
⋮----
portia = Portia(config=config, tools=ToolRegistry([AdditionTool()]))
plan = portia.plan(
plan_run = portia.run_plan(plan, plan_run_inputs=plan_inputs)
⋮----
@pytest.mark.asyncio
async def test_aplan_inputs() -> None
⋮----
plan_run = await portia.arun_plan(plan, plan_run_inputs=plan_inputs)
⋮----
def test_run_plan_with_large_step_input() -> None
⋮----
class StoryToolSchema(BaseModel)
class StoryTool(Tool)
⋮----
id: str = "story_tool"
name: str = "Story Tool"
description: str = "Returns the first chapter of War and Peace"
args_schema: type[BaseModel] = StoryToolSchema
output_schema: tuple[str, str] = ("str", "str: The first chapter of War and Peace")
def run(self, _: ToolRunContext) -> str
⋮----
path = Path(__file__).parent / "data" / "war_and_peace_ch1.txt"
⋮----
class EmailToolSchema(BaseModel)
⋮----
recipient: str = Field(..., description="The email address of the recipient")
subject: str = Field(..., description="The subject line of the email")
body: str = Field(..., description="The content of the email")
email_tool_called = False
class EmailTool(Tool)
⋮----
id: str = "email_tool"
name: str = "Email Tool"
description: str = "Sends an email to a recipient"
args_schema: type[BaseModel] = EmailToolSchema
output_schema: tuple[str, str] = ("str", "str: Confirmation message for the sent email")
def run(self, _: ToolRunContext, recipient: str, subject: str, body: str) -> str
⋮----
email_tool_called = True
⋮----
plan = (
portia = Portia(config=config, tools=ToolRegistry([StoryTool(), EmailTool()]))
⋮----
@pytest.mark.asyncio
async def test_arun_plan_with_large_step_input() -> None
⋮----
@pytest.mark.asyncio
async def test_llm_caching(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
original_model = cast("OpenAIGenerativeModel", config.get_planning_model())
client_mock = MagicMock()
⋮----
monkeypatch.setattr(Config, "get_planning_model", lambda self: original_model)  # noqa: ARG005
# Run the same query again
⋮----
# Check we have data in the cache
redis_client = redis.get_client()
keys = redis_client.keys()
assert len(keys) > 0, "Redis should have at least one key"  # pyright: ignore[reportArgumentType]
⋮----
for key in keys  # pyright: ignore[reportGeneralTypeIssues]
</file>

<file path="tests/integration/test_execution_agent.py">
class QuantumEntanglementSimulatorArgs(BaseModel)
⋮----
particle_count: int
simulation_time_femtoseconds: float
initial_state_vector: list[float]
measurement_basis: str = "computational"
environmental_noise_model: str = "gaussian"
noise_amplitude: float = 0.01
class QuantumEntanglementSimulatorTool(Tool)
⋮----
id: str = "quantum_entanglement_simulator"
name: str = "Quantum Entanglement Simulator"
description: str = """
args_schema: type[BaseModel] = QuantumEntanglementSimulatorArgs
output_schema: tuple[str, str] = (
def run(self, _: ToolRunContext, **__: Any) -> dict[str, Any]
async def arun(self, _: ToolRunContext, **__: Any) -> dict[str, Any]
⋮----
def test_execution_agent_with_long_tool_description(model: str) -> None
⋮----
config = Config.from_default(
tool = QuantumEntanglementSimulatorTool()
end_user = EndUser(external_id="test_user")
plan = (
plan_run = PlanRun(plan_id=plan.id, end_user_id=end_user.external_id)
agent_memory = InMemoryStorage()
agent = OneShotAgent(
output = agent.execute_sync()
⋮----
@pytest.mark.asyncio
async def test_execution_agent_with_long_tool_description_async(model: str) -> None
⋮----
output = await agent.execute_async()
</file>

<file path="tests/integration/test_mcp_session.py">
SERVER_FILE_PATH = Path(__file__).parent / "mcp_server.py"
def is_port_in_use(port: int) -> bool
⋮----
@pytest.mark.asyncio
async def test_mcp_session_stdio() -> None
⋮----
tools = await session.list_tools()
⋮----
@pytest.fixture
def sse_background_server() -> Iterator[None]
⋮----
process = subprocess.Popen(["uv", "run", "python", str(SERVER_FILE_PATH.absolute()), "sse"])
⋮----
@pytest.mark.asyncio
@pytest.mark.usefixtures("sse_background_server")
async def test_mcp_session_sse() -> None
⋮----
@pytest.fixture
def streamable_http_background_server() -> Iterator[None]
⋮----
process = subprocess.Popen(
⋮----
@pytest.mark.asyncio
@pytest.mark.usefixtures("streamable_http_background_server")
async def test_mcp_session_streamable_http() -> None
</file>

<file path="tests/integration/test_model.py">
class Response(BaseModel)
⋮----
message: str
MODELS = [
LOW_CAPABILITY_MODELS = [
⋮----
@pytest.fixture(autouse=True)
def ollama_model() -> None
⋮----
@pytest.fixture(autouse=True)
def patch_azure_model() -> Iterator[None]
⋮----
class AzureOpenAIWrapper(OpenAI)
⋮----
def __init__(self, *args: Any, **kwargs: Any) -> None
⋮----
new_kwargs = kwargs.copy()
⋮----
class AzureChatOpenAIWrapper(ChatOpenAI)
⋮----
@pytest.fixture(autouse=True)
def azure_openai_config(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
@pytest.fixture
def messages() -> list[Message]
⋮----
@pytest.mark.parametrize("model_str", MODELS + LOW_CAPABILITY_MODELS)
def test_get_response(model_str: str, messages: list[Message]) -> None
⋮----
model = Config.from_default(default_model=model_str).get_default_model()
response = model.get_response(messages)
⋮----
@pytest.mark.parametrize("model_str", MODELS + LOW_CAPABILITY_MODELS)
@pytest.mark.asyncio
async def test_aget_response(model_str: str, messages: list[Message]) -> None
⋮----
response = await model.aget_response(messages)
⋮----
@pytest.mark.parametrize("model_str", MODELS + LOW_CAPABILITY_MODELS)
@pytest.mark.flaky(reruns=4)
def test_get_structured_response(model_str: str, messages: list[Message]) -> None
⋮----
response = model.get_structured_response(messages, Response)
⋮----
@pytest.mark.parametrize("model_str", MODELS + LOW_CAPABILITY_MODELS)
@pytest.mark.flaky(reruns=4)
@pytest.mark.asyncio
async def test_aget_structured_response(model_str: str, messages: list[Message]) -> None
⋮----
response = await model.aget_structured_response(messages, Response)
⋮----
@pytest.mark.parametrize("model_str", MODELS)
def test_get_structured_response_steps_or_error(model_str: str, messages: list[Message]) -> None
⋮----
response = model.get_structured_response(messages, StepsOrError)
⋮----
response = await model.aget_structured_response(messages, StepsOrError)
⋮----
def test_google_gemini_temperature(messages: list[Message]) -> None
⋮----
config = Config.from_default(llm_provider=LLMProvider.GOOGLE)
model = GoogleGenAiGenerativeModel(
⋮----
@pytest.mark.asyncio
async def test_google_gemini_temperature_async(messages: list[Message]) -> None
</file>

<file path="tests/integration/test_plan_v2.py">
class CommodityPrice(BaseModel)
⋮----
price: float
class CommodityPriceWithCurrency(BaseModel)
⋮----
currency: str
class CurrencyConversionResult(BaseModel)
⋮----
converted_amount: str
class CurrencyConversionToolSchema(BaseModel)
⋮----
amount: CommodityPrice = Field(..., description="The amount to convert")
currency_from: str = Field(..., description="The currency to convert from")
currency_to: str = Field(..., description="The currency to convert to")
class CurrencyConversionTool(Tool[CurrencyConversionResult])
⋮----
id: str = "currency_conversion_tool"
name: str = "Currency conversion tool"
description: str = "Converts money between currencies"
args_schema: type[BaseModel] = CurrencyConversionToolSchema
output_schema: tuple[str, str] = ("CurrencyConversionResult", "The converted amount")
⋮----
converted_amount = f"{amount.price * 1.2} {currency_to}"
⋮----
class FinalOutput(BaseModel)
⋮----
poem: str
example_similar_poem: str
⋮----
@pytest.mark.parametrize("is_async", [False, True])
def test_example_builder(is_async: bool) -> None
⋮----
config = Config.from_default(
portia = Portia(config=config)
plan = (
⋮----
plan_run = asyncio.run(portia.arun_plan(plan, plan_run_inputs={"purchase_quantity": 100}))
⋮----
plan_run = portia.run_plan(plan, plan_run_inputs={"purchase_quantity": 100})
⋮----
final_output = plan_run.outputs.final_output.get_value()
⋮----
def test_plan_v2_conditionals() -> None
⋮----
config = Config.from_default(storage_class=StorageClass.CLOUD)
⋮----
messages: list[str] = []
def record_func(message: str) -> None
⋮----
plan_run = portia.run_plan(plan)
⋮----
def test_plan_v2_conditionals_else_if() -> None
def test_plan_v2_conditionals_else() -> None
def test_plan_v2_conditionals_nested_branches() -> None
def test_plan_v2_conditionals_nested_branches_else_if() -> None
def test_plan_v2_unclosed_conditionals() -> None
def test_plan_v2_unclosed_conditionals_complex() -> None
def test_plan_v2_conditional_if_without_else_if() -> None
def test_plan_v2_conditional_if_without_else() -> None
def test_plan_v2_conditional_if_without_else_if_or_else() -> None
def test_plan_v2_legacy_condition_string() -> None
⋮----
def dummy(message: str) -> None
def evals_true() -> bool
⋮----
condition_strings = [s.to_legacy_step(plan).condition for s in plan.steps]
</file>

<file path="tests/integration/test_portia_cloud.py">
def test_portia_run_query_with_cloud() -> None
⋮----
config = Config.from_default(storage_class=StorageClass.CLOUD)
portia = Portia(config=config)
query = "Where is the next Olympics being hosted?"
plan_run = portia.run(query)
⋮----
storage = portia.storage
⋮----
def test_run_tool_error() -> None
⋮----
registry = PortiaToolRegistry(
⋮----
tool = registry.get_tool("portia:tavily::search")
⋮----
ctx = get_test_tool_context()
⋮----
def test_portia_run_query_with_cloud_and_local() -> None
⋮----
registry = ToolRegistry([AdditionTool()]) + PortiaToolRegistry(
portia = Portia(config=config, tools=registry)
query = "Get the temperature in London and Sydney and then add the two temperatures together."
⋮----
def test_portia_run_query_with_oauth() -> None
⋮----
portia = Portia()
query = "Star the portiaai/portia-sdk-repo"
plan_run = portia.run(query, end_user=EndUser(external_id=str(uuid.uuid4())))
⋮----
def test_portia_cloud_storage() -> None
⋮----
config = Config.from_default()
storage = PortiaCloudStorage(config)
⋮----
def test_default_portia_has_correct_tools() -> None
⋮----
tools = portia.tool_registry.get_tools()
⋮----
def test_portia_end_user_update_with_cloud() -> None
⋮----
portia = Portia(
query = "Update the name of my end user to Jeff"
plan_run = portia.run(query, end_user=EndUser(external_id="123", name="Bob"))
⋮----
end_user = storage.get_end_user("123")
</file>

<file path="tests/integration/test_runner_context.py">
class ExecutionContextTrackerTool(Tool)
⋮----
id: str = "execution_tracker_tool"
name: str = "Execution Tracker Tool"
description: str = "Tracks tool execution context"
output_schema: tuple[str, str] = (
tool_context: ToolRunContext | None = None
⋮----
def get_test_plan_run() -> tuple[Plan, PlanRun]
⋮----
step1 = Step(
plan = Plan(
</file>

<file path="tests/integration/test_summariser.py">
config = Config.from_default(
summarizer = StepSummarizer(
summary = summarizer.invoke(
</file>

<file path="tests/unit/builder/test_plan_builder_v2.py">
class OutputSchema(BaseModel)
⋮----
result: str
count: int
def example_function_for_testing(x: int, y: str) -> str
class MockTool(Tool)
⋮----
def __init__(self) -> None
def run(self, ctx: Any, **kwargs: Any) -> str
class TestPlanBuilderV2
⋮----
def test_initialization_default_label(self) -> None
⋮----
builder = PlanBuilderV2()
⋮----
def test_initialization_custom_label(self) -> None
⋮----
custom_label = "Custom Plan Label"
builder = PlanBuilderV2(label=custom_label)
⋮----
def test_input_method(self) -> None
⋮----
result = builder.input(name="user_name")
⋮----
def test_input_method_with_description(self) -> None
def test_input_method_multiple_inputs(self) -> None
def test_input_method_with_default_value(self) -> None
def test_input_method_with_various_default_values(self) -> None
⋮----
default_bool = True
⋮----
inputs = {inp.name: inp for inp in builder.plan.plan_inputs}
⋮----
def test_llm_step_method_basic(self) -> None
⋮----
result = builder.llm_step(task="Analyze the data")
assert result is builder  # Should return self for chaining
⋮----
def test_llm_step_method_with_all_parameters(self) -> None
⋮----
inputs = ["input1", StepOutput(0), Input("user_input")]
⋮----
step = builder.plan.steps[0]
⋮----
def test_llm_step_method_auto_generated_step_name(self) -> None
def test_invoke_tool_step_method_with_string_tool(self) -> None
⋮----
args = {"param1": "value1", "param2": StepOutput(0)}
result = builder.invoke_tool_step(tool="search_tool", args=args)
⋮----
def test_invoke_tool_step_method_with_tool_instance(self) -> None
⋮----
mock_tool = MockTool()
⋮----
def test_invoke_tool_step_method_with_all_parameters(self) -> None
def test_invoke_tool_step_method_no_args(self) -> None
def test_function_step_method_basic(self) -> None
⋮----
result = builder.function_step(function=example_function_for_testing)
⋮----
def test_function_step_method_with_all_parameters(self) -> None
⋮----
args = {"x": 42, "y": Input("user_input")}
⋮----
def test_single_tool_agent_step_method_basic(self) -> None
⋮----
result = builder.single_tool_agent_step(tool="agent_tool", task="Complete the task")
⋮----
def test_single_tool_agent_step_method_with_all_parameters(self) -> None
⋮----
inputs = ["context", StepOutput(0)]
⋮----
def test_final_output_method_basic(self) -> None
⋮----
result = builder.final_output()
⋮----
def test_final_output_method_with_schema(self) -> None
def test_final_output_method_with_summarize(self) -> None
def test_final_output_method_with_all_parameters(self) -> None
def test_build_method(self) -> None
⋮----
builder = PlanBuilderV2(label="Test Plan")
⋮----
plan = builder.build()
⋮----
assert plan is builder.plan  # Should return the same instance
⋮----
def test_method_chaining(self) -> None
⋮----
builder = PlanBuilderV2("Chaining Test")
result = (
⋮----
# Verify the plan was built correctly
⋮----
# Verify default values are set correctly
inputs = {inp.name: inp for inp in plan.plan_inputs}
⋮----
def test_empty_plan_build(self) -> None
def test_step_name_generation_with_mixed_steps(self) -> None
def test_custom_step_names_override_auto_generation(self) -> None
⋮----
builder.llm_step(task="Second")  # Should get step_1
⋮----
def test_references_in_inputs_and_args(self) -> None
⋮----
# Add inputs to reference
⋮----
# Add steps with references
⋮----
# Verify references are preserved
llm_step = plan.steps[0]
⋮----
tool_step = plan.steps[1]
⋮----
func_step = plan.steps[2]
</file>

<file path="tests/unit/builder/test_plan_v2.py">
class OutputSchema(BaseModel)
⋮----
result: str
count: int
class MockStepV2(StepV2)
⋮----
def __init__(self, step_name: str = "mock_step") -> None
async def run(self, run_data: Any) -> str
def describe(self) -> str
def to_legacy_step(self, plan: PlanV2) -> Step
class TestPlanV2
⋮----
def test_initialization_default_values(self) -> None
⋮----
plan = PlanV2(steps=[])
⋮----
def test_initialization_custom_values(self) -> None
⋮----
mock_step = MockStepV2("custom_step")
plan_input = PlanInput(name="test_input", description="Test input description")
plan = PlanV2(
⋮----
def test_to_legacy_plan_basic(self) -> None
⋮----
mock_step = MockStepV2("test_step")
plan_input = PlanInput(name="input1", description="Test input")
plan_context = PlanContext(query="Test query", tool_ids=["mock_tool"])
⋮----
legacy_plan = plan.to_legacy_plan(plan_context)
⋮----
def test_to_legacy_plan_multiple_steps(self) -> None
⋮----
step1 = MockStepV2("step_1")
step2 = MockStepV2("step_2")
plan_context = PlanContext(query="Multi-step query", tool_ids=["mock_tool"])
plan = PlanV2(steps=[step1, step2])
⋮----
def test_step_output_name_with_step_index(self) -> None
⋮----
step1 = MockStepV2("first_step")
step2 = MockStepV2("second_step")
⋮----
def test_step_output_name_with_step_name(self) -> None
⋮----
step1 = MockStepV2("custom_step_name")
step2 = MockStepV2("another_step")
⋮----
def test_step_output_name_with_step_instance(self) -> None
⋮----
step1 = MockStepV2("instance_step")
step2 = MockStepV2("another_instance")
⋮----
def test_step_output_name_invalid_step_index(self) -> None
⋮----
plan = PlanV2(steps=[MockStepV2("test_step")])
result = plan.step_output_name(999)
⋮----
def test_step_output_name_invalid_step_name(self) -> None
⋮----
plan = PlanV2(steps=[MockStepV2("valid_step")])
⋮----
result = plan.step_output_name("nonexistent_step")
⋮----
def test_step_output_name_step_not_in_plan(self) -> None
⋮----
plan = PlanV2(steps=[MockStepV2("in_plan")])
external_step = MockStepV2("not_in_plan")
⋮----
result = plan.step_output_name(external_step)
⋮----
def test_idx_by_name_valid_names(self) -> None
⋮----
step1 = MockStepV2("first")
step2 = MockStepV2("second")
step3 = MockStepV2("third")
plan = PlanV2(steps=[step1, step2, step3])
⋮----
def test_idx_by_name_invalid_name(self) -> None
⋮----
plan = PlanV2(steps=[MockStepV2("existing_step")])
⋮----
def test_idx_by_name_empty_plan(self) -> None
def test_plan_with_real_step_types(self) -> None
⋮----
llm_step = LLMStep(
tool_step = InvokeToolStep(
plan = PlanV2(steps=[llm_step, tool_step])
⋮----
def test_plan_with_no_steps(self) -> None
⋮----
result = plan.step_output_name(0)
⋮----
def test_plan_id_generation(self) -> None
⋮----
plan1 = PlanV2(steps=[])
plan2 = PlanV2(steps=[])
⋮----
def test_plan_with_complex_configuration(self) -> None
⋮----
steps: list[StepV2] = [
inputs = [
⋮----
plan_context = PlanContext(
⋮----
def test_validation_duplicate_step_names(self) -> None
def test_validation_duplicate_plan_input_names(self) -> None
def test_validation_multiple_duplicate_step_names(self) -> None
def test_validation_multiple_duplicate_input_names(self) -> None
def test_validation_no_duplicates_passes(self) -> None
⋮----
plan = PlanV2(steps=steps, plan_inputs=inputs)
⋮----
def test_validation_empty_plan_passes(self) -> None
⋮----
plan = PlanV2(steps=[], plan_inputs=[])
</file>

<file path="tests/unit/builder/test_reference.py">
class TestDefaultStepName
⋮----
def test_default_step_name_zero_index(self) -> None
⋮----
result = default_step_name(0)
⋮----
def test_default_step_name_positive_indices(self) -> None
def test_default_step_name_large_index(self) -> None
⋮----
result = default_step_name(123456)
⋮----
class TestStepOutput
⋮----
def test_step_output_initialization_with_int(self) -> None
⋮----
step_output = StepOutput(5)
⋮----
def test_step_output_initialization_with_string(self) -> None
⋮----
step_output = StepOutput("my_step")
⋮----
def test_step_output_str_representation_int(self) -> None
⋮----
step_output = StepOutput(3)
result = str(step_output)
⋮----
def test_step_output_str_representation_string(self) -> None
⋮----
step_output = StepOutput("custom_step")
⋮----
def test_get_legacy_name_with_int_step(self) -> None
⋮----
step_output = StepOutput(2)
mock_plan = Mock(spec=PlanV2)
⋮----
result = step_output.get_legacy_name(mock_plan)
⋮----
def test_get_legacy_name_with_string_step(self) -> None
⋮----
step_output = StepOutput("named_step")
⋮----
def test_get_value_with_int_step_success(self) -> None
⋮----
step_output = StepOutput(1)
test_output = LocalDataValue(value="test result", summary="Test output")
mock_reference_value = ReferenceValue(value=test_output, description="Test output")
mock_run_data = Mock()
⋮----
result = step_output.get_value(mock_run_data)
⋮----
def test_get_value_with_string_step_success(self) -> None
def test_get_value_with_int_step_index_error(self) -> None
def test_get_value_with_string_step_value_error(self) -> None
⋮----
step_output = StepOutput("nonexistent_step")
⋮----
class TestInput
⋮----
def test_input_initialization(self) -> None
⋮----
input_ref = Input("user_name")
⋮----
def test_input_str_representation(self) -> None
⋮----
input_ref = Input("api_key")
result = str(input_ref)
⋮----
def test_get_legacy_name(self) -> None
⋮----
input_ref = Input("my_input")
result = input_ref.get_legacy_name(Mock(spec=PlanV2))
⋮----
def test_get_value_success(self) -> None
⋮----
# Create mock plan input
mock_plan_input = PlanInput(name="user_name", description="The user's name")
# Create mock output value
test_output = LocalDataValue(value="John Doe", summary="User name")
# Create mock run data
⋮----
result = input_ref.get_value(mock_run_data)
⋮----
def test_get_value_success_no_description(self) -> None
⋮----
# Create mock plan input without description
mock_plan_input = PlanInput(name="api_key", description=None)
⋮----
test_output = LocalDataValue(value="secret-key-123", summary="API key")
⋮----
assert result.description == "Input to plan"  # Default description
def test_get_value_input_not_found_in_plan(self) -> None
⋮----
input_ref = Input("missing_input")
# Create mock run data with different input
mock_plan_input = PlanInput(name="other_input", description="Other input")
⋮----
def test_get_value_value_not_found_in_run_inputs(self) -> None
⋮----
# Create mock run data without the value in plan_run_inputs
⋮----
mock_run_data.plan_run.plan_run_inputs = {}  # Empty inputs
⋮----
def test_get_value_value_is_none_in_run_inputs(self) -> None
⋮----
input_ref = Input("optional_input")
⋮----
mock_plan_input = PlanInput(name="optional_input", description="Optional input")
# Create mock run data with None value
⋮----
class TestReferenceValue
⋮----
def test_reference_value_initialization(self) -> None
⋮----
test_output = LocalDataValue(value="test data", summary="Test summary")
ref_value = ReferenceValue(value=test_output, description="Test description")
⋮----
def test_reference_value_default_description(self) -> None
⋮----
ref_value = ReferenceValue(value=test_output)
⋮----
class TestIntegration
⋮----
def test_step_output_and_input_with_real_plan(self) -> None
⋮----
# Create a real plan with steps and inputs
step = LLMStep(task="Test task", step_name="test_step")
plan_input = PlanInput(name="test_input", description="Test input")
plan = PlanV2(steps=[step], plan_inputs=[plan_input])
# Test StepOutput
step_output = StepOutput(0)
legacy_name = step_output.get_legacy_name(plan)
⋮----
step_output_by_name = StepOutput("test_step")
legacy_name_by_name = step_output_by_name.get_legacy_name(plan)
⋮----
# Test Input
input_ref = Input("test_input")
legacy_input_name = input_ref.get_legacy_name(plan)
⋮----
def test_multiple_inputs_and_outputs(self) -> None
⋮----
# Create plan with multiple steps and inputs
steps: list[StepV2] = [
inputs = [
plan = PlanV2(steps=steps, plan_inputs=inputs)
# Test various StepOutput references
⋮----
# Test Input references
</file>

<file path="tests/unit/builder/test_step_v2.py">
class MockOutputSchema(BaseModel)
⋮----
result: str
count: int
def example_function(x: int, y: str) -> str
class MockTool(Tool)
⋮----
def __init__(self) -> None
def run(self, ctx: Any, **kwargs: Any) -> str
class ConcreteStepV2(StepV2)
⋮----
def __init__(self, step_name: str = "test_step") -> None
async def run(self, run_data: Any) -> str
def describe(self) -> str
def to_legacy_step(self, plan: PlanV2) -> PlanStep
class TestStepV2Base
⋮----
def test_step_v2_initialization(self) -> None
⋮----
step = ConcreteStepV2("my_step")
⋮----
def test_resolve_input_reference_with_non_reference(self) -> None
⋮----
step = ConcreteStepV2()
mock_run_data = Mock()
result = step._resolve_input_reference("plain_string", mock_run_data)
⋮----
result = step._resolve_input_reference(42, mock_run_data)
⋮----
def test_resolve_input_reference_with_reference(self) -> None
⋮----
reference = StepOutput(0)
⋮----
result = step._resolve_input_reference(reference, mock_run_data)
⋮----
def test_resolve_input_reference_with_string_template_step_output(self) -> None
def test_resolve_input_reference_with_string_template_input(self) -> None
def test_get_value_for_input_with_reference_value(self) -> None
⋮----
reference_input = StepOutput(0)
mock_data_value = LocalDataValue(value="extracted_value")
mock_reference_value = ReferenceValue(value=mock_data_value, description="Step 0")
⋮----
result = step._get_value_for_input(reference_input, mock_run_data)
⋮----
def test_get_value_for_input_with_regular_value(self) -> None
⋮----
result = step._get_value_for_input("regular_value", mock_run_data)
⋮----
def test_resolve_input_names_for_printing_with_reference(self) -> None
⋮----
mock_plan = Mock()
⋮----
result = step._resolve_input_names_for_printing(reference, mock_plan)
⋮----
def test_resolve_input_names_for_printing_with_reference_already_prefixed(self) -> None
⋮----
mock_reference = StepOutput(0)
⋮----
result = step._resolve_input_names_for_printing(mock_reference, mock_plan)
⋮----
def test_resolve_input_names_for_printing_with_list(self) -> None
⋮----
reference = Input("test_input")
⋮----
input_list = ["regular_value", reference, 42]
result = step._resolve_input_names_for_printing(input_list, mock_plan)
⋮----
def test_resolve_input_names_for_printing_with_regular_value(self) -> None
⋮----
result = step._resolve_input_names_for_printing("regular_value", mock_plan)
⋮----
def test_inputs_to_legacy_plan_variables(self) -> None
⋮----
ref1 = Input("test_input")
ref2 = StepOutput(0)
⋮----
inputs = ["regular_value", ref1, 42, ref2]
result = step._inputs_to_legacy_plan_variables(inputs, mock_plan)
⋮----
class TestLLMStep
⋮----
def test_llm_step_initialization(self) -> None
⋮----
step = LLMStep(task="Test task", step_name="llm_step")
⋮----
def test_llm_step_initialization_with_all_parameters(self) -> None
⋮----
inputs = [Input("user_query"), "additional context"]
step = LLMStep(
⋮----
def test_llm_step_describe(self) -> None
⋮----
step = LLMStep(task="Test task", step_name="test")
⋮----
def test_llm_step_describe_with_output_schema(self) -> None
⋮----
step = LLMStep(task="Test task", step_name="test", output_schema=MockOutputSchema)
⋮----
@pytest.mark.asyncio
    async def test_llm_step_run_no_inputs(self) -> None
⋮----
step = LLMStep(task="Analyze data", step_name="analysis")
⋮----
mock_llm_tool = Mock()
⋮----
result = await step.run(mock_run_data)
⋮----
call_args = mock_llm_tool.arun.call_args
⋮----
@pytest.mark.asyncio
    async def test_llm_step_run_one_regular_input(self) -> None
⋮----
step = LLMStep(task="Process text", step_name="process", inputs=["Hello world"])
⋮----
@pytest.mark.asyncio
    async def test_llm_step_run_one_reference_input(self) -> None
⋮----
step = LLMStep(task="Summarize result", step_name="summarize", inputs=[reference_input])
⋮----
mock_data_value = LocalDataValue(value="Previous step result")
⋮----
expected_task_data = "Previous step Step 0 had output: Previous step result"
⋮----
@pytest.mark.asyncio
    async def test_llm_step_run_mixed_inputs(self) -> None
⋮----
ref1 = Input("user_name")
ref2 = StepOutput(1)
⋮----
mock_data_value1 = LocalDataValue(value="John")
mock_ref1_value = ReferenceValue(value=mock_data_value1, description="User input")
mock_data_value2 = LocalDataValue(value="Analysis complete")
mock_ref2_value = ReferenceValue(value=mock_data_value2, description="Step 1")
⋮----
expected_task_data = [
⋮----
def test_llm_step_to_legacy_step(self) -> None
⋮----
inputs = [Input("user_query"), StepOutput(0)]
⋮----
legacy_step = step.to_legacy_step(mock_plan)
⋮----
class TestInvokeToolStep
⋮----
def test_invoke_tool_step_initialization_with_string_tool(self) -> None
⋮----
step = InvokeToolStep(tool="search_tool", step_name="search")
⋮----
def test_invoke_tool_step_initialization_with_tool_instance(self) -> None
⋮----
mock_tool = MockTool()
args = {"query": "test", "limit": StepOutput(0)}
step = InvokeToolStep(
⋮----
def test_invoke_tool_step_describe_with_string_tool(self) -> None
⋮----
result = step.describe()
⋮----
def test_invoke_tool_step_describe_with_tool_instance(self) -> None
⋮----
expected = "InvokeToolStep(tool='mock_tool', args={'query': 'test'} -> MockOutputSchema)"
⋮----
def test_tool_name_with_string_tool(self) -> None
def test_tool_name_with_tool_instance(self) -> None
⋮----
step = InvokeToolStep(tool=mock_tool, step_name="search")
⋮----
@pytest.mark.asyncio
    async def test_invoke_tool_step_with_regular_value_input(self) -> None
⋮----
step = InvokeToolStep(tool="mock_tool", step_name="run_tool", args={"query": "search term"})
⋮----
mock_tool = Mock()
⋮----
call_args = mock_tool.run.call_args
⋮----
@pytest.mark.asyncio
    async def test_invoke_tool_step_with_regular_value_input_and_output_schema(self) -> None
⋮----
mock_model = Mock()
⋮----
@pytest.mark.asyncio
    async def test_invoke_tool_step_with_reference_input(self) -> None
⋮----
mock_data_value = LocalDataValue(value="previous step output")
⋮----
@pytest.mark.asyncio
    async def test_invoke_tool_step_with_mixed_inputs(self) -> None
⋮----
ref1 = Input("user_query")
⋮----
mock_data_value1 = LocalDataValue(value="user question")
⋮----
mock_data_value2 = LocalDataValue(value="step 1 output")
⋮----
call_args = mock_tool.run.call_args[1]
⋮----
@pytest.mark.asyncio
    async def test_invoke_tool_step_no_args_with_clarification(self) -> None
⋮----
step = InvokeToolStep(tool="mock_tool", step_name="run_tool", args={})
⋮----
mock_clarification = Clarification(
⋮----
@pytest.mark.asyncio
    async def test_invoke_tool_step_with_tool_instance(self) -> None
⋮----
step = InvokeToolStep(tool=mock_tool, step_name="run_tool", args={"input": "test input"})
⋮----
@pytest.mark.asyncio
    async def test_invoke_tool_step_with_nonexistent_tool_id(self) -> None
⋮----
step = InvokeToolStep(tool="nonexistent_tool", step_name="run_tool", args={"query": "test"})
⋮----
def test_invoke_tool_step_to_legacy_step(self) -> None
⋮----
args = {"query": Input("user_query"), "limit": 10}
⋮----
class TestFunctionStep
⋮----
def test_function_step_initialization(self) -> None
⋮----
args = {"x": 42, "y": Input("user_input")}
step = FunctionStep(
⋮----
def test_function_step_describe(self) -> None
⋮----
expected = "FunctionStep(function='example_function', args={'x': 42, 'y': 'test'})"
⋮----
def test_function_step_describe_with_output_schema(self) -> None
⋮----
expected = "FunctionStep(function='example_function', args={'x': 42} -> MockOutputSchema)"
⋮----
@pytest.mark.asyncio
    async def test_function_step_with_normal_arg(self) -> None
⋮----
@pytest.mark.asyncio
    async def test_function_step_with_reference_arg(self) -> None
⋮----
mock_data_value = LocalDataValue(value="Previous step")
⋮----
@pytest.mark.asyncio
    async def test_function_step_no_args_with_clarification(self) -> None
⋮----
def clarification_function() -> Clarification
step = FunctionStep(function=clarification_function, step_name="clarify", args={})
⋮----
@pytest.mark.asyncio
    async def test_function_step_no_args_with_output_schema(self) -> None
⋮----
def raw_output_function() -> str
⋮----
call_args = mock_model.aget_structured_response.call_args
messages = call_args[0][0]
⋮----
expected_content = "Convert this output to the desired schema: raw function output"
⋮----
def test_function_step_to_legacy_step(self) -> None
⋮----
args = {"x": Input("number"), "y": "constant"}
⋮----
class TestSingleToolAgent
⋮----
def test_single_tool_agent_initialization(self) -> None
⋮----
inputs = [Input("query"), "context"]
step = SingleToolAgentStep(
⋮----
def test_single_tool_agent_describe(self) -> None
⋮----
expected = "SingleToolAgentStep(tool='search_tool', query='Search for info')"
⋮----
def test_single_tool_agent_describe_with_output_schema(self) -> None
⋮----
expected = (
⋮----
@pytest.mark.asyncio
    async def test_single_tool_agent_run_with_mocked_agent(self) -> None
⋮----
mock_agent = Mock()
mock_output_obj = Mock()
⋮----
call_args = mock_get_agent.call_args[0]
legacy_step = call_args[0]
⋮----
def test_single_tool_agent_to_legacy_step(self) -> None
⋮----
inputs = [Input("query"), StepOutput(0)]
</file>

<file path="tests/unit/execution_agents/utils/test_final_output_summarizer.py">
@pytest.fixture
def mock_summarizer_model() -> mock.MagicMock
⋮----
model = mock.MagicMock(spec=GenerativeModel)
⋮----
@pytest.fixture
def summarizer_config(mock_summarizer_model: mock.MagicMock) -> Config
⋮----
expected_summary = "Weather is sunny and warm in London, visit to Hyde Park for a picnic"
⋮----
summarizer = FinalOutputSummarizer(config=summarizer_config, agent_memory=InMemoryStorage())
output = summarizer.create_summary(plan=plan, plan_run=plan_run)
⋮----
expected_context = (
expected_prompt = FinalOutputSummarizer.summarizer_only_prompt + expected_context
⋮----
expected_prompt = FinalOutputSummarizer.summarizer_only_prompt + (
⋮----
context = summarizer._build_tasks_and_outputs_context(
⋮----
def test_build_tasks_and_outputs_context_empty() -> None
⋮----
summarizer = FinalOutputSummarizer(config=get_test_config(), agent_memory=InMemoryStorage())
⋮----
def test_build_tasks_and_outputs_context_partial_outputs() -> None
def test_build_tasks_and_outputs_context_with_conditional_outcomes() -> None
⋮----
class TestStructuredOutput(BaseModel)
⋮----
mock_field: str
⋮----
class SchemaWithSummary(TestStructuredOutput)
⋮----
fo_summary: str
mock_response = SchemaWithSummary(mock_field="mock_value", fo_summary="mock_summary")
⋮----
expected_prompt = (
⋮----
large_memory_output = AgentMemoryValue(
⋮----
mock_memory = mock.MagicMock()
large_retrieved_output = LocalDataValue(
⋮----
summarizer = FinalOutputSummarizer(config=summarizer_config, agent_memory=mock_memory)
</file>

<file path="tests/unit/execution_agents/utils/test_step_summarizer.py">
class TestError(Exception)
def test_summarizer_model_normal_output() -> None
⋮----
summary = AIMessage(content="Short summary")
tool = AdditionTool()
mock_model = get_mock_generative_model(response=summary)
ai_message = AIMessage(content="", tool_calls=[{"id": "123", "name": tool.name, "args": {}}])
tool_message = ToolMessage(
summarizer_model = StepSummarizer(
base_chat_model = mock_model.to_langchain()
result = summarizer_model.invoke({"messages": [ai_message, tool_message]})
⋮----
messages = base_chat_model.invoke.call_args[0][0]
⋮----
output_message = result["messages"][0]
⋮----
@pytest.mark.asyncio
async def test_summarizer_model_normal_output_async() -> None
⋮----
result = await summarizer_model.ainvoke({"messages": [ai_message, tool_message]})
⋮----
messages = base_chat_model.ainvoke.call_args[0][0]
⋮----
def test_summarizer_model_non_tool_message() -> None
⋮----
mock_model = get_mock_generative_model()
ai_message = AIMessage(content="AI message content")
⋮----
result = summarizer_model.invoke({"messages": [ai_message]})
⋮----
@pytest.mark.asyncio
async def test_summarizer_model_non_tool_message_async() -> None
⋮----
result = await summarizer_model.ainvoke({"messages": [ai_message]})
⋮----
def test_summarizer_model_no_messages() -> None
⋮----
result = summarizer_model.invoke({"messages": []})
⋮----
@pytest.mark.asyncio
async def test_summarizer_model_no_messages_async() -> None
⋮----
result = await summarizer_model.ainvoke({"messages": []})
⋮----
def test_summarizer_model_large_output() -> None
⋮----
ai_message = AIMessage(
⋮----
@pytest.mark.asyncio
async def test_summarizer_model_large_output_async() -> None
def test_summarizer_model_error_handling() -> None
⋮----
class TestError(Exception)
⋮----
result = summarizer_model.invoke({"messages": [tool_message]})
⋮----
@pytest.mark.asyncio
async def test_summarizer_model_error_handling_async() -> None
⋮----
result = await summarizer_model.ainvoke({"messages": [tool_message]})
⋮----
def test_summarizer_model_structured_output_schema() -> None
⋮----
class AdditionOutput(BaseModel)
⋮----
result: int
so_summary: str
⋮----
mock_model = MagicMock()
output = AdditionOutput(result=3, so_summary="Short summary")
⋮----
@pytest.mark.asyncio
async def test_summarizer_model_structured_output_schema_async() -> None
⋮----
async def mock_aget_structured_response(*_args: Any, **_kwargs: Any) -> AdditionOutput
⋮----
def test_summarizer_model_structured_output_schema_error_fallback() -> None
⋮----
summary = AIMessage(content="Short Summary")
⋮----
@pytest.mark.asyncio
async def test_summarizer_model_structured_output_schema_error_fallback_async() -> None
⋮----
async def mock_aget_structured_response(*_args: Any, **_kwargs: Any) -> None
async def mock_aget_response(*_args: Any, **_kwargs: Any) -> AIMessage
⋮----
def test_summarizer_model_multiple_tool_calls() -> None
⋮----
tool_call_1_id = "123"
tool_call_2_id = "456"
tool_call_1_args = {"a": 1, "b": 2}
tool_call_2_args = {"a": 3, "b": 4}
⋮----
tool_message_1 = ToolMessage(
tool_message_2 = ToolMessage(
⋮----
result = summarizer_model.invoke({"messages": [ai_message, tool_message_1, tool_message_2]})
⋮----
prompt_content = messages[1].content
⋮----
@pytest.mark.asyncio
async def test_summarizer_model_multiple_tool_calls_async() -> None
⋮----
result = await summarizer_model.ainvoke(
</file>

<file path="tests/unit/execution_agents/test_base_execution_agent.py">
class TestBaseExecutionAgent(BaseExecutionAgent)
⋮----
def execute_sync(self) -> Output
def test_base_agent_default_context() -> None
⋮----
agent = BaseExecutionAgent(
context = agent.get_system_context(
⋮----
@pytest.mark.asyncio
async def test_base_agent_execute_async_calls_sync() -> None
⋮----
agent = TestBaseExecutionAgent(
result = await agent.execute_async()
⋮----
def test_output_serialize() -> None
⋮----
class MyModel(BaseModel)
⋮----
id: str
class NotAModel
⋮----
def __init__(self, id: str) -> None
not_a_model = NotAModel(id="123")
now = datetime.now(tz=UTC)
clarification = ActionClarification(
tcs: list[tuple[Any, Any]] = [
⋮----
output = LocalDataValue(value=tc[0]).serialize_value()
⋮----
def test_next_state_after_tool_call_no_error() -> None
⋮----
execution_hooks = ExecutionHooks(
⋮----
messages: list[ToolMessage] = [
state: MessagesState = {"messages": messages}
result = agent.next_state_after_tool_call(agent.config, state, agent.tool)
⋮----
def test_next_state_after_tool_call_with_summarize() -> None
⋮----
tool = AdditionTool()
⋮----
result = agent.next_state_after_tool_call(agent.config, state, tool)
⋮----
def test_next_state_after_tool_call_with_large_output() -> None
def test_next_state_after_tool_call_with_error_retry() -> None
⋮----
result = agent.next_state_after_tool_call(agent.config, state)
expected_state = END if i == MAX_RETRIES else AgentNode.TOOL_AGENT
⋮----
def test_next_state_after_tool_call_with_clarification_artifact() -> None
⋮----
clarification = InputClarification(
⋮----
def test_next_state_after_tool_call_with_list_of_clarifications() -> None
⋮----
clarifications = [
</file>

<file path="tests/unit/execution_agents/test_clarification_tool.py">
def test_clarification_tool_raises_clarification() -> None
⋮----
tool = ClarificationTool(step=plan_run.current_step_index)
ctx = ToolRunContext(
argument_name = "test_argument"
result = tool.run(ctx, argument_name)
clarification = InputClarification.model_validate_json(result)
</file>

<file path="tests/unit/execution_agents/test_context.py">
@pytest.fixture
def inputs() -> list[Variable]
⋮----
@pytest.fixture
def outputs() -> dict[str, Output]
def test_context_empty() -> None
⋮----
context = build_context(
⋮----
def test_context_execution_context() -> None
def test_context_inputs_and_outputs(inputs: list[Variable], outputs: dict[str, Output]) -> None
⋮----
val = output.get_value()
⋮----
def test_all_contexts(inputs: list[Variable], outputs: dict[str, Output]) -> None
⋮----
clarifications = [
</file>

<file path="tests/unit/execution_agents/test_default_execution_agent.py">
@pytest.fixture(scope="session", autouse=True)
def _setup() -> None
class _TestToolSchema(BaseModel)
⋮----
content: str = Field(..., description="INPUT_DESCRIPTION")
def test_parser_model() -> None
⋮----
tool_inputs = ToolInputs(
mock_model = get_mock_base_chat_model(response=tool_inputs)
agent = SimpleNamespace(
⋮----
parser_model = ParserModel(
⋮----
messages = mock_model.invoke.call_args[0][0]
⋮----
def test_parser_model_with_retries() -> None
⋮----
mock_invoker = get_mock_base_chat_model(response=tool_inputs)
⋮----
def test_parser_model_with_retries_invalid_structured_response() -> None
⋮----
mock_model = get_mock_base_chat_model(
⋮----
def test_parser_model_with_invalid_args() -> None
⋮----
invalid_tool_inputs = ToolInputs(
valid_tool_inputs = ToolInputs(
responses = [invalid_tool_inputs, valid_tool_inputs]
current_response_index = 0
def mock_invoke(*_, **__)
⋮----
response = responses[current_response_index]
⋮----
mock_model = get_mock_base_chat_model(response=None)
⋮----
class TestSchema(BaseModel)
⋮----
content: str
number: int
⋮----
result = parser_model.invoke({"messages": [], "step_inputs": []})
⋮----
result_inputs = ToolInputs.model_validate_json(result["messages"][0])
⋮----
content_arg = next(arg for arg in result_inputs.args if arg.name == "content")
number_arg = next(arg for arg in result_inputs.args if arg.name == "number")
⋮----
def test_parser_model_schema_validation_success_with_templating() -> None
⋮----
class ComplexSchema(BaseModel)
⋮----
email_list: list[str] = Field(..., description="List of email addresses")
config_dict: dict[str, str] = Field(..., description="Configuration dictionary")
⋮----
def test_parser_model_schema_validation_failure_with_templating() -> None
⋮----
class EmailSchema(BaseModel)
⋮----
email: str = Field(..., pattern=r"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$")
⋮----
def test_verifier_model() -> None
⋮----
verified_tool_inputs = VerifiedToolInputs(
mock_model = get_mock_base_chat_model(response=verified_tool_inputs)
⋮----
verifier_model = VerifierModel(
⋮----
def test_verifier_model_schema_validation() -> None
⋮----
required_field1: str
required_field2: int
optional_field: str | None = None
⋮----
result = verifier_model.invoke(
result_inputs = VerifiedToolInputs.model_validate_json(result["messages"][0])
required_field1 = next(arg for arg in result_inputs.args if arg.name == "required_field1")
required_field2 = next(arg for arg in result_inputs.args if arg.name == "required_field2")
⋮----
optional_field = next(arg for arg in result_inputs.args if arg.name == "optional_field")
⋮----
def test_verifier_model_validates_schema_with_templating() -> None
⋮----
output_args = VerifiedToolInputs.model_validate_json(result["messages"][0])
⋮----
def test_tool_calling_model_no_hallucinations() -> None
⋮----
mock_model = get_mock_generative_model(
⋮----
mock_before_tool_call = mock.MagicMock(return_value=None)
mock_telemetry = mock.MagicMock()
⋮----
tool_calling_model = ToolCallingModel(
⋮----
base_chat_model = mock_model.to_langchain()
⋮----
messages = base_chat_model.invoke.call_args[0][0]
⋮----
telemetry_call = mock_telemetry.capture.call_args[0][0]
⋮----
def test_tool_calling_model_with_hallucinations() -> None
⋮----
clarification = InputClarification(
failed_clarification = InputClarification(
⋮----
def test_tool_calling_model_templates_inputs() -> None
⋮----
templated_response = AIMessage(content="")
⋮----
mock_model = get_mock_generative_model(templated_response)
⋮----
step_inputs = [
⋮----
addition_tool = AdditionTool()
⋮----
result = tool_calling_model.invoke({"messages": [], "step_inputs": step_inputs})
result_message = result["messages"][0]
⋮----
tool_call = result_message.tool_calls[0]
⋮----
def test_tool_calling_model_handles_missing_args_gracefully() -> None
⋮----
invalid_response = AIMessage(content="")
⋮----
mock_model = get_mock_generative_model(invalid_response)
⋮----
def test_basic_agent_task(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
tool = AdditionTool()
def memory_extraction_step(self, state)
⋮----
def parser_model(self, state)
⋮----
def verifier_model(self, state)
⋮----
def tool_calling_model(self, state)
⋮----
response = AIMessage(content="")
⋮----
def tool_call(self, input, config)
⋮----
mock_after_tool_call = mock.MagicMock(return_value=None)
⋮----
agent = DefaultExecutionAgent(
output = agent.execute_sync()
⋮----
def test_basic_agent_task_with_verified_args(monkeypatch: pytest.MonkeyPatch) -> None
def test_default_execution_agent_edge_cases() -> None
def test_get_last_resolved_clarification() -> None
⋮----
resolved_clarification1 = InputClarification(
resolved_clarification2 = InputClarification(
unresolved_clarification = InputClarification(
⋮----
def test_clarifications_or_continue() -> None
⋮----
inputs = VerifiedToolInputs(
output = agent.clarifications_or_continue(
⋮----
def test_default_execution_agent_none_tool_execute_sync() -> None
class MockToolSchema(BaseModel)
⋮----
optional_arg: str | None = Field(default=None, description="An optional argument")
class MockAgent
⋮----
def __init__(self) -> None
class MockTool(Tool)
⋮----
def run(self, **kwargs: Any) -> Any
def test_optional_args_with_none_values() -> None
⋮----
model = VerifierModel(
updated_tool_inputs = model._validate_args_against_schema(
⋮----
class ListToolSchema(BaseModel)
⋮----
list_arg: list[str] = Field(..., description="An optional argument")
class ListToolAgent
class ListTool(Tool)
def test_list_args_with_str_values() -> None
def test_verifier_model_edge_cases() -> None
def test_before_tool_call_with_clarification(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
model_response = AIMessage(content="")
⋮----
mock_model = get_mock_generative_model(response=model_response)
⋮----
tool_node_called = False
⋮----
tool_node_called = True
⋮----
return_clarification = True
def before_tool_call(tool, args, plan_run, step) -> InputClarification | None
⋮----
output_value = output.get_value()[0]
⋮----
return_clarification = False
⋮----
def test_after_tool_call_with_clarification(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
def after_tool_call(tool, output, plan_run, step) -> InputClarification | None
</file>

<file path="tests/unit/execution_agents/test_execution_utils.py">
@pytest.fixture
def step() -> MagicMock
⋮----
step = MagicMock(spec=Step)
⋮----
def test_tool_call_or_end() -> None
⋮----
message_with_calls = AIMessage(content="test")
state_with_calls: MessagesState = {"messages": [message_with_calls]}
message_without_calls = HumanMessage(content="test")
state_without_calls: MessagesState = {"messages": [message_without_calls]}
⋮----
def test_process_output_with_clarifications(step: MagicMock) -> None
⋮----
clarifications = [
message = HumanMessage(content="test")
result = process_output(step, [message], clarifications=clarifications)
⋮----
def test_process_output_with_tool_errors(step: MagicMock) -> None
⋮----
tool = AdditionTool()
soft_error = ToolMessage(
hard_error = ToolMessage(
⋮----
def test_process_output_with_invalid_message(step: MagicMock) -> None
⋮----
invalid_message = AIMessage(content="test")
⋮----
def test_process_output_with_output_artifacts(step: MagicMock) -> None
⋮----
message = ToolMessage(tool_call_id="1", content="", artifact=LocalDataValue(value="test"))
message2 = ToolMessage(tool_call_id="2", content="", artifact=LocalDataValue(value="bar"))
result = process_output(step, [message, message2], clarifications=[])
⋮----
def test_process_output_with_artifacts(step: MagicMock) -> None
⋮----
message = ToolMessage(tool_call_id="1", content="", artifact="test")
result = process_output(step, [message], clarifications=[])
⋮----
def test_process_output_with_content(step: MagicMock) -> None
⋮----
message = ToolMessage(tool_call_id="1", content="test")
⋮----
def test_process_output_with_clarification(step: MagicMock) -> None
⋮----
clarification = InputClarification(
message = ToolMessage(tool_call_id="1", content=clarification.model_dump_json())
⋮----
def test_process_output_summary_matches_serialized_value(step: MagicMock) -> None
⋮----
dict_value = {"key1": "value1", "key2": "value2"}
message = ToolMessage(
⋮----
def test_process_output_summary_not_updated_if_provided(step: MagicMock) -> None
⋮----
provided_summary = "This is a provided summary."
⋮----
def test_get_arg_value_with_templating_no_templating() -> None
⋮----
result = get_arg_value_with_templating([], "simple string")
⋮----
def test_get_arg_value_with_templating_string_with_templating() -> None
⋮----
step_inputs = [
arg = "Hello {{$name}}, you are {{$age}} years old"
result = get_arg_value_with_templating(step_inputs, arg)
⋮----
def test_get_arg_value_with_templating_list_with_templating() -> None
⋮----
arg = ["Hello {{$name}}", "Goodbye {{$name}}"]
⋮----
def test_get_arg_value_with_templating_dict_with_templating() -> None
⋮----
arg = {"greeting": "Hello {{$name}}", "farewell": "Goodbye {{$name}}"}
⋮----
def test_template_in_required_inputs_multiple_args() -> None
⋮----
message = AIMessage(content="")
⋮----
result = template_in_required_inputs(message, step_inputs)
⋮----
def test_template_in_required_inputs_missing_args() -> None
def test_template_in_required_inputs_extra_var_raises_error() -> None
def test_process_output_with_mixed_list_and_scalar_values(step: MagicMock) -> None
⋮----
message1 = ToolMessage(
message2 = ToolMessage(
message3 = ToolMessage(
result = process_output(step, [message1, message2, message3], clarifications=[])
⋮----
expected_values = ["item1", "item2", "scalar_value", "item3", "item4"]
⋮----
def test_process_output_with_multiple_outputs_no_summaries(step: MagicMock) -> None
⋮----
message1 = ToolMessage(tool_call_id="1", content="", artifact=LocalDataValue(value=["a", "b"]))
message2 = ToolMessage(tool_call_id="2", content="", artifact=LocalDataValue(value="c"))
message3 = ToolMessage(tool_call_id="3", content="", artifact=LocalDataValue(value=["d", "e"]))
⋮----
expected_values = ["a", "b", "c", "d", "e"]
⋮----
# When no summaries are provided, it should join the serialized values
expected_summary = '["a", "b"], c, ["d", "e"]'
⋮----
def test_process_output_with_nested_list_values(step: MagicMock) -> None
⋮----
expected_values = [{"id": 1}, {"id": 2}, {"status": "success"}, {"id": 3}, {"id": 4}]
⋮----
# Should use the last output's summary
⋮----
def test_process_output_with_structured_output_schema(step: MagicMock) -> None
def test_process_output_with_structured_output_schema_no_summary(step: MagicMock) -> None
⋮----
# Set up step with structured output schema
⋮----
# Create multiple tool messages to avoid single output case, with last one having no summary
⋮----
artifact=LocalDataValue(value={"result": "structured_data"}),  # No summary provided
⋮----
result = process_output(step, [message1, message2], clarifications=[])
⋮----
# Should use serialized value when no summary is provided
</file>

<file path="tests/unit/execution_agents/test_memory_extraction.py">
def test_memory_extraction_step_no_inputs() -> None
⋮----
agent = BaseExecutionAgent(
memory_extraction_step = MemoryExtractionStep(agent=agent)
result = memory_extraction_step.invoke({})
⋮----
def test_memory_extraction_step_with_inputs() -> None
⋮----
storage = InMemoryStorage()
saved_output = storage.save_plan_run_output(
⋮----
def test_memory_extraction_step_errors_with_missing_input() -> None
def test_memory_extraction_step_with_plan_run_inputs() -> None
⋮----
result = memory_extraction_step.invoke({"messages": [], "step_inputs": []})
⋮----
def test_memory_extraction_step_uses_summary_when_value_too_large() -> None
def test_memory_extraction_step_uses_summaries_when_multiple_values_too_large() -> None
</file>

<file path="tests/unit/execution_agents/test_oneshot_agent.py">
def test_oneshot_agent_task(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
mock_telemetry = mock.MagicMock()
def memory_extraction_step(self, _) -> dict[str, Any]
⋮----
model_response = AIMessage(content="")
⋮----
mock_model = get_mock_generative_model(response=model_response)
⋮----
def tool_call(self, input, config) -> dict[str, Any]
⋮----
mock_before_tool_call = mock.MagicMock(return_value=None)
mock_after_tool_call = mock.MagicMock(return_value=None)
tool = AdditionTool()
agent = OneShotAgent(
⋮----
output = agent.execute_sync()
⋮----
call_args = mock_telemetry.capture.call_args[0][0]
⋮----
@pytest.mark.asyncio
async def test_oneshot_agent_task_async(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
async def tool_call(self, input, config) -> dict[str, Any]
⋮----
output = await agent.execute_async()
⋮----
def test_oneshot_agent_without_tool_raises() -> None
⋮----
@pytest.mark.asyncio
async def test_oneshot_agent_without_tool_raises_async() -> None
def test_oneshot_before_tool_call_with_clarifications(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
tool_node_called = False
⋮----
tool_node_called = True
⋮----
call_count = 0
return_clarification = True
def before_tool_call(tool, args, plan_run, step) -> InputClarification | None
⋮----
output_values = output.get_value()
⋮----
output_value_1 = output_values[0]
⋮----
output_value_2 = output_values[1]
⋮----
return_clarification = False
⋮----
def test_oneshot_after_tool_call_with_clarification(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
def after_tool_call(tool, output, plan_run, step) -> InputClarification | None
⋮----
output_value = output.get_value()[0]
⋮----
def test_oneshot_agent_calls_clarification_tool(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
def test_oneshot_agent_templates_values(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
tool_args = None
⋮----
tool_args = input["messages"][0].tool_calls[0]["args"]
⋮----
step = plan.steps[0]
⋮----
def test_oneshot_model_fails_without_tool() -> None
⋮----
tool_context = ToolRunContext(
tool_calling_model = OneShotToolCallingModel(
⋮----
@pytest.mark.asyncio
async def test_oneshot_model_fails_without_tool_async() -> None
⋮----
@pytest.mark.asyncio
async def test_oneshot_tool_calling_model_ainvoke_success() -> None
⋮----
result = await tool_calling_model.ainvoke({"messages": [], "step_inputs": []})
⋮----
@pytest.mark.asyncio
async def test_oneshot_tool_calling_model_ainvoke_with_execution_hooks() -> None
⋮----
@pytest.mark.asyncio
async def test_oneshot_tool_calling_model_ainvoke_with_clarification_return() -> None
⋮----
def before_tool_call(tool, args, plan_run, step) -> InputClarification
</file>

<file path="tests/unit/execution_agents/test_output.py">
class MyModel(BaseModel)
⋮----
id: str
class NotAModel
⋮----
def __init__(self, id: str) -> None
not_a_model = NotAModel(id="123")
now = datetime.now(tz=UTC)
clarification = ActionClarification(
⋮----
def test_output_serialize(input_value: Any, expected: Any) -> None
⋮----
output = LocalDataValue(value=input_value).serialize_value()
⋮----
def test_local_output() -> None
⋮----
output = LocalDataValue(value="test value")
⋮----
mock_agent_memory = MagicMock(spec=AgentMemory)
⋮----
def test_agent_memory_output() -> None
⋮----
output = AgentMemoryValue(
⋮----
mock_agent_memory = MagicMock()
⋮----
result = output.full_value(mock_agent_memory)
</file>

<file path="tests/unit/introspection_agents/__init__.py">

</file>

<file path="tests/unit/introspection_agents/test_default_introspection_agent.py">
@pytest.fixture
def mock_introspection_model() -> MagicMock
⋮----
@pytest.fixture
def introspection_agent(mock_introspection_model: MagicMock) -> DefaultIntrospectionAgent
⋮----
mock_config = get_test_config(
⋮----
@pytest.fixture
def mock_plan() -> Plan
⋮----
@pytest.fixture
def mock_plan_run() -> PlanRun
def test_base_introspection_agent_initialization() -> None
⋮----
class TestIntrospectionAgent(BaseIntrospectionAgent)
config = get_test_config()
agent_memory = InMemoryStorage()
agent = TestIntrospectionAgent(config, agent_memory)
⋮----
empty_plan = Plan(
empty_plan_run = PlanRun(
result = agent.pre_step_introspection(empty_plan, empty_plan_run)
⋮----
def test_base_introspection_agent_abstract_method_raises_error() -> None
⋮----
class IncompleteIntrospectionAgent(BaseIntrospectionAgent)
⋮----
agent = IncompleteIntrospectionAgent(config, InMemoryStorage())
⋮----
result = introspection_agent.pre_step_introspection(
⋮----
mock_messages = [HumanMessage(content="Test message")]
⋮----
stored_output = introspection_agent.agent_memory.save_plan_run_output(
⋮----
result = await introspection_agent.apre_step_introspection(
</file>

<file path="tests/unit/open_source_tools/conftest.py">
@pytest.fixture
def mock_tool_run_context(mock_model: MagicMock) -> ToolRunContext
⋮----
mock_config = MagicMock(spec=Config)
⋮----
@pytest.fixture(autouse=True)
def mock_openai_api_key_env(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
@pytest.fixture
def mock_model() -> MagicMock
</file>

<file path="tests/unit/open_source_tools/test_browser_tool.py">
@pytest.fixture
def browser_tool() -> BrowserTool
⋮----
@pytest.fixture
def mock_agent() -> MagicMock
⋮----
mock_agent = MagicMock()
⋮----
@pytest.fixture
def local_browser_provider() -> BrowserInfrastructureProviderLocal
⋮----
@pytest.fixture
def mock_browserbase() -> MagicMock
⋮----
mock_bb = MagicMock()
⋮----
provider = BrowserInfrastructureProviderBrowserBase()
⋮----
class MockBrowserInfrastructureProvider(BrowserInfrastructureProvider)
⋮----
def setup_browser(self, _: ToolRunContext) -> Browser
def construct_auth_clarification_url(self, _: ToolRunContext, sign_in_url: str) -> HttpUrl
def step_complete(self, _: ToolRunContext) -> None
⋮----
@pytest.fixture
def mock_browser_infrastructure_provider() -> BrowserInfrastructureProvider
⋮----
mock_auth_response = BrowserTaskOutput(
mock_result = MagicMock()
⋮----
mock_run = AsyncMock(return_value=mock_result)
⋮----
mock_agent_instance = MagicMock()
⋮----
browser_tool = BrowserTool(
context = get_test_tool_context()
result = browser_tool.run(context, "https://example.com", "test task")
⋮----
mock_auth_result = MagicMock()
⋮----
mock_run = AsyncMock(return_value=mock_auth_result)
⋮----
mock_task_response = BrowserTaskOutput(
mock_task_result = MagicMock()
⋮----
mock_run = AsyncMock(return_value=mock_task_result)
⋮----
def test_browser_tool_infrastructure_provider_custom() -> None
⋮----
mock_provider = MockBrowserInfrastructureProvider()
browser_tool = BrowserTool(custom_infrastructure_provider=mock_provider)
⋮----
def test_browser_tool_infrastructure_provider_remote() -> None
⋮----
browser_tool = BrowserTool(infrastructure_option=BrowserInfrastructureOption.REMOTE)
⋮----
mock_provider = MagicMock()
⋮----
provider = browser_tool.infrastructure_provider
⋮----
def test_browser_tool_infrastructure_provider_local() -> None
⋮----
browser_tool = BrowserTool(infrastructure_option=BrowserInfrastructureOption.LOCAL)
⋮----
platforms = {
⋮----
local_browser_provider = BrowserInfrastructureProviderLocal()
⋮----
test_args = "--headless,--disable-gpu"
⋮----
mock_logger_instance = MagicMock()
mock_logger = MagicMock(return_value=mock_logger_instance)
⋮----
browser = local_browser_provider.setup_browser(context)
⋮----
sign_in_url = "https://example.com/login"
result = local_browser_provider.construct_auth_clarification_url(context, sign_in_url)
⋮----
def test_browser_infra_local_custom_chrome_path_initialization() -> None
⋮----
custom_path = "/custom/chrome/path"
provider = BrowserInfrastructureProviderLocal(chrome_path=custom_path)
⋮----
def test_browser_infra_local_custom_chromium_args_initialization() -> None
⋮----
custom_args = ["--headless", "--disable-gpu"]
provider = BrowserInfrastructureProviderLocal(extra_chromium_args=custom_args)
⋮----
def test_browserbase_provider_init_missing_api_key() -> None
def test_browserbase_provider_init_missing_project_id() -> None
⋮----
mock_ctx = MagicMock()
⋮----
context_id = mock_browserbase_provider.get_context_id(mock_ctx, mock_browserbase_provider.bb)
⋮----
mock_context = MagicMock()
⋮----
mock_session = MagicMock()
⋮----
session = mock_browserbase_provider.create_session("test_context_id")
⋮----
connect_url = mock_browserbase_provider.get_or_create_session(
⋮----
mock_debug = MagicMock()
⋮----
url = mock_browserbase_provider.construct_auth_clarification_url(
⋮----
browser = mock_browserbase_provider.setup_browser(context)
⋮----
def test_browser_tool_for_url_init_default_parameters() -> None
⋮----
url = "https://example.com"
tool = BrowserToolForUrl(url=url)
⋮----
def test_browser_tool_for_url_init_custom_parameters() -> None
⋮----
custom_id = "custom_browser_tool"
custom_name = "Custom Browser Tool"
custom_description = "Custom description for browser tool"
tool = BrowserToolForUrl(
⋮----
def test_browser_tool_for_url_init_subdomain_handling() -> None
⋮----
url = "https://sub.example.com"
⋮----
class TestStructuredOutputSchema(BaseModel)
⋮----
result: str = Field(description="Test result field")
status: str = Field(description="Test status field")
⋮----
mock_task_response = BrowserTaskOutput[TestStructuredOutputSchema](
⋮----
mock_auth_response = BrowserTaskOutput[TestStructuredOutputSchema](
⋮----
end_user = EndUser(external_id="123", additional_data={"bb_session_id": "session123"})
⋮----
end_user = EndUser(external_id="123", additional_data={})
⋮----
def test_process_task_data() -> None
⋮----
task_data = "this is the data"
⋮----
task_data = ["this is the data"]
⋮----
plan = (
end_user = EndUser(
</file>

<file path="tests/unit/open_source_tools/test_calculator_tool.py">
@pytest.fixture
def calculator_tool() -> CalculatorTool
def test_math_expression_conversion(calculator_tool: CalculatorTool) -> None
def test_run_valid_expressions(calculator_tool: CalculatorTool) -> None
⋮----
context = get_test_tool_context()
⋮----
def test_run_invalid_expressions(calculator_tool: CalculatorTool) -> None
⋮----
patched_tool = CalculatorTool()
⋮----
def test_run_division_by_zero(calculator_tool: CalculatorTool) -> None
def test_run_complex_expressions(calculator_tool: CalculatorTool) -> None
def test_run_decimal_numbers(calculator_tool: CalculatorTool) -> None
</file>

<file path="tests/unit/open_source_tools/test_crawl_tool.py">
def test_crawl_tool_missing_api_key() -> None
⋮----
tool = CrawlTool()
⋮----
ctx = get_test_tool_context()
⋮----
def test_crawl_tool_successful_response() -> None
⋮----
mock_api_key = "tvly-mock-api-key"
mock_response = {
⋮----
result = tool.run(
⋮----
def test_crawl_tool_with_advanced_options() -> None
⋮----
call_args = mock_post.call_args
payload = call_args[1]["json"]
⋮----
def test_crawl_tool_default_parameters() -> None
⋮----
result = tool.run(ctx, "https://example.com")
⋮----
def test_crawl_tool_optional_parameters_only_when_provided() -> None
⋮----
mock_response = {"base_url": "example.com", "results": []}
⋮----
def test_crawl_tool_no_results_in_response() -> None
⋮----
mock_response = {"error": "No pages could be crawled"}
⋮----
def test_crawl_tool_http_error() -> None
def test_crawl_tool_authorization_header() -> None
⋮----
headers = call_args[1]["headers"]
⋮----
def test_crawl_tool_api_endpoint() -> None
def test_crawl_tool_timeout_error() -> None
def test_crawl_tool_http_status_error() -> None
⋮----
mock_response = Mock()
⋮----
def test_crawl_tool_exclude_paths_parameter() -> None
def test_crawl_tool_exclude_domains_parameter() -> None
def test_crawl_tool_all_optional_parameters() -> None
def test_crawl_tool_http_status_error_with_json_response() -> None
def test_crawl_tool_http_status_error_with_invalid_json() -> None
</file>

<file path="tests/unit/open_source_tools/test_extract_tool.py">
def test_extract_tool_missing_api_key() -> None
⋮----
tool = ExtractTool()
⋮----
ctx = get_test_tool_context()
⋮----
def test_extract_tool_successful_response() -> None
⋮----
mock_api_key = "tvly-mock-api-key"
mock_response = {
⋮----
result = tool.run(ctx, ["https://example.com"])
⋮----
def test_extract_tool_multiple_urls() -> None
⋮----
result = tool.run(ctx, ["https://example1.com", "https://example2.com"])
⋮----
def test_extract_tool_with_custom_options() -> None
⋮----
result = tool.run(
⋮----
call_args = mock_post.call_args
⋮----
def test_extract_tool_no_results_in_response() -> None
⋮----
mock_response = {"error": "No content could be extracted"}
⋮----
def test_extract_tool_http_error() -> None
def test_extract_tool_authorization_header() -> None
⋮----
headers = call_args[1]["headers"]
</file>

<file path="tests/unit/open_source_tools/test_file_reader_tool.py">
def test_file_reader_tool_read_txt(tmp_path: Path) -> None
⋮----
tool = FileReaderTool()
ctx = get_test_tool_context()
filename = tmp_path / "test.txt"
content = "Hello, world!"
⋮----
result = tool.run(ctx, str(filename))
⋮----
def test_file_reader_tool_read_log(tmp_path: Path) -> None
⋮----
filename = tmp_path / "test.log"
⋮----
def test_file_reader_tool_read_json(tmp_path: Path) -> None
⋮----
filename = tmp_path / "test.json"
content = {"key": "value"}
⋮----
def test_file_reader_tool_read_csv(tmp_path: Path) -> None
⋮----
filename = tmp_path / "test.csv"
frame = pd.DataFrame({"col1": [1, 2], "col2": [3, 4]})
⋮----
def test_file_reader_tool_read_xlsx(tmp_path: Path) -> None
⋮----
filename = tmp_path / "test.xlsx"
⋮----
def test_file_reader_tool_read_xls(tmp_path: Path) -> None
⋮----
filename = tmp_path / "test.xls"
⋮----
def test_file_reader_tool_unsupported_format(tmp_path: Path) -> None
⋮----
filename = tmp_path / "test.unsupported"
⋮----
def test_file_reader_tool_file_alt_files(tmp_path: Path) -> None
⋮----
filename = tmp_path / "non_existent.txt"
subfolder = tmp_path / "test"
⋮----
alt_filename = subfolder / "non_existent.txt"
⋮----
output = tool.run(ctx, str(filename))
⋮----
def test_file_reader_tool_file_no_files(tmp_path: Path) -> None
</file>

<file path="tests/unit/open_source_tools/test_file_writer_tool.py">
def test_file_writer_tool_successful_write(tmp_path: Path) -> None
⋮----
tool = FileWriterTool()
ctx = get_test_tool_context()
filename = tmp_path / "test_file.txt"
content = "Hello, world!"
result = tool.run(ctx, str(filename), content)
⋮----
def test_file_writer_tool_overwrite_existing_file(tmp_path: Path) -> None
⋮----
filename = tmp_path / "existing_file.txt"
⋮----
content = "New content!"
⋮----
def test_file_writer_tool_handles_file_creation_error(tmp_path: Path) -> None
⋮----
filename = tmp_path / "error_file.txt"
</file>

<file path="tests/unit/open_source_tools/test_image_understanding_tool.py">
@pytest.fixture
def mock_image_understanding_tool() -> ImageUnderstandingTool
⋮----
mock_response = MagicMock()
⋮----
schema_data = {
result = mock_image_understanding_tool.run(mock_tool_run_context, **schema_data)
⋮----
def test_image_understanding_tool_schema_valid_input() -> None
⋮----
schema = ImageUnderstandingToolSchema(**schema_data)
⋮----
def test_image_understanding_tool_schema_missing_task() -> None
def test_image_understanding_tool_schema_missing_image_url_and_file() -> None
def test_image_understanding_tool_schema_both_image_url_and_file() -> None
⋮----
called_with = mock_model.to_langchain.return_value.invoke.call_args_list[0].args[0]
</file>

<file path="tests/unit/open_source_tools/test_llm_tool.py">
@pytest.fixture
def mock_llm_tool() -> LLMTool
⋮----
task = "What is the capital of France?"
result = mock_llm_tool.run(mock_tool_run_context, task)
⋮----
class TestStructuredOutput(BaseModel)
⋮----
capital: str
⋮----
def test_llm_tool_schema_valid_input() -> None
⋮----
schema_data = {"task": "Solve a math problem", "task_data": ["1 + 1 = 2"]}
schema = LLMToolSchema(**schema_data)
⋮----
def test_llm_tool_schema_missing_task() -> None
def test_llm_tool_initialization(mock_llm_tool: LLMTool) -> None
⋮----
called_with = mock_model.get_response.call_args_list[0].args[0]
⋮----
# Assert the result is the expected response
⋮----
def test_process_task_data_with_string() -> None
⋮----
result = LLMTool.process_task_data("String data")
⋮----
def test_process_task_data_with_list() -> None
⋮----
result = LLMTool.process_task_data(["Item 1", "Item 2"])
⋮----
def test_process_task_data_with_none() -> None
⋮----
result = LLMTool.process_task_data(None)
⋮----
def test_process_task_data_with_complex_objects() -> None
⋮----
class TestObject
⋮----
def __str__(self) -> str
result = LLMTool.process_task_data([TestObject(), {"nested": "value"}])
⋮----
# Async tests for LLMTool.arun function
⋮----
# Setup mock responses
⋮----
# Define task input
⋮----
# Run the tool asynchronously
result = await mock_llm_tool.arun(mock_tool_run_context, task)
⋮----
# Define task and context
⋮----
# Verify that the Model's aget_response method is called
called_with = mock_model.aget_response.call_args_list[0].args[0]
⋮----
task = "Analyze this data"
task_data = ["Data point 1", "Data point 2", {"key": "value"}]
result = await mock_llm_tool.arun(mock_tool_run_context, task, task_data)
⋮----
# Define task input with string task data
task = "Analyze this text"
task_data = "This is a string of text data"
⋮----
# Verify that the Model's aget_response method is called with the correct messages
⋮----
task = "Answer this question"
result = await mock_llm_tool.arun(mock_tool_run_context, task, None)
</file>

<file path="tests/unit/open_source_tools/test_map_tool.py">
def test_map_tool_missing_api_key() -> None
⋮----
tool = MapTool()
⋮----
ctx = get_test_tool_context()
⋮----
def test_map_tool_successful_response() -> None
⋮----
mock_api_key = "tvly-mock-api-key"
mock_response = {
⋮----
result = tool.run(ctx, "https://docs.tavily.com")
⋮----
def test_map_tool_with_advanced_options() -> None
⋮----
result = tool.run(
⋮----
call_args = mock_post.call_args
payload = call_args[1]["json"]
⋮----
def test_map_tool_default_parameters() -> None
⋮----
result = tool.run(ctx, "https://example.com")
⋮----
def test_map_tool_optional_parameters_only_when_provided() -> None
⋮----
mock_response = {"base_url": "example.com", "results": []}
⋮----
def test_map_tool_complex_website() -> None
⋮----
result = tool.run(ctx, "https://example.com", max_depth=2, limit=100)
⋮----
def test_map_tool_single_page_site() -> None
⋮----
result = tool.run(ctx, "https://simple.com")
⋮----
def test_map_tool_no_results_in_response() -> None
⋮----
mock_response = {"error": "No pages could be mapped"}
⋮----
def test_map_tool_http_error() -> None
def test_map_tool_authorization_header() -> None
⋮----
mock_response = {"base_url": "example.com", "results": ["https://example.com/"]}
⋮----
headers = call_args[1]["headers"]
⋮----
def test_map_tool_api_endpoint() -> None
def test_map_tool_payload_structure() -> None
def test_map_tool_exclude_domains_parameter() -> None
def test_map_tool_all_optional_parameters() -> None
</file>

<file path="tests/unit/open_source_tools/test_pdf_reader_tool.py">
@pytest.fixture
def pdf_reader_tool() -> PDFReaderTool
⋮----
@pytest.fixture
def mock_mistral_client() -> MagicMock
⋮----
mock = MagicMock()
pages = [MagicMock(markdown="Page 1 content"), MagicMock(markdown="Page 2 content")]
⋮----
pdf_path = tmp_path / "test.pdf"
⋮----
result = pdf_reader_tool.run(get_test_tool_context(), str(pdf_path))
⋮----
def test_pdf_reader_tool_file_not_found(pdf_reader_tool: PDFReaderTool) -> None
</file>

<file path="tests/unit/open_source_tools/test_search_tool.py">
def test_search_tool_missing_api_key() -> None
⋮----
tool = SearchTool()
⋮----
ctx = get_test_tool_context()
⋮----
def test_search_tool_successful_response() -> None
⋮----
mock_api_key = "mock-api-key"
mock_response = {
⋮----
result = tool.run(ctx, "What is the capital of France?")
⋮----
def test_search_tool_fewer_results_than_max() -> None
def test_search_tool_no_answer_in_response() -> None
⋮----
mock_response = {"no_answer": "No relevant information found."}
⋮----
def test_search_tool_http_error() -> None
⋮----
@pytest.mark.asyncio
async def test_search_tool_async_missing_api_key() -> None
⋮----
@pytest.mark.asyncio
async def test_search_tool_async_successful_response(httpx_mock: HTTPXMock) -> None
⋮----
result = await tool.arun(ctx, "What is the capital of France?")
⋮----
@pytest.mark.asyncio
async def test_search_tool_async_fewer_results_than_max(httpx_mock: HTTPXMock) -> None
⋮----
@pytest.mark.asyncio
async def test_search_tool_async_no_answer_in_response(httpx_mock: HTTPXMock) -> None
⋮----
@pytest.mark.asyncio
async def test_search_tool_async_http_error(httpx_mock: HTTPXMock) -> None
⋮----
@pytest.mark.asyncio
async def test_search_tool_async_different_query(httpx_mock: HTTPXMock) -> None
⋮----
result = await tool.arun(ctx, "Who won the US election in 2020?")
</file>

<file path="tests/unit/open_source_tools/test_weather_tool.py">
def test_weather_tool_missing_api_key() -> None
⋮----
tool = WeatherTool()
⋮----
ctx = get_test_tool_context()
⋮----
def test_weather_tool_successful_response(httpx_mock: HTTPXMock) -> None
⋮----
mock_api_key = "mock-api-key"
mock_response = {"main": {"temp": 10}, "weather": [{"description": "sunny"}]}
⋮----
result = tool.run(ctx, "paris")
⋮----
def test_weather_tool_no_answer_in_response(httpx_mock: HTTPXMock) -> None
⋮----
mock_response = {"no_answer": "No relevant information found."}
⋮----
def test_weather_tool_no_main_answer_in_response(httpx_mock: HTTPXMock) -> None
⋮----
mock_response = {
⋮----
def test_weather_tool_http_error(httpx_mock: HTTPXMock) -> None
⋮----
@pytest.mark.asyncio
async def test_weather_tool_async_missing_api_key() -> None
⋮----
@pytest.mark.asyncio
async def test_weather_tool_async_successful_response(httpx_mock: HTTPXMock) -> None
⋮----
result = await tool.arun(ctx, "paris")
⋮----
@pytest.mark.asyncio
async def test_weather_tool_async_no_answer_in_response(httpx_mock: HTTPXMock) -> None
⋮----
@pytest.mark.asyncio
async def test_weather_tool_async_no_main_answer_in_response(httpx_mock: HTTPXMock) -> None
⋮----
@pytest.mark.asyncio
async def test_weather_tool_async_http_error(httpx_mock: HTTPXMock) -> None
</file>

<file path="tests/unit/planning_agents/test_default_planning_agent.py">
@pytest.fixture
def mock_config() -> Config
def test_generate_steps_or_error_success(mock_config: Config) -> None
⋮----
query = "Send hello@portialabs.ai an email with a summary of the latest news on AI"
mock_model = get_mock_generative_model(
⋮----
planning_agent = DefaultPlanningAgent(mock_config)
result = planning_agent.generate_steps_or_error(
⋮----
def test_base_classes() -> None
⋮----
class MyPlanningAgent(BasePlanningAgent)
wrapper = MyPlanningAgent(get_test_config())
⋮----
def test_generate_steps_or_error_failure(mock_config: Config) -> None
def test_render_prompt() -> None
⋮----
plan_input = PlanInput(name="$plan_input", description="Plan input description")
plans = [
rendered_prompt = render_prompt_insert_defaults(
overall_pattern = re.compile(
⋮----
tool_pattern = re.compile(r"<Tools>(.*?)</Tools>", re.DOTALL)
tool_match = tool_pattern.findall(example_match)[0]
⋮----
query_pattern = re.compile(r"<Query>(.*?)</Query>", re.DOTALL)
query_match = query_pattern.findall(example_match)[0]
⋮----
response_pattern = re.compile(r"<Response>(.*?)</Response>", re.DOTALL)
response_match = response_pattern.findall(example_match)[0]
⋮----
def test_generate_steps_or_error_invalid_tool_id(mock_config: Config) -> None
⋮----
query = "Calculate something"
mock_response = StepsOrError(
mock_model = get_mock_generative_model(response=mock_response)
⋮----
def test_generate_steps_or_error_invalid_input_with_retry(mock_config: Config) -> None
⋮----
plan_inputs = [
mock_response1 = StepsOrError(
mock_response2 = StepsOrError(
mock_model = get_mock_generative_model(response=mock_response1)
⋮----
def test_generate_steps_assigns_llm_tool_id(mock_config: Config) -> None
⋮----
query = "Generate a creative story"
⋮----
def test_generate_steps_with_plan_inputs(mock_config: Config) -> None
⋮----
prompt_text = mock_model._client.invoke.call_args[0][0][1].content
⋮----
@pytest.mark.asyncio
async def test_agenerate_steps_or_error_success(mock_config: Config) -> None
⋮----
result = await planning_agent.agenerate_steps_or_error(
⋮----
@pytest.mark.asyncio
async def test_agenerate_steps_or_error_failure(mock_config: Config) -> None
⋮----
@pytest.mark.asyncio
async def test_agenerate_steps_or_error_invalid_tool_id(mock_config: Config) -> None
⋮----
@pytest.mark.asyncio
async def test_agenerate_steps_or_error_invalid_input_with_retry(mock_config: Config) -> None
⋮----
@pytest.mark.asyncio
async def test_agenerate_steps_assigns_llm_tool_id(mock_config: Config) -> None
⋮----
@pytest.mark.asyncio
async def test_agenerate_steps_with_plan_inputs(mock_config: Config) -> None
⋮----
prompt_text = mock_model._client.ainvoke.call_args[0][0][1].content
</file>

<file path="tests/unit/telemetry/test_telemetry_service.py">
class TelemetryEvent(BaseTelemetryEvent)
⋮----
def __init__(self, name: str, properties: dict) -> None
⋮----
@property
    def name(self) -> str
⋮----
@property
    def properties(self) -> dict
def test_xdg_cache_home_default() -> None
def test_xdg_cache_home_custom() -> None
⋮----
custom_path = "/custom/cache/path"
⋮----
def test_get_project_id_key_localhost() -> None
def test_get_project_id_key_dev() -> None
def test_get_project_id_key_default() -> None
class TestProductTelemetry
⋮----
@pytest.fixture(autouse=True)
    def mock_version(self) -> Any
⋮----
@pytest.fixture
    def telemetry(self) -> Any
⋮----
@pytest.fixture
    def mock_logger(self) -> MagicMock
⋮----
logger = MagicMock()
⋮----
def test_init_telemetry_disabled(self, mock_logger: MagicMock) -> None
⋮----
telemetry = ProductTelemetry()
⋮----
def test_init_telemetry_enabled(self, mock_logger: MagicMock) -> None
def test_capture_when_disabled(self, mock_logger: MagicMock) -> None
⋮----
event = TelemetryEvent("test_event", {})
⋮----
def test_capture_when_enabled(self, mock_logger: MagicMock) -> None
⋮----
mock_client = MagicMock()
⋮----
event = TelemetryEvent("test_event", {"key": "value"})
⋮----
args = mock_client.capture.call_args[0]
⋮----
kwargs = mock_client.capture.call_args[1]
⋮----
def test_capture_when_enabled_with_exception(self, mock_logger: MagicMock) -> None
def test_user_id_generation(self, telemetry: ProductTelemetry, tmp_path: Path) -> None
⋮----
user_id1 = telemetry.user_id
⋮----
user_id2 = telemetry.user_id
⋮----
user_id3 = telemetry.user_id
⋮----
def test_user_id_error_handling(self, telemetry: ProductTelemetry) -> None
</file>

<file path="tests/unit/conftest.py">
@pytest.fixture
def telemetry() -> MagicMock
⋮----
@pytest.fixture
def planning_model() -> MagicMock
⋮----
@pytest.fixture
def default_model() -> MagicMock
⋮----
@pytest.fixture
def portia(planning_model: MagicMock, default_model: MagicMock, telemetry: MagicMock) -> Portia
⋮----
config = get_test_config(
tool_registry = ToolRegistry([AdditionTool(), ClarificationTool()])
</file>

<file path="tests/unit/test_clarification_handler.py">
class TestClarificationHandler(ClarificationHandler)
def test_action_clarification() -> None
⋮----
handler = TestClarificationHandler()
on_resolution = MagicMock()
on_error = MagicMock()
clarification = ActionClarification(
⋮----
def test_input_clarification() -> None
⋮----
clarification = InputClarification(
⋮----
def test_multiple_choice_clarification() -> None
⋮----
clarification = MultipleChoiceClarification(
⋮----
def test_value_confirmation_clarification() -> None
⋮----
clarification = ValueConfirmationClarification(
⋮----
def test_user_verification_clarification() -> None
⋮----
clarification = UserVerificationClarification(
⋮----
def test_custom_clarification_routing() -> None
⋮----
clarification = CustomClarification(
⋮----
def test_invalid_clarification() -> None
⋮----
class UnhandledClarification(Clarification)
clarification = UnhandledClarification(
</file>

<file path="tests/unit/test_clarifications.py">
def test_action_clarification_ser() -> None
⋮----
clarification = ActionClarification(
clarification_model = clarification.model_dump()
⋮----
def test_clarification_uuid_assign() -> None
def test_value_multi_choice_validation() -> None
def test_custom_clarification_deserialize(tmp_path: Path) -> None
⋮----
clarification_one = CustomClarification(
storage = DiskFileStorage(storage_dir=str(tmp_path))
⋮----
retrieved = storage.get_plan_run(plan_run.id)
⋮----
def test_user_verification_clarification() -> None
⋮----
clarification = UserVerificationClarification(
⋮----
def test_user_verification_clarification_validation() -> None
</file>

<file path="tests/unit/test_cli_clarification_handler.py">
@pytest.fixture
def cli_handler() -> CLIClarificationHandler
⋮----
@patch("portia.cli_clarification_handler.click.echo")
def test_action_clarification(mock_echo: MagicMock, cli_handler: CLIClarificationHandler) -> None
⋮----
on_resolution = MagicMock()
on_error = MagicMock()
clarification = ActionClarification(
⋮----
echo_message = mock_echo.call_args[0][0]
⋮----
confirm_message = mock_confirm.call_args[1]["text"]
⋮----
@patch("portia.cli_clarification_handler.click.prompt")
def test_input_clarification(mock_prompt: MagicMock, cli_handler: CLIClarificationHandler) -> None
⋮----
clarification = InputClarification(
⋮----
prompt_text = mock_prompt.call_args[0][0]
⋮----
clarification = MultipleChoiceClarification(
⋮----
choice_type = mock_prompt.call_args[1]["type"]
⋮----
clarification = ValueConfirmationClarification(
⋮----
confirm_text = mock_confirm.call_args[1]["text"]
⋮----
clarification = CustomClarification(
⋮----
guidance_text = mock_echo.call_args_list[0][0][0]
data_text = mock_echo.call_args_list[1][0][0]
⋮----
clarification = UserVerificationClarification(
</file>

<file path="tests/unit/test_cli.py">
@pytest.fixture(autouse=True)
def mock_env_vars(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
@pytest.fixture
def mock_portia_cls() -> Iterator[MagicMock]
⋮----
@pytest.fixture(autouse=True)
def mock_config() -> Iterator[None]
def test_cli_run(mock_portia_cls: MagicMock) -> None
⋮----
mock_portia = mock_portia_cls.return_value
runner = CliRunner()
result = runner.invoke(cli, ["run", "Calculate 1 + 2"], input="y\n")
⋮----
result = runner.invoke(
⋮----
config = mock_portia_cls.call_args.kwargs["config"]
⋮----
def test_cli_run_config_set_planner_model(mock_portia_cls: MagicMock) -> None
def test_cli_run_config_multi_setting(mock_portia_cls: MagicMock) -> None
⋮----
tools = mock_portia_cls.call_args.kwargs["tools"]
⋮----
def test_cli_run_no_confirmation(mock_portia_cls: MagicMock) -> None
⋮----
result = runner.invoke(cli, ["run", "Compute 3 * 3", "--confirm", "false"], input="")
⋮----
def test_cli_run_reject_confirmation(mock_portia_cls: MagicMock) -> None
⋮----
result = runner.invoke(cli, ["run", "Subtract 5 - 3"], input="n\n")
⋮----
def test_cli_plan_default(mock_portia_cls: MagicMock) -> None
⋮----
result = runner.invoke(cli, ["plan", "What is the weather?"], input="")
⋮----
def test_cli_list_tools() -> None
⋮----
llm_tool = LLMTool()
⋮----
result = runner.invoke(cli, ["list-tools"], input="")
⋮----
def test_cli_version() -> None
⋮----
result = runner.invoke(cli, ["version"], input="")
</file>

<file path="tests/unit/test_common.py">
def test_portia_enum() -> None
⋮----
class MyEnum(PortiaEnum)
⋮----
OK = "OK"
⋮----
def test_combine_args_kwargs() -> None
⋮----
class Comment(BaseModel)
⋮----
body: str
public: bool
comment = Comment(body="The issue is being reviewed.", public=True)
result = combine_args_kwargs(
⋮----
class TestPrefixedUUID
⋮----
def test_default_prefix(self) -> None
⋮----
prefixed_uuid = PrefixedUUID()
⋮----
def test_custom_prefix(self) -> None
⋮----
class CustomPrefixUUID(PrefixedUUID)
⋮----
prefix = "test"
prefixed_uuid = CustomPrefixUUID()
⋮----
def test_from_string(self) -> None
⋮----
uuid_str = "123e4567-e89b-12d3-a456-426614174000"
prefixed_uuid = PrefixedUUID.from_string(uuid_str)
⋮----
prefixed_str = f"test-{uuid_str}"
prefixed_uuid = CustomPrefixUUID.from_string(prefixed_str)
⋮----
def test_serialization(self) -> None
⋮----
uuid = PrefixedUUID()
⋮----
def test_model_validation(self) -> None
⋮----
class CustomID(PrefixedUUID)
class TestModel(BaseModel)
⋮----
id: CustomID = Field(default_factory=CustomID)
⋮----
json_data = f'{{"id": "test-{uuid_str}"}}'
model = TestModel.model_validate_json(json_data)
⋮----
json_data = json.dumps(
⋮----
json_data = f'{{"id": "monkey-{uuid_str}"}}'
⋮----
class TestModelNoPrefix(BaseModel)
⋮----
id: PrefixedUUID
json_data = f'{{"id": "{uuid_str}"}}'
model = TestModelNoPrefix.model_validate_json(json_data)
⋮----
def test_hash(self) -> None
def test_validate_extras_dependencies() -> None
def test_validate_extras_dependencies_raise_error_false() -> None
⋮----
extras_installed = validate_extras_dependencies("test", raise_error=False)
⋮----
def test_validate_extras_dependencies_success() -> None
def test_singleton() -> None
⋮----
@singleton
    class TestClass
⋮----
def __init__(self, value: int = 0) -> None
instance1 = TestClass(1)
instance2 = TestClass(2)
⋮----
instance3 = TestClass(3)
</file>

<file path="tests/unit/test_config.py">
PROVIDER_ENV_VARS = [
⋮----
@pytest.fixture(autouse=True)
def clean_env(monkeypatch: pytest.MonkeyPatch) -> None
def test_from_default(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
c = Config.from_default(
⋮----
def test_set_keys(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
c = Config.from_default(default_log_level=LogLevel.CRITICAL)
⋮----
def test_set_with_strings(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
c = Config.from_default(storage_class="MEMORY")
⋮----
c = Config.from_default(storage_class="DISK", storage_dir="/test")
⋮----
c = Config.from_default(storage_class="DISK")
⋮----
c = Config.from_default(storage_class="OTHER")
⋮----
c = Config.from_default(storage_class=123)
c = Config.from_default(default_log_level="CRITICAL")
⋮----
c = Config.from_default(default_log_level="some level")
c = Config.from_default(execution_agent_type="default")
⋮----
c = Config.from_default(execution_agent_type="my agent")
⋮----
def test_llm_redis_cache_url_env(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
mock_redis_cache_instance = MagicMock()
mock_redis_cache = MagicMock(return_value=mock_redis_cache_instance)
⋮----
config = Config.from_default(openai_api_key=SecretStr("123"))
⋮----
def test_llm_redis_cache_url_kwarg(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
mock_redis_cache_instance = InMemoryCache()
⋮----
config = Config.from_default(
⋮----
c = Config.from_default(default_model=model_string)
model = c.get_default_model()
⋮----
c = Config.from_default(planning_model=model_string)
model = c.get_planning_model()
⋮----
def test_set_default_model_from_model_instance() -> None
⋮----
model = OpenAIGenerativeModel(model_name="gpt-4o", api_key=SecretStr("test-openai-key"))
c = Config.from_default(default_model=model)
resolved_model = c.get_default_model()
⋮----
planner_model = c.get_planning_model()
⋮----
MODEL_KEYS = sorted(set(GenerativeModelsConfig.model_fields) - {"default_model"})
⋮----
@pytest.mark.parametrize("model_key", MODEL_KEYS)
def test_set_agent_model_default_model_not_set_fails(model_key: str) -> None
⋮----
_ = Config.from_default(**{model_key: model})
⋮----
model_str = "openai/gpt-4o"
c = Config.from_default(**{model_key: model_str})
method = getattr(c, f"get_{model_key}")
resolved_model = method()
⋮----
default_model = c.get_default_model()
⋮----
def test_set_model_with_string_api_key_env_var_not_set() -> None
⋮----
_ = Config.from_default(default_model=model_str)
⋮----
_ = Config.from_default(
⋮----
c = Config.from_default(default_model="mistralai/mistral-tiny-latest", llm_provider="anthropic")
⋮----
def test_provider_set_from_env_planner_model_overriden(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
def test_set_default_and_planner_model_with_instances_no_provider_set() -> None
def test_get_planning_model_azure() -> None
def test_getters() -> None
def test_azure_openai_requires_endpoint(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
c = Config.from_default(llm_provider=LLMProvider.AZURE_OPENAI)
⋮----
def test_custom_model_from_string_raises_error() -> None
⋮----
_ = Config.from_default(default_model="custom/test")
⋮----
def test_check_model_supported_raises_deprecation_warning() -> None
def test_summarizer_model_not_instantiable(monkeypatch: pytest.MonkeyPatch) -> None
def test_config_error_resolve_model_raises_error(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
config = Config.from_default()
⋮----
def test_config_model_in_kwargs_and_models_raises_error(monkeypatch: pytest.MonkeyPatch) -> None
def test_no_provider_or_default_model_raises_error() -> None
def test_llm_model_name_deprecation(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
c = Config.from_default(llm_model_name="openai/gpt-4o")
⋮----
def test_get_model(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
c = Config.from_default()
model = c.get_generative_model("openai/gpt-4o")
⋮----
from_instsance = c.get_generative_model(
⋮----
c = Config.from_default(**config_kwargs)
⋮----
def test_deprecated_llm_model_cannot_instantiate_from_string() -> None
⋮----
_ = LLMModel("adijabisfbgiwjebr")
def test_provider_default_models_with_reasoning(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
c = Config.from_default(llm_provider=LLMProvider.ANTHROPIC)
planning_model = c.get_planning_model()
⋮----
introspection_model = c.get_introspection_model()
⋮----
def test_provider_default_models_with_reasoning_openai(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
c = Config.from_default(llm_provider=LLMProvider.OPENAI)
⋮----
def test_parse_str_to_enum(value: str, expected: LLMProvider) -> None
</file>

<file path="tests/unit/test_end_user.py">
def test_portia_local_default_config_with_api_keys() -> None
⋮----
end_user = EndUser(external_id="123")
</file>

<file path="tests/unit/test_execution_hooks.py">
def test_clarify_before_tool_call_first_call() -> None
⋮----
tool = AdditionTool()
args = {"a": 1, "b": 2}
⋮----
step = plan.steps[0]
result = clarify_on_all_tool_calls(tool, args, plan_run, step)
⋮----
def test_clarify_before_tool_call_with_previous_yes_response() -> None
⋮----
prev_clarification = UserVerificationClarification(
⋮----
def test_clarify_before_tool_call_with_previous_negative_response() -> None
def test_clarify_before_tool_call_with_previous_negative_response_bare_clarification() -> None
⋮----
prev_clarification = Clarification(
⋮----
def test_clarify_before_tool_call_with_unresolved_clarification() -> None
def test_log_step_outputs() -> None
⋮----
step = Step(task="Test task", tool_id="test_tool", output="$output")
output = LocalDataValue(value="Test output", summary="Test summary")
⋮----
hook = clarify_on_tool_calls(tool_id)
result = hook(tool, args, plan_run, step)
</file>

<file path="tests/unit/test_gemini_langsmith_wrapper.py">
def test_get_ls_params() -> None
⋮----
params = _get_ls_params("gemini-pro", {})
⋮----
def test_process_outputs_valid() -> None
⋮----
candidate = types.Candidate(content=types.Content(parts=[types.Part(text="Hello world")]))
outputs = types.GenerateContentResponse(candidates=[candidate])
result = _process_outputs(outputs)
⋮----
def test_process_outputs_empty() -> None
⋮----
outputs = types.GenerateContentResponse(candidates=[])
⋮----
def test_process_inputs_with_two_parts() -> None
⋮----
inputs = {
result = _process_inputs(inputs)
⋮----
def test_process_inputs_single_part() -> None
def test_process_inputs_no_list() -> None
def test_process_inputs_invalid() -> None
⋮----
result = _process_inputs({})
⋮----
@pytest.fixture
def fake_client() -> tuple[Mock, Mock]
⋮----
mock_client = Mock()
mock_model_interface = Mock()
mock_generate_content = Mock(return_value="original result")
⋮----
@patch("portia.gemini_langsmith_wrapper.run_helpers.traceable")
def test_wrap_gemini_traces_and_calls_original(traceable_mock: Mock, fake_client: Mock) -> None
⋮----
traced_func = Mock(return_value="traced result")
⋮----
wrapped_client = wrap_gemini(client)
result = wrapped_client.models.generate_content(
⋮----
def inner(*args, **kwargs) -> None
</file>

<file path="tests/unit/test_logging.py">
def test_logger_formatter_get_function_color(record: dict, expected_color: str) -> None
⋮----
logger_formatter = Formatter()
⋮----
def test_logger_sanitize_message() -> None
⋮----
long_message = "test\n" * 100
truncated_message = logger_formatter._sanitize_message_(long_message)
⋮----
def test_logger_manager_initialization() -> None
⋮----
logger_manager = LoggerManager()
⋮----
def test_logger_manager_with_custom_logger() -> None
⋮----
mock_logger = Mock(spec=LoggerInterface)
logger_manager = LoggerManager(custom_logger=mock_logger)
⋮----
def test_set_logger() -> None
def test_configure_from_config() -> None
⋮----
mock_config = Mock(
⋮----
def test_configure_from_config_stderr() -> None
def test_configure_from_config_custom_logger() -> None
def test_logger() -> None
def test_safe_logger_successful_logs() -> None
⋮----
safe_logger = SafeLogger(mock_logger)
⋮----
def test_safe_logger_error_handling() -> None
def test_formatter_sanitizes_stack_trace() -> None
⋮----
def will_fail() -> None
captured_exc = None
⋮----
captured_exc = exc
⋮----
record = {
formatted = logger_formatter.format(record)
</file>

<file path="tests/unit/test_model.py">
message = Message.from_langchain(langchain_message)
⋮----
def test_message_from_langchain_unsupported_type() -> None
⋮----
class UnsupportedMessage
⋮----
content = "test"
⋮----
langchain_message = portia_message.to_langchain()
⋮----
def test_message_to_langchain_unsupported_role() -> None
⋮----
message = Message(role="user", content="test")
⋮----
def test_map_message_to_instructor_unsupported_role() -> None
⋮----
message = SimpleNamespace(role="invalid", content="Hello")
⋮----
def test_message_validation() -> None
⋮----
message = Message(role="user", content="Hello")
⋮----
class DummyGenerativeModel(GenerativeModel)
⋮----
provider: LLMProvider = LLMProvider.CUSTOM
def __init__(self, model_name: str) -> None
def get_response(self, messages: list[Message]) -> Message
⋮----
async def aget_response(self, messages: list[Message]) -> Message
⋮----
def to_langchain(self) -> BaseChatModel
def test_model_to_string() -> None
⋮----
model = DummyGenerativeModel(model_name="test")
⋮----
class StructuredOutputTestModel(BaseModel)
⋮----
test_field: str
def test_langchain_model_structured_output_returns_dict() -> None
⋮----
base_chat_model = MagicMock(spec=BaseChatModel)
structured_output = MagicMock()
⋮----
model = LangChainGenerativeModel(client=base_chat_model, model_name="test")
result = model.get_structured_response(
⋮----
mock_set = MagicMock()
⋮----
mock_chat_anthropic = MagicMock(spec=ChatAnthropic)
⋮----
model = AnthropicGenerativeModel(model_name="test", api_key=SecretStr("test"))
⋮----
def test_anthropic_model_structured_output_returns_dict() -> None
def test_amazon_bedrock_model_structured_output_returns_dict() -> None
⋮----
mock_chat_bedrock = MagicMock(spec=BaseChatModel)
⋮----
mock_instructor_client = MagicMock()
⋮----
model = AmazonBedrockGenerativeModel(
⋮----
def test_anthropic_model_get_response_list_content() -> None
⋮----
result = model.get_response(
⋮----
def test_anthropic_model_get_response_with_human_message() -> None
⋮----
human_message = Message(role="user", content="Hello")
⋮----
def test_anthropic_model_structured_output_fallback_to_instructor() -> None
⋮----
mock_cache = MagicMock()
⋮----
model = AnthropicGenerativeModel(
_ = model.get_structured_response(
⋮----
def test_instructor_manual_cache(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
class DummyModel(BaseModel)
⋮----
mock_create = MagicMock(return_value=DummyModel())
⋮----
cache = MagicMock(spec=BaseCache)
⋮----
model = OpenAIGenerativeModel(
⋮----
@pytest.mark.asyncio
async def test_dummy_model_async_methods() -> None
⋮----
messages = [Message(role="user", content="Hello")]
response = await model.aget_response(messages)
⋮----
result = await model.aget_structured_response(messages, StructuredOutputTestModel)
⋮----
@pytest.mark.asyncio
async def test_langchain_model_async_structured_output_returns_dict() -> None
⋮----
async def mock_ainvoke(*_: Any, **__: Any) -> StructuredOutputTestModel
⋮----
result = await model.aget_structured_response(
⋮----
@pytest.mark.asyncio
async def test_langchain_model_async_get_response() -> None
⋮----
result = await model.aget_response(
⋮----
async def mock_ainvoke(*_: Any, **__: Any) -> None
⋮----
@pytest.mark.asyncio
async def test_anthropic_model_async_structured_output_returns_dict() -> None
⋮----
async def mock_ainvoke(*_: Any, **__: Any) -> dict[str, Any]
⋮----
@pytest.mark.asyncio
async def test_anthropic_model_async_get_response_list_content() -> None
⋮----
@pytest.mark.asyncio
async def test_anthropic_model_async_get_response_with_human_message() -> None
⋮----
@pytest.mark.asyncio
async def test_anthropic_model_async_structured_output_fallback_to_instructor() -> None
⋮----
async def mock_alookup(*_: Any, **__: Any) -> None
⋮----
async def mock_aupdate(*_: Any, **__: Any) -> None
⋮----
mock_create = MagicMock()
async def mock_create_async(*args: Any, **kwargs: Any) -> StructuredOutputTestModel
⋮----
_ = await model.aget_structured_response(
⋮----
@pytest.mark.asyncio
async def test_instructor_async_manual_cache(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
async def mock_create(*_: Any, **__: Any) -> DummyModel
⋮----
@pytest.mark.asyncio
async def test_model_async_methods_parameterized(model_class: type, model_kwargs: dict) -> None
⋮----
mock_model = MagicMock(spec=model_class)
⋮----
model = mock_model
⋮----
@pytest.mark.asyncio
async def test_openai_model_async_methods() -> None
⋮----
mock_chat_openai = MagicMock()
async def mock_ainvoke(*_: Any, **__: Any) -> AIMessage
⋮----
async def mock_structured_ainvoke(*_: Any, **__: Any) -> StructuredOutputTestModel
⋮----
@pytest.mark.asyncio
async def test_openai_model_async_instructor_fallback() -> None
⋮----
result = await model.aget_structured_response(messages, StepsOrError)
⋮----
@pytest.mark.asyncio
async def test_azure_openai_model_async_methods() -> None
⋮----
mock_chat_azure_openai = MagicMock()
⋮----
model = AzureOpenAIGenerativeModel(
⋮----
@pytest.mark.asyncio
async def test_mistralai_model_async_methods_if_available() -> None
⋮----
mock_chat_mistral = MagicMock()
async def mock_ainvoke(*_: Any, **__: Any) -> AIMessage
⋮----
async def mock_structured_ainvoke(*_: Any, **__: Any) -> dict[str, Any]
⋮----
model = MistralAIGenerativeModel(
⋮----
@pytest.mark.asyncio
async def test_google_model_async_methods_if_available() -> None
⋮----
mock_chat_google = MagicMock()
⋮----
model = GoogleGenAiGenerativeModel(
⋮----
@pytest.mark.asyncio
async def test_ollama_model_async_methods_if_available() -> None
⋮----
mock_chat_ollama = MagicMock()
⋮----
model = OllamaGenerativeModel(
⋮----
async def mock_create(*_: Any, **__: Any) -> StructuredOutputTestModel
⋮----
def test_get_context_window_size(model_name: str, expected_result: int) -> None
⋮----
model = LangChainGenerativeModel(client=get_mock_base_chat_model(), model_name=model_name)
result = model.get_context_window_size()
</file>

<file path="tests/unit/test_plan_builder.py">
def test_plan_builder_syntax() -> None
⋮----
plan_builder = (
base_plan = Plan(
⋮----
def test_plan_variable_no_steps() -> None
⋮----
plan = PlanBuilder("Find the best offers for a flight from London to New York")
⋮----
def test_plan_condition() -> None
⋮----
plan = (
</file>

<file path="tests/unit/test_plan_run.py">
@pytest.fixture
def mock_clarification() -> InputClarification
⋮----
@pytest.fixture
def plan_run(mock_clarification: InputClarification) -> PlanRun
def test_run_initialization() -> None
⋮----
plan_id = PlanUUID()
plan_run_inputs = {"$input1": LocalDataValue(value="test_input_value")}
plan_run = PlanRun(
⋮----
outstanding = plan_run.get_outstanding_clarifications()
⋮----
def test_run_get_outstanding_clarifications_none() -> None
def test_run_state_enum() -> None
def test_read_only_run_immutable() -> None
⋮----
read_only = ReadOnlyPlanRun.from_plan_run(plan_run)
⋮----
def test_read_only_step_immutable() -> None
⋮----
step = Step(task="add", output="$out")
read_only = ReadOnlyStep.from_step(step)
⋮----
def test_run_serialization() -> None
⋮----
plan_run_id = PlanRunUUID()
⋮----
json_str = plan_run.model_dump_json()
parsed_plan_run = PlanRun.model_validate_json(json_str)
⋮----
def test_get_clarification_for_step_with_matching_clarification(plan_run: PlanRun) -> None
⋮----
clarification = InputClarification(
⋮----
result = plan_run.get_clarification_for_step(ClarificationCategory.INPUT)
⋮----
def test_get_clarification_for_step_without_matching_clarification(plan_run: PlanRun) -> None
</file>

<file path="tests/unit/test_plan_storage.py">
def test_in_memory_storage_save_and_get_plan() -> None
⋮----
storage = InMemoryStorage()
plan = Plan(plan_context=PlanContext(query="query", tool_ids=[]), steps=[])
⋮----
retrieved_plan = storage.get_plan(plan.id)
⋮----
def test_in_memory_storage_save_and_get_plan_run() -> None
⋮----
plan_run = PlanRun(
⋮----
retrieved_plan_run = storage.get_plan_run(plan_run.id)
⋮----
def test_disk_file_storage_save_and_get_plan(tmp_path: Path) -> None
⋮----
storage = DiskFileStorage(storage_dir=str(tmp_path))
⋮----
def test_disk_file_storage_save_and_get_plan_run(tmp_path: Path) -> None
⋮----
plan = Plan(
⋮----
def test_disk_file_storage_save_and_get_plan_runs(tmp_path: Path) -> None
⋮----
runs = storage.get_plan_runs(PlanRunState.IN_PROGRESS)
⋮----
def test_disk_file_storage_invalid_plan_retrieval(tmp_path: Path) -> None
⋮----
invalid_file = tmp_path / "plan-invalid.json"
⋮----
def test_disk_file_storage_invalid_run_retrieval(tmp_path: Path) -> None
⋮----
invalid_file = tmp_path / "run-invalid.json"
</file>

<file path="tests/unit/test_plan.py">
def test_plan_serialization() -> None
def test_plan_uuid_assign() -> None
⋮----
plan = Plan(
⋮----
def test_read_only_plan_immutable() -> None
⋮----
read_only = ReadOnlyPlan.from_plan(plan)
⋮----
def test_read_only_plan_preserves_data() -> None
⋮----
original_plan = Plan(
read_only = ReadOnlyPlan.from_plan(original_plan)
⋮----
def test_read_only_plan_serialization() -> None
⋮----
json_str = read_only.model_dump_json()
deserialized = ReadOnlyPlan.model_validate_json(json_str)
⋮----
def test_plan_outputs_must_be_unique() -> None
def test_plan_outputs_and_conditions_must_be_unique() -> None
def test_pretty_print() -> None
⋮----
output = plan.pretty_print()
⋮----
def test_plan_builder_with_plan_input() -> None
⋮----
plan = (
⋮----
def test_plan_inputs_must_be_unique() -> None
def test_plan_input_equality() -> None
⋮----
original_input = PlanInput(name="$test", description="Test input")
identical_input = PlanInput(name="$test", description="Test input")
⋮----
different_descr_input = PlanInput(name="$test", description="Different description")
⋮----
different_name_input = PlanInput(name="$different", description="Test input")
</file>

<file path="tests/unit/test_portia_async.py">
query = "example query"
⋮----
plan = await portia.aplan(query)
⋮----
plan = await portia.aplan(query, tools=["add_tool"])
⋮----
@pytest.mark.asyncio
async def test_portia_aplan_with_use_cached_plan_success(portia: Portia) -> None
⋮----
cached_plan = Plan(
⋮----
plan = await portia.aplan(query, use_cached_plan=True)
⋮----
plan = await portia.aplan(query, use_cached_plan=False)
# Verify get_plan_by_query was NOT called
⋮----
# Verify a new plan was generated
⋮----
assert plan.id != cached_plan.id  # Should be a different plan
⋮----
@pytest.mark.asyncio
async def test_portia_aplan_with_use_cached_plan_and_tools(portia: Portia) -> None
⋮----
# Create a cached plan with tools
⋮----
# Mock the storage.get_plan_by_query to return the cached plan
⋮----
plan = await portia.aplan(query, tools=["add_tool"], use_cached_plan=True)
# Verify get_plan_by_query was called
⋮----
# Verify the cached plan was returned (tools parameter should be ignored when using
# cached plan)
⋮----
assert plan.plan_context.tool_ids == ["add_tool", "subtract_tool"]  # Original cached tools
⋮----
# Mock the storage.get_plan_by_query to raise StorageError
⋮----
# Mock the planning model to return a successful plan
⋮----
# Verify a new plan was generated despite the storage error
⋮----
assert plan.id != "plan-00000000-0000-0000-0000-000000000000"  # Not a default UUID
⋮----
# Mock the planning model to return a successful plan
⋮----
plan = await portia.aplan(query, plan_inputs=plan_inputs)
⋮----
# Should have plan inputs
⋮----
plan_run = await portia.arun(query)
⋮----
@pytest.mark.asyncio
async def test_portia_arun_query_tool_list(planning_model: MagicMock, telemetry: MagicMock) -> None
⋮----
addition_tool = AdditionTool()
clarification_tool = ClarificationTool()
portia = Portia(
⋮----
@pytest.mark.asyncio
async def test_portia_arun_query_disk_storage(planning_model: MagicMock) -> None
⋮----
config = Config.from_default(
tool_registry = ToolRegistry([AdditionTool(), ClarificationTool()])
portia = Portia(config=config, tools=tool_registry)
⋮----
# Use Path to check for the files
plan_files = list(Path(tmp_dir).glob("plan-*.json"))
run_files = list(Path(tmp_dir).glob("prun-*.json"))
⋮----
@pytest.mark.asyncio
async def test_portia_arun_with_use_cached_plan_success(portia: Portia) -> None
⋮----
# Create a cached plan
⋮----
plan_run = await portia.arun(query, use_cached_plan=True)
⋮----
# Verify the plan run was created from the cached plan
⋮----
# Verify a new plan was generated and run
⋮----
assert plan_run.plan_id != "plan-00000000-0000-0000-0000-000000000000"  # Not a default UUID
⋮----
# Mock the storage.get_plan_by_query to ensure it's not called
⋮----
plan_run = await portia.arun(query, use_cached_plan=False)
⋮----
@pytest.mark.asyncio
async def test_portia_arun_with_use_cached_plan_and_plan_run_inputs(portia: Portia) -> None
⋮----
plan_run_inputs = [PlanInput(name="$num_a", value=5)]
⋮----
plan_run = await portia.arun(query, plan_run_inputs=plan_run_inputs, use_cached_plan=True)
⋮----
@pytest.mark.asyncio
async def test_portia_arun_error_handling(portia: Portia, planning_model: MagicMock) -> None
⋮----
mock_plan_run = MagicMock()
mock_resumed_plan_run = MagicMock()
⋮----
result = await portia.arun_plan(plan)
⋮----
plan = Plan(
mock_agent = MagicMock()
⋮----
plan_run = await portia.arun_plan(plan, plan_run_inputs=plan_run_inputs)
⋮----
@pytest.mark.asyncio
async def test_portia_arun_plan_with_plan_uuid(portia: Portia, telemetry: MagicMock) -> None
⋮----
result = await portia.arun_plan(plan.id)
⋮----
@pytest.mark.asyncio
async def test_portia_arun_plan_with_new_plan(portia: Portia, planning_model: MagicMock) -> None
⋮----
@pytest.mark.asyncio
async def test_portia_arun_plan_with_uuid(portia: Portia) -> None
⋮----
result = await portia.arun_plan(plan.id.uuid)
⋮----
@pytest.mark.asyncio
async def test_portia_arun_plan_with_missing_inputs(portia: Portia) -> None
⋮----
required_input1 = PlanInput(name="$required1", description="Required input 1")
required_input2 = PlanInput(name="$required2", description="Required input 2")
⋮----
@pytest.mark.asyncio
async def test_portia_arun_plan_with_extra_input_when_expecting_none(portia: Portia) -> None
⋮----
extra_input = PlanInput(name="$extra", description="Extra unused input", value="value")
⋮----
plan_run = await portia.arun_plan(plan, plan_run_inputs=[extra_input])
⋮----
@pytest.mark.asyncio
async def test_portia_arun_plan_with_unknown_inputs_mixed_case(portia: Portia) -> None
⋮----
plan_run_inputs = [
⋮----
@pytest.mark.asyncio
async def test_portia_arun_plan_logs_unknown_input_warning(portia: Portia) -> None
⋮----
plan_run = portia.create_plan_run(plan)
plan_run = await portia.aresume(plan_run)
⋮----
@pytest.mark.asyncio
async def test_portia_aresume_after_interruption(portia: Portia, planning_model: MagicMock) -> None
⋮----
@pytest.mark.asyncio
async def test_portia_aresume_edge_cases(portia: Portia, planning_model: MagicMock) -> None
⋮----
plan_run = await portia.aresume(plan_run_id=plan_run.id)
⋮----
@pytest.mark.asyncio
async def test_portia_arun_invalid_state(portia: Portia, planning_model: MagicMock) -> None
⋮----
result = await portia.aresume(plan_run)
⋮----
@pytest.mark.asyncio
async def test_portia_ahandle_clarification(planning_model: MagicMock) -> None
⋮----
clarification_handler = TestClarificationHandler()
⋮----
mock_step_agent = mock.MagicMock()
mock_summarizer_agent = mock.MagicMock()
⋮----
plan = await portia.aplan("Raise a clarification")
⋮----
clarification_value = LocalDataValue(
final_value = LocalDataValue(value="I caught the clarification")
⋮----
@pytest.mark.asyncio
async def test_portia_aerror_clarification(portia: Portia, planning_model: MagicMock) -> None
⋮----
plan_run = await portia.arun("test query")
⋮----
step1 = Step(task="Step 1", inputs=[], output="$step1_result", condition="some_condition")
step2 = Step(task="Step 2", inputs=[], output="$step2_result")
⋮----
mock_introspection = MagicMock()
⋮----
mock_step_agent = MagicMock()
⋮----
plan_run = await portia.arun("Test query with skipped step")
⋮----
step1 = Step(task="Step 1", inputs=[], output="$step1_result")
step2 = Step(task="Step 2", inputs=[], output="$step2_result", condition="some_condition")
step3 = Step(task="Step 3", inputs=[], output="$step3_result")
⋮----
mock_introspection_complete = PreStepIntrospection(
final_output = LocalDataValue(
async def custom_handle_introspection(*args, **kwargs)
⋮----
plan_run = kwargs.get("plan_run")
⋮----
plan_run = await portia.arun("Test query with early completed execution")
⋮----
@pytest.mark.asyncio
async def test_portia_agenerate_introspection_outcome_complete(portia: Portia) -> None
⋮----
step = Step(task="Test step", inputs=[], output="$test_output", condition="some_condition")
⋮----
plan_run = PlanRun(
⋮----
mock_final_output = LocalDataValue(value="Final result", summary="Final summary")
⋮----
previous_output = LocalDataValue(value="Previous step result")
⋮----
@pytest.mark.asyncio
async def test_portia_agenerate_introspection_outcome_skip(portia: Portia) -> None
⋮----
@pytest.mark.asyncio
async def test_portia_agenerate_introspection_outcome_no_condition(portia: Portia) -> None
⋮----
step = Step(task="Test step", inputs=[], output="$test_output")
⋮----
@pytest.mark.asyncio
async def test_portia_aexecute_step_hooks(portia: Portia, planning_model: MagicMock) -> None
⋮----
execution_hooks = ExecutionHooks(
⋮----
step1 = Step(task="Step 1", tool_id="add_tool", output="$step1_result")
step2 = Step(task="Step 2", tool_id="add_tool", output="$step2_result")
⋮----
step_1_result = LocalDataValue(value="Step 1 result")
step_2_result = LocalDataValue(value="Step 2 result")
⋮----
plan_run = await portia.arun("Test execution hooks")
⋮----
plan = await portia.storage.aget_plan(plan_run.plan_id)
⋮----
@pytest.mark.asyncio
async def test_portia_aresume_with_skipped_steps(portia: Portia) -> None
⋮----
step2 = Step(task="Step 2", inputs=[], output="$step2_result", condition="true")
step3 = Step(task="Step 3", inputs=[], output="$step3_result", condition="false")
step4 = Step(task="Step 4", inputs=[], output="$step4_result", condition="false")
⋮----
current_step_index=1,  # Resume from step 2
⋮----
# Mock the storage to return our plan
⋮----
# Mock introspection agent to SKIP steps 3 and 4
⋮----
async def mock_introspection_outcome(*args, **kwargs):  # noqa: ANN002, ANN003, ANN202, ARG001
⋮----
if plan_run.current_step_index in (2, 3):  # pyright: ignore[reportOptionalMemberAccess] # Skip both step3 and step4
⋮----
# Mock step agent to return expected output for step 2 only (steps 3 and 4 will be skipped)
⋮----
# Mock the final output summarizer
expected_summary = "Combined summary of steps 1 and 2"
mock_summarizer = MagicMock()
⋮----
result_plan_run = await portia.aresume(plan_run)
⋮----
# Mock the first agent to raise an error
⋮----
plan_run = await portia.arun("Test execution hooks with error")
⋮----
assert execution_hooks.before_plan_run.call_count == 1  # pyright: ignore[reportFunctionMemberAccess, reportOptionalMemberAccess]
assert execution_hooks.before_step_execution.call_count == 1  # pyright: ignore[reportFunctionMemberAccess, reportOptionalMemberAccess]
assert execution_hooks.after_step_execution.call_count == 1  # pyright: ignore[reportFunctionMemberAccess, reportOptionalMemberAccess]
assert execution_hooks.after_plan_run.call_count == 1  # pyright: ignore[reportFunctionMemberAccess, reportOptionalMemberAccess]
⋮----
# Create execution hooks that will skip the first step
⋮----
before_step_execution=lambda plan, plan_run, step: (  # noqa: ARG005
⋮----
plan_run = await portia.arun("Test execution hooks with skip")
⋮----
execution_hooks.after_step_execution.assert_called_once_with(  # pyright: ignore[reportFunctionMemberAccess, reportOptionalMemberAccess]
⋮----
def failing_after_step_hook(plan, plan_run, step, output):  # noqa: ANN202, ARG001, ANN001
⋮----
plan_run = await portia.arun("Test after_step_execution hook exception")
⋮----
assert plan_run.outputs.final_output.get_value() == "Test after_step_execution hook exception"  # pyright: ignore[reportOptionalMemberAccess]
class ReadyTool(Tool)
⋮----
id: str = "ready_tool"
name: str = "Ready Tool"
description: str = "A dummy tool that can be set to ready or not ready."
args_schema: type[BaseModel] = _ArgsSchemaPlaceholder
output_schema: tuple[str, str] = ("ReadyResponse", "A response from the tool")
auth_url: str = "https://fake.portiaai.test/auth"
is_ready: bool | list[bool] = False
def _get_clarifications(self, plan_run_id: PlanRunUUID) -> list[Clarification]
def ready(self, ctx: ToolRunContext) -> ReadyResponse
⋮----
clarifications = self._get_clarifications(ctx.plan_run.id)
⋮----
def run(self, ctx: ToolRunContext) -> None
⋮----
@pytest.mark.asyncio
async def test_portia_acustom_tool_ready_not_ready() -> None
⋮----
ready_tool = ReadyTool()
⋮----
plan = PlanBuilder().step("", ready_tool.id).build()
plan_run = await portia.acreate_plan_run(plan, end_user="123")
await portia.storage.asave_plan(plan)  # Explicitly save plan for test
output_plan_run = await portia.aresume(plan_run)
⋮----
outstanding_clarification = output_plan_run.get_outstanding_clarifications()[0]
⋮----
@pytest.mark.asyncio
async def test_portia_acustom_tool_ready_resume_multiple_instances_of_same_tool() -> None
⋮----
plan = PlanBuilder().step("1", ready_tool.id).step("2", ready_tool.id).build()
⋮----
@pytest.mark.asyncio
async def test_portia_acustom_tool_ready_resume_multiple_custom_tools() -> None
⋮----
ready_tool = ReadyTool(id="ready_tool", auth_url="https://fake.portiaai.test/auth")
ready_tool_2 = ReadyTool(id="ready_tool_2", auth_url="https://fake.portiaai.test/auth2")
portia = Portia(config=get_test_config(), tools=[ready_tool, ready_tool_2])
plan = PlanBuilder().step("1", ready_tool.id).step("2", ready_tool_2.id).build()
⋮----
outstanding_clarifications = output_plan_run.get_outstanding_clarifications()
⋮----
@pytest.mark.asyncio
async def test_portia_arun_plan_with_all_plan_types_error_handling(portia: Portia) -> None
⋮----
# Plan objects are automatically saved to storage, so they never raise PlanNotFoundError
# Only PlanUUID can raise PlanNotFoundError
# Test with non-existent PlanUUID
⋮----
# Test that Plan objects work (they get auto-saved, no error)
non_existent_plan = Plan(
# This should succeed because Plan objects are auto-saved
⋮----
plan_run = await portia.arun_plan(non_existent_plan)
⋮----
result = await portia.aexecute_plan_run_and_handle_clarifications(plan, plan_run)
⋮----
@pytest.mark.asyncio
async def test_portia_ainitialize_end_user_default(portia: Portia) -> None
⋮----
async def save_end_user(end_user: EndUser) -> EndUser
⋮----
end_user = await portia.ainitialize_end_user()
⋮----
end_user = await portia.ainitialize_end_user(None)
⋮----
end_user = await portia.ainitialize_end_user("")
⋮----
end_user = await portia.ainitialize_end_user("123")
⋮----
end_user = await portia.ainitialize_end_user(EndUser(external_id="123"))
⋮----
@pytest.mark.asyncio
async def test_portia__aresolve_single_example_plan(portia: Portia) -> None
⋮----
# Create example plans for testing
example_plan_1 = Plan(
example_plan_2 = Plan(
# Save plans to storage
⋮----
# Test with Plan object (should return directly)
resolved_plan = await portia._aresolve_single_example_plan(example_plan_1)
assert resolved_plan is example_plan_1  # Should be same object
⋮----
# Test with PlanUUID (should load from storage)
resolved_plan = await portia._aresolve_single_example_plan(example_plan_2.id)
⋮----
# Test with plan ID string (should load from storage)
plan_id_string = str(example_plan_1.id)  # "plan-uuid"
resolved_plan = await portia._aresolve_single_example_plan(plan_id_string)
⋮----
# Test with non-existent PlanUUID (should raise PlanNotFoundError)
non_existent_uuid = PlanUUID.from_string("plan-99fc470b-4cbd-489b-b251-7076bf7e8f05")
⋮----
# Test with non-existent plan ID string (should raise PlanNotFoundError)
⋮----
# Test with invalid string format (should raise ValueError)
⋮----
# Test with invalid type (should raise TypeError)
⋮----
await portia._aresolve_single_example_plan(123)  # type: ignore[arg-type]
⋮----
@pytest.mark.asyncio
async def test_portia__aresolve_example_plans(portia: Portia) -> None
⋮----
# Test with None (should return None)
result = await portia._aresolve_example_plans(None)
⋮----
# Test with empty list (should return empty list)
result = await portia._aresolve_example_plans([])
⋮----
# Test with mixed types: Plan, PlanUUID, plan ID string
example_plans = [
⋮----
example_plan_1,  # Plan object
example_plan_2.id,  # PlanUUID
str(example_plan_1.id),  # plan ID string
⋮----
resolved_plans = await portia._aresolve_example_plans(example_plans)
# Verify all plans were resolved correctly
⋮----
assert resolved_plans[0] is example_plan_1  # Plan object should be returned directly
assert resolved_plans[1].id == example_plan_2.id  # PlanUUID resolved
assert resolved_plans[2].id == example_plan_1.id  # Plan ID string resolved
# Test error handling - one invalid plan in the list should raise error
example_plans_with_error = [
⋮----
PlanUUID.from_string("plan-99fc470b-4cbd-489b-b251-7076bf7e8f05"),  # Non-existent
⋮----
# Test with invalid string in list
example_plans_with_invalid_string = [
⋮----
"invalid query string",  # Should raise ValueError
⋮----
# Test with invalid type in list
example_plans_with_invalid_type = [
⋮----
123,  # Should raise TypeError
⋮----
await portia._aresolve_example_plans(example_plans_with_invalid_type)  # type: ignore[arg-type]
⋮----
@pytest.mark.asyncio
async def test_portia__aload_plan_by_uuid(portia: Portia) -> None
⋮----
# Create and save a plan
test_plan = Plan(
⋮----
# Test successful plan loading
loaded_plan = await portia._aload_plan_by_uuid(test_plan.id)
⋮----
# Test error case - non-existent plan UUID should raise PlanNotFoundError
⋮----
@pytest.mark.asyncio
async def test_portia__aresolve_string_example_plan(portia: Portia) -> None
⋮----
# Create and save a test plan
⋮----
# Test success case: valid plan ID string that exists
plan_id_string = str(test_plan.id)  # e.g., "plan-uuid"
resolved_plan = await portia._aresolve_string_example_plan(plan_id_string)
⋮----
# Test error case: string doesn't start with "plan-"
</file>

<file path="tests/unit/test_portia.py">
def test_portia_local_default_config_with_api_keys() -> None
⋮----
portia = Portia()
⋮----
# BrowserTool is in open_source_tool_registry but not in the default tool registry
# avaialble to the Portia instance. PDF reader is in open_source_tool_registry if
# Mistral API key is set, and isn't in the default tool registry.
# the Mistral API key here.
expected_diff = 1
⋮----
expected_diff = 2
⋮----
def test_portia_local_default_config_without_api_keys() -> None
⋮----
# Unset the Tavily and weather API and check that these aren't included in
⋮----
# registry Unfortunately this is determined when the registry file is imported, so we
# can't just mock the Mistral API key here.
expected_diff = 6
⋮----
expected_diff = 7
⋮----
def test_portia_run_query(portia: Portia, planning_model: MagicMock, telemetry: MagicMock) -> None
⋮----
query = "example query"
⋮----
plan_run = portia.run(query)
⋮----
def test_portia_run_query_tool_list(planning_model: MagicMock, telemetry: MagicMock) -> None
⋮----
addition_tool = AdditionTool()
clarification_tool = ClarificationTool()
portia = Portia(
⋮----
def test_portia_run_query_disk_storage(planning_model: MagicMock) -> None
⋮----
config = Config.from_default(
tool_registry = ToolRegistry([AdditionTool(), ClarificationTool()])
portia = Portia(config=config, tools=tool_registry)
⋮----
plan_files = list(Path(tmp_dir).glob("plan-*.json"))
run_files = list(Path(tmp_dir).glob("prun-*.json"))
⋮----
plan = portia.plan(query)
⋮----
plan = portia.plan(query, tools=["add_tool"])
⋮----
def test_portia_plan_with_use_cached_plan_success(portia: Portia) -> None
⋮----
cached_plan = Plan(
⋮----
plan = portia.plan(query, use_cached_plan=True)
⋮----
def test_portia_plan_with_use_cached_plan_false(portia: Portia, planning_model: MagicMock) -> None
⋮----
plan = portia.plan(query, use_cached_plan=False)
# Verify get_plan_by_query was NOT called
⋮----
# Verify a new plan was generated
⋮----
assert plan.id != cached_plan.id  # Should be a different plan
def test_portia_run_with_use_cached_plan_success(portia: Portia) -> None
⋮----
# Create a cached plan
⋮----
# Mock the storage.get_plan_by_query to return the cached plan
⋮----
plan_run = portia.run(query, use_cached_plan=True)
# Verify get_plan_by_query was called
⋮----
# Verify the plan run was created from the cached plan
⋮----
# Mock the storage.get_plan_by_query to raise StorageError
⋮----
# Mock the planning model to return a successful plan
⋮----
# Verify a new plan was generated and run
⋮----
assert plan_run.plan_id != "plan-00000000-0000-0000-0000-000000000000"  # Not a default UUID
def test_portia_run_with_use_cached_plan_false(portia: Portia, planning_model: MagicMock) -> None
⋮----
# Mock the planning model to return a successful plan
⋮----
# Mock the storage.get_plan_by_query to ensure it's not called
⋮----
plan_run = portia.run(query, use_cached_plan=False)
⋮----
def test_portia_plan_with_use_cached_plan_and_tools(portia: Portia) -> None
⋮----
tools = ["add_tool", "clarification_tool"]
⋮----
plan = portia.plan(query, tools=tools, use_cached_plan=True)
⋮----
def test_portia_run_with_use_cached_plan_and_plan_run_inputs(portia: Portia) -> None
⋮----
plan_run_inputs = [PlanInput(name="$num_a", value=5)]
⋮----
plan_run = portia.run(query, plan_run_inputs=plan_run_inputs, use_cached_plan=True)
⋮----
def test_portia_resume(portia: Portia, planning_model: MagicMock, telemetry: MagicMock) -> None
⋮----
plan_run = portia.create_plan_run(plan)
plan_run = portia.resume(plan_run)
⋮----
def test_portia_resume_after_interruption(portia: Portia, planning_model: MagicMock) -> None
⋮----
def test_portia_resume_edge_cases(portia: Portia, planning_model: MagicMock) -> None
⋮----
plan_run = portia.resume(plan_run_id=plan_run.id)
⋮----
def test_portia_run_invalid_state(portia: Portia, planning_model: MagicMock) -> None
⋮----
result = portia.resume(plan_run)
⋮----
plan_run = portia.wait_for_ready(plan_run)
⋮----
def update_run_state() -> None
⋮----
update_thread = threading.Thread(target=update_run_state)
⋮----
def test_portia_wait_for_ready_tool(portia: Portia) -> None
⋮----
mock_call_count = MagicMock()
⋮----
class ReadyTool(Tool)
⋮----
id: str = "ready_tool"
name: str = "Ready Tool"
description: str = "Returns a clarification"
output_schema: tuple[str, str] = (
def run(self, ctx: ToolRunContext, user_guidance: str) -> str
def ready(self, ctx: ToolRunContext) -> ReadyResponse
⋮----
step0 = Step(
step1 = Step(
plan = Plan(
unresolved_action = ActionClarification(
plan_run = PlanRun(
⋮----
def test_portia_run_query_with_summary(portia: Portia, planning_model: MagicMock) -> None
⋮----
query = "What activities can I do in London based on weather?"
weather_step = Step(
activities_step = Step(
⋮----
weather_output = LocalDataValue(value="Sunny and warm")
activities_output = LocalDataValue(value="Visit Hyde Park and have a picnic")
expected_summary = "Weather is sunny and warm in London, visit to Hyde Park for a picnic"
mock_step_agent = mock.MagicMock()
⋮----
mock_summarizer_agent = mock.MagicMock()
⋮----
def test_portia_sets_final_output_with_summary(portia: Portia) -> None
⋮----
mock_summarizer = mock.MagicMock()
⋮----
last_step_output = LocalDataValue(value="Visit Hyde Park and have a picnic")
output = portia._get_final_output(plan, plan_run, last_step_output)
⋮----
call_args = mock_summarizer.create_summary.call_args[1]
⋮----
def test_portia_sets_final_output_with_structured_summary(portia: Portia) -> None
⋮----
class WeatherOutput(BaseModel)
⋮----
temperature: str
activities: list[str]
class WeatherOutputWithSummary(WeatherOutput)
⋮----
fo_summary: str
expected_output = WeatherOutputWithSummary(
⋮----
last_step_output = LocalDataValue(value=expected_output)
⋮----
output_value = output.get_value()
⋮----
weather_summary = "sunny"
weather_output = LocalDataValue(value="The weather is sunny and warm", summary=weather_summary)
activities_summary = "picnic"
activities_output = LocalDataValue(
⋮----
plan_run = portia_with_agent_memory.run(query)
⋮----
def test_portia_get_final_output_handles_summary_error(portia: Portia) -> None
⋮----
mock_agent = mock.MagicMock()
⋮----
step_output = LocalDataValue(value="Some output")
final_output = portia._get_final_output(plan, plan_run, step_output)
⋮----
def test_portia_wait_for_ready_max_retries(portia: Portia) -> None
def test_portia_wait_for_ready_backoff_period(portia: Portia) -> None
def test_portia_resolve_clarification_error(portia: Portia) -> None
⋮----
clarification = InputClarification(
⋮----
def test_portia_resolve_clarification(portia: Portia, telemetry: MagicMock) -> None
⋮----
plan_run = portia.resolve_clarification(clarification, "test", plan_run)
⋮----
def test_portia_get_tool_for_step_none_tool_id() -> None
⋮----
portia = Portia(config=get_test_config(), tools=[AdditionTool()])
⋮----
step = Step(
tool = portia.get_tool(step.tool_id, plan_run)
⋮----
def test_get_llm_tool() -> None
⋮----
portia = Portia(config=get_test_config(), tools=example_tool_registry)
⋮----
def test_portia_run_plan(portia: Portia, planning_model: MagicMock, telemetry: MagicMock) -> None
⋮----
mock_plan_run = MagicMock()
mock_resumed_plan_run = MagicMock()
⋮----
result = portia.run_plan(plan)
⋮----
def test_portia_run_plan_with_new_plan(portia: Portia, planning_model: MagicMock) -> None
def test_portia_handle_clarification(planning_model: MagicMock) -> None
⋮----
clarification_handler = TestClarificationHandler()
⋮----
plan = portia.plan("Raise a clarification")
⋮----
def test_portia_error_clarification(portia: Portia, planning_model: MagicMock) -> None
⋮----
plan_run = portia.run("test query")
⋮----
def test_portia_run_with_introspection_skip(portia: Portia, planning_model: MagicMock) -> None
⋮----
step1 = Step(task="Step 1", inputs=[], output="$step1_result", condition="some_condition")
step2 = Step(task="Step 2", inputs=[], output="$step2_result")
⋮----
mock_introspection = MagicMock()
⋮----
mock_step_agent = MagicMock()
⋮----
plan_run = portia.run("Test query with skipped step")
⋮----
def test_portia_run_with_introspection_complete(portia: Portia, planning_model: MagicMock) -> None
⋮----
step1 = Step(task="Step 1", inputs=[], output="$step1_result")
step2 = Step(task="Step 2", inputs=[], output="$step2_result", condition="some_condition")
step3 = Step(task="Step 3", inputs=[], output="$step3_result")
⋮----
mock_introspection_complete = PreStepIntrospection(
final_output = LocalDataValue(
def custom_handle_introspection(*args, **kwargs)
⋮----
plan_run: PlanRun = kwargs.get("plan_run")
⋮----
plan_run = portia.run("Test query with early completed execution")
⋮----
def test_handle_introspection_outcome_complete(portia: Portia) -> None
⋮----
step = Step(task="Test step", inputs=[], output="$test_output", condition="some_condition")
⋮----
mock_final_output = LocalDataValue(value="Final result", summary="Final summary")
⋮----
previous_output = LocalDataValue(value="Previous step result")
⋮----
def test_handle_introspection_outcome_skip(portia: Portia) -> None
def test_handle_introspection_outcome_no_condition(portia: Portia) -> None
⋮----
step = Step(task="Test step", inputs=[], output="$test_output")
⋮----
def test_portia_resume_with_skipped_steps(portia: Portia) -> None
⋮----
step2 = Step(task="Step 2", inputs=[], output="$step2_result", condition="true")
step3 = Step(task="Step 3", inputs=[], output="$step3_result", condition="false")
step4 = Step(task="Step 4", inputs=[], output="$step4_result", condition="false")
⋮----
current_step_index=1,  # Resume from step 2
⋮----
# Mock the storage to return our plan
⋮----
# Mock introspection agent to SKIP steps 3 and 4
⋮----
def mock_introspection_outcome(*args, **kwargs):  # noqa: ANN002, ANN003, ANN202, ARG001
⋮----
plan_run = kwargs.get("plan_run")
if plan_run.current_step_index in (2, 3):  # pyright: ignore[reportOptionalMemberAccess] # Skip both step3 and step4
⋮----
# Mock step agent to return expected output for step 2 only (steps 3 and 4 will be skipped)
⋮----
# Mock the final output summarizer
expected_summary = "Combined summary of steps 1 and 2"
mock_summarizer = MagicMock()
⋮----
result_plan_run = portia.resume(plan_run)
⋮----
def test_portia_initialize_end_user(portia: Portia) -> None
⋮----
end_user = EndUser(external_id="123")
⋮----
# with no end user should return default
⋮----
# with empty end user should return default
⋮----
# with str should return full user
⋮----
# with full user should save + return
⋮----
storage_end_user = portia.storage.get_end_user(end_user.external_id)
⋮----
with pytest.raises(ValueError):  # noqa: PT011
⋮----
plan_run = portia.run(
⋮----
plan = portia.plan(
⋮----
mock_agent = MagicMock()
⋮----
# Mock the get_agent_for_step method to return our mock agent
⋮----
plan_run = portia.run_plan(plan, plan_run_inputs=plan_run_inputs)
⋮----
def test_portia_run_plan_with_missing_inputs(portia: Portia) -> None
⋮----
required_input1 = PlanInput(name="$required1", description="Required input 1")
required_input2 = PlanInput(name="$required2", description="Required input 2")
⋮----
# Try to run the plan without providing required inputs
with pytest.raises(ValueError):  # noqa: PT011
⋮----
# Should fail with just one of the two required
⋮----
# Should work if we provide both required inputs
⋮----
def test_portia_run_plan_with_extra_input_when_expecting_none(portia: Portia) -> None
⋮----
# Create a plan with no inputs
⋮----
plan_inputs=[],  # No inputs required
⋮----
# Run with input that isn't in the plan's inputs
extra_input = PlanInput(name="$extra", description="Extra unused input", value="value")
plan_run = portia.run_plan(plan, plan_run_inputs=[extra_input])
⋮----
def test_portia_run_plan_with_additional_extra_input(portia: Portia) -> None
⋮----
expected_input = PlanInput(
⋮----
unknown_input = PlanInput(name="$unknown", description="Unknown input", value="unknown_value")
⋮----
plan_run = portia.run_plan(
⋮----
def test_portia_run_plan_with_plan_uuid(portia: Portia, telemetry: MagicMock) -> None
⋮----
# Save the plan to storage
⋮----
# Mock the resume method to verify it gets called with the correct plan run
⋮----
plan_run = portia.run_plan(plan.id)
⋮----
def test_portia_run_plan_with_uuid(portia: Portia) -> None
def test_portia_execution_step_hooks(portia: Portia, planning_model: MagicMock) -> None
⋮----
execution_hooks = ExecutionHooks(
⋮----
# Create a plan with two steps
step1 = Step(task="Step 1", tool_id="add_tool", output="$step1_result")
step2 = Step(task="Step 2", tool_id="add_tool", output="$step2_result")
⋮----
step_1_result = LocalDataValue(value="Step 1 result")
step_2_result = LocalDataValue(value="Step 2 result")
⋮----
plan_run = portia.run("Test execution hooks")
⋮----
assert execution_hooks.before_plan_run.call_count == 1  # pyright: ignore[reportFunctionMemberAccess, reportOptionalMemberAccess]
assert execution_hooks.before_step_execution.call_count == 2  # pyright: ignore[reportFunctionMemberAccess, reportOptionalMemberAccess]
assert execution_hooks.after_step_execution.call_count == 2  # pyright: ignore[reportFunctionMemberAccess, reportOptionalMemberAccess]
assert execution_hooks.after_plan_run.call_count == 1  # pyright: ignore[reportFunctionMemberAccess, reportOptionalMemberAccess]
plan = portia.storage.get_plan(plan_run.plan_id)
execution_hooks.before_plan_run.assert_called_once_with(  # pyright: ignore[reportFunctionMemberAccess, reportOptionalMemberAccess]
⋮----
execution_hooks.before_step_execution.assert_any_call(  # pyright: ignore[reportFunctionMemberAccess, reportOptionalMemberAccess]
⋮----
execution_hooks.after_step_execution.assert_any_call(  # pyright: ignore[reportFunctionMemberAccess, reportOptionalMemberAccess]
⋮----
execution_hooks.after_plan_run.assert_called_once_with(  # pyright: ignore[reportFunctionMemberAccess, reportOptionalMemberAccess]
⋮----
def test_portia_execution_step_hooks_with_error(portia: Portia, planning_model: MagicMock) -> None
⋮----
# Mock the first agent to raise an error
⋮----
plan_run = portia.run("Test execution hooks with error")
⋮----
assert execution_hooks.before_step_execution.call_count == 1  # pyright: ignore[reportFunctionMemberAccess, reportOptionalMemberAccess]
assert execution_hooks.after_step_execution.call_count == 1  # pyright: ignore[reportFunctionMemberAccess, reportOptionalMemberAccess]
⋮----
def test_portia_execution_step_hooks_with_skip(portia: Portia, planning_model: MagicMock) -> None
⋮----
# Create execution hooks that will skip the first step
⋮----
before_step_execution=lambda plan, plan_run, step: (  # noqa: ARG005
⋮----
plan_run = portia.run("Test execution hooks with skip")
⋮----
execution_hooks.after_step_execution.assert_called_once_with(  # pyright: ignore[reportFunctionMemberAccess, reportOptionalMemberAccess]
⋮----
def failing_after_step_hook(plan, plan_run, step, output):  # noqa: ANN202, ARG001, ANN001
⋮----
plan_run = portia.run("Test after_step_execution hook exception")
⋮----
assert plan_run.outputs.final_output.get_value() == "Test after_step_execution hook exception"  # pyright: ignore[reportOptionalMemberAccess]
class MockPortiaTool(PortiaRemoteTool)
⋮----
id: str = "portia:mock_portia_tool"
name: str = "Mock Portia Tool"
description: str = "A dummy portia remote tool"
args_schema: type[BaseModel] = _ArgsSchemaPlaceholder
output_schema: tuple[str, str] = ("str", "A response from the tool")
def run(self, ctx: ToolRunContext) -> str:  # noqa: ARG002
class ReadyTool(Tool)
⋮----
description: str = "A dummy tool that can be set to ready or not ready."
⋮----
output_schema: tuple[str, str] = ("ReadyResponse", "A response from the tool")
auth_url: str = "https://fake.portiaai.test/auth"
is_ready: bool | list[bool] = False
def _get_clarifications(self, plan_run_id: PlanRunUUID) -> list[Clarification]
⋮----
is_ready = (
⋮----
return [  # pyright: ignore[reportReturnType]
⋮----
def ready(self, ctx: ToolRunContext) -> ReadyResponse
⋮----
clarifications = self._get_clarifications(ctx.plan_run.id)
⋮----
def run(self, ctx: ToolRunContext) -> None:  # noqa: ARG002
⋮----
@pytest.fixture
def mock_cloud_client() -> Iterator[httpx.Client]
⋮----
client = httpx.Client(base_url="https://fake.portiaai.test")
⋮----
portia_tool = MockPortiaTool(client=mock_cloud_client)
⋮----
plan = PlanBuilder().step("", portia_tool.id).build()
plan_run = portia.create_plan_run(plan, end_user="123")
portia.storage.save_plan(plan)  # Explicitly save plan for test
action_url = HttpUrl("https://example.com/auth")
⋮----
output_plan_run = portia.resume(plan_run)
⋮----
outstanding_clarification = output_plan_run.get_outstanding_clarifications()[0]
⋮----
portia_tool_2 = MockPortiaTool(id="portia:mock_portia_tool_2", client=mock_cloud_client)
⋮----
plan = PlanBuilder().step("", portia_tool.id).step("", portia_tool_2.id).build()
⋮----
def test_custom_tool_ready_not_ready() -> None
⋮----
ready_tool = ReadyTool()
⋮----
plan = PlanBuilder().step("", ready_tool.id).build()
⋮----
def test_custom_tool_ready_resume_multiple_instances_of_same_tool() -> None
⋮----
plan = PlanBuilder().step("1", ready_tool.id).step("2", ready_tool.id).build()
⋮----
def test_custom_tool_ready_resume_multiple_custom_tools() -> None
⋮----
ready_tool = ReadyTool(id="ready_tool", auth_url="https://fake.portiaai.test/auth")
ready_tool_2 = ReadyTool(id="ready_tool_2", auth_url="https://fake.portiaai.test/auth2")
portia = Portia(config=get_test_config(), tools=[ready_tool, ready_tool_2])
plan = PlanBuilder().step("1", ready_tool.id).step("2", ready_tool_2.id).build()
⋮----
outstanding_clarifications = output_plan_run.get_outstanding_clarifications()
⋮----
ready_tool = ReadyTool(is_ready=False)
⋮----
plan = PlanBuilder().step("", portia_tool.id).step("", ready_tool.id).build()
⋮----
class PortiaWithoutExecution(Portia)
⋮----
def _execute_plan_run(self, plan: Plan, plan_run: PlanRun) -> PlanRun:  # noqa: ARG002
⋮----
ready_tool = ReadyTool(is_ready=True)
# ExecutionHooks are required to trigger the wait_for_ready behaviour
⋮----
portia = PortiaWithoutExecution(
plan = PlanBuilder().step("", ready_tool.id).step("", portia_tool.id).build()
⋮----
# Initially the portia tool is not ready
# Have wait_for_ready check twice to iterate through the full loop
⋮----
# After a couple of iterations, the tool becomes ready
⋮----
class RaiseClarificationAgent(BaseExecutionAgent)
⋮----
def execute_sync(self) -> Output
class CustomPortia(Portia)
⋮----
def get_agent_for_step(self, step: Step, plan: Plan, plan_run: PlanRun) -> BaseExecutionAgent
⋮----
tool = self.get_tool(step.tool_id, plan_run)
⋮----
def test_tool_raise_clarification_all_remaining_tool_ready_status_rechecked() -> None
⋮----
ready_once_tool = ReadyTool(id="ready_once_tool", is_ready=[True, False])
portia = CustomPortia(
plan = (
⋮----
# Initially all tools are ready
⋮----
# Second time, a clarification is raised
⋮----
def test_portia_run_plan_with_all_plan_types(portia: Portia, telemetry: MagicMock) -> None
⋮----
# Create a test plan
⋮----
# Mock the resume method to verify it gets called correctly
⋮----
# Test 1: Run with Plan object
plan_run_1 = portia.run_plan(plan)
⋮----
# Test 2: Run with PlanUUID
plan_run_2 = portia.run_plan(plan.id)
⋮----
# Verify resume was called 2 times
⋮----
# Verify all plan runs have the same plan_id
⋮----
# Verify telemetry captured the correct plan types
telemetry_calls = telemetry.capture.call_args_list
⋮----
# Check telemetry for each plan type
⋮----
def test_portia_run_plan_with_all_plan_types_error_handling(portia: Portia) -> None
⋮----
# Plan objects are automatically saved to storage, so they never raise PlanNotFoundError
# Only PlanUUID can raise PlanNotFoundError
# Test with non-existent PlanUUID
⋮----
# Test that Plan objects work (they get auto-saved, no error)
non_existent_plan = Plan(
# This should succeed because Plan objects are auto-saved
⋮----
plan_run = portia.run_plan(non_existent_plan)
⋮----
def test_portia_example_plans_with_all_types(portia: Portia, planning_model: MagicMock) -> None
⋮----
# Create example plans
example_plan_1 = Plan(
example_plan_2 = Plan(
# Save plans to storage
⋮----
# Mock planning model
⋮----
# Test with mixed example_plans types: Plan, PlanUUID, string
example_plans = [
⋮----
example_plan_1,  # Plan object
example_plan_2.id,  # PlanUUID
"example query 2",  # string query
⋮----
# Mock the resolve method to return the expected plans
⋮----
plan = portia.plan("test query", example_plans=example_plans)
# Verify _resolve_example_plans was called with all types
⋮----
# Verify plan was created successfully
⋮----
def test_portia_resolve_example_plans_error_handling(portia: Portia) -> None
⋮----
# Test with non-existent plan ID string
⋮----
# Test with non-plan-ID string (should raise ValueError, not PlanNotFoundError)
⋮----
# Test with invalid type
⋮----
portia._resolve_example_plans([123])  # type: ignore[arg-type]  # Invalid type
# Test with raw UUID, should raise TypeError
⋮----
portia._resolve_example_plans([UUID("99fc470b-4cbd-489b-b251-7076bf7e8f05")])  # type: ignore[arg-type]
def test_portia_run_with_example_plans_all_types(portia: Portia, planning_model: MagicMock) -> None
⋮----
# Test with mixed example_plans types: Plan, PlanUUID
⋮----
# Test the run method with all example plan types
plan_run = portia.run("test query with examples", example_plans=example_plans)
# Verify the run completed successfully
⋮----
def test_portia_resolve_example_plans_with_all_types(portia: Portia) -> None
⋮----
# Test with all supported types: Plan, PlanUUID, plan ID string
⋮----
str(example_plan_2.id),  # plan ID string
⋮----
resolved_plans = portia._resolve_example_plans(example_plans)
# Verify all plans were resolved correctly
⋮----
assert resolved_plans[0].id == example_plan_1.id  # Plan object
assert resolved_plans[1].id == example_plan_2.id  # PlanUUID
assert resolved_plans[2].id == example_plan_2.id  # Plan ID string resolved to plan
⋮----
# Test with plan ID strings (like the user's example)
plan_id_1 = PlanUUID.from_string(str(example_plan_1.id))
plan_id_2 = str(example_plan_2.id)
example_plans = [plan_id_1, plan_id_2]
plan_run = portia.run("Get the weather in Paris", example_plans=example_plans)
⋮----
def test_portia_resolve_example_plans_with_plan_id_strings(portia: Portia) -> None
⋮----
plan_id_string_1 = str(example_plan_1.id)
plan_id_string_2 = str(example_plan_2.id)
example_plans: list[Plan | PlanUUID | str] = [plan_id_string_1, plan_id_string_2]
⋮----
def test_portia_resolve_example_plans_none(portia: Portia) -> None
⋮----
result = portia._resolve_example_plans(None)
⋮----
def test_portia_resolve_single_example_plan_invalid_type(portia: Portia) -> None
def test_portia_load_plan_by_uuid_exception(portia: Portia) -> None
⋮----
plan_uuid = PlanUUID.from_string("plan-12345678-1234-5678-1234-567812345678")
⋮----
def test_portia_resolve_string_example_plan_invalid_format(portia: Portia) -> None
def test_portia_resolve_string_example_plan_not_found(portia: Portia) -> None
⋮----
result = portia.execute_plan_run_and_handle_clarifications(plan, plan_run)
⋮----
def test_portia_run_plan_planv2_inside_async_context_raises_runtime_error(portia: Portia) -> None
⋮----
class MockStepV2(StepV2)
⋮----
async def run(self, run_data: RunContext) -> str
def describe(self) -> str
def to_legacy_step(self, plan: PlanV2) -> Step
plan_v2 = PlanV2(steps=[MockStepV2(step_name="test_step")], label="Test plan")
⋮----
async def async_function() -> PlanRun
</file>

<file path="tests/unit/test_storage_async.py">
@pytest.mark.asyncio
async def test_async_plan_storage_methods() -> None
⋮----
storage = InMemoryStorage()
plan = Plan(
⋮----
retrieved_plan = await storage.aget_plan(plan.id)
⋮----
exists = await storage.aplan_exists(plan.id)
⋮----
retrieved_plan = await storage.aget_plan_by_query("test query")
⋮----
@pytest.mark.asyncio
async def test_async_run_storage_methods() -> None
⋮----
plan_run = PlanRun(
⋮----
retrieved_run = storage.get_plan_run(plan_run.id)
⋮----
retrieved_run = await storage.aget_plan_run(plan_run.id)
⋮----
runs_response = await storage.aget_plan_runs()
⋮----
@pytest.mark.asyncio
async def test_async_additional_storage_methods() -> None
⋮----
tool_call = ToolCallRecord(
⋮----
end_user = EndUser(
saved_user = await storage.asave_end_user(end_user)
⋮----
retrieved_user = await storage.aget_end_user("test_user")
⋮----
@pytest.mark.asyncio
async def test_async_aget_plan_run_success(httpx_mock: HTTPXMock) -> None
⋮----
config = get_test_config(portia_api_key="test_api_key")
storage = PortiaCloudStorage(config)
plan_run_id = PlanRunUUID.from_string("prun-87654321-4321-8765-4321-876543210987")
mock_response_data = {
⋮----
result = await storage.aget_plan_run(plan_run_id)
⋮----
request = httpx_mock.get_requests()[0]
⋮----
@pytest.mark.asyncio
async def test_async_aget_plan_run_http_error(httpx_mock: HTTPXMock) -> None
⋮----
@pytest.mark.asyncio
async def test_async_aget_plan_run_request_exception(httpx_mock: HTTPXMock) -> None
⋮----
@pytest.mark.asyncio
async def test_async_aget_plan_runs_no_params(httpx_mock: HTTPXMock) -> None
⋮----
result = await storage.aget_plan_runs()
⋮----
plan_run_1 = result.results[0]
⋮----
plan_run_2 = result.results[1]
⋮----
@pytest.mark.asyncio
async def test_async_aget_plan_runs_with_state(httpx_mock: HTTPXMock) -> None
⋮----
result = await storage.aget_plan_runs(run_state=PlanRunState.COMPLETE)
⋮----
@pytest.mark.asyncio
async def test_async_aget_plan_runs_with_page(httpx_mock: HTTPXMock) -> None
⋮----
mock_response_data = {"results": [], "count": 0, "current_page": 2, "total_pages": 2}
⋮----
result = await storage.aget_plan_runs(page=2)
⋮----
@pytest.mark.asyncio
async def test_async_aget_plan_runs_both_params(httpx_mock: HTTPXMock) -> None
⋮----
result = await storage.aget_plan_runs(run_state=PlanRunState.FAILED, page=3)
⋮----
query_str = str(request.url.query)
⋮----
@pytest.mark.asyncio
async def test_async_aget_plan_runs_http_error(httpx_mock: HTTPXMock) -> None
⋮----
@pytest.mark.asyncio
async def test_async_aget_plan_runs_request_exception(httpx_mock: HTTPXMock) -> None
⋮----
@pytest.mark.asyncio
async def test_async_aget_end_user_success(httpx_mock: HTTPXMock) -> None
⋮----
external_id = "test_user_123"
⋮----
result = await storage.aget_end_user(external_id)
⋮----
@pytest.mark.asyncio
async def test_async_aget_end_user_minimal_data(httpx_mock: HTTPXMock) -> None
⋮----
external_id = "minimal_user"
⋮----
@pytest.mark.asyncio
async def test_async_aget_end_user_not_found(httpx_mock: HTTPXMock) -> None
⋮----
external_id = "nonexistent_user"
⋮----
@pytest.mark.asyncio
async def test_async_aget_end_user_server_error(httpx_mock: HTTPXMock) -> None
⋮----
external_id = "test_user"
⋮----
@pytest.mark.asyncio
async def test_async_aget_end_user_request_exception(httpx_mock: HTTPXMock) -> None
⋮----
@pytest.mark.asyncio
async def test_async_aget_end_user_special_characters(httpx_mock: HTTPXMock) -> None
⋮----
external_id = "user@domain.com"
⋮----
@pytest.mark.asyncio
async def test_async_aget_similar_plans_threaded_execution() -> None
⋮----
mock_plan = Plan(
mock_plans = [mock_plan]
⋮----
result = await storage.aget_similar_plans(query="test query", threshold=0.7, limit=5)
⋮----
@pytest.mark.asyncio
async def test_async_aget_similar_plans_default_parameters() -> None
⋮----
mock_plans = []
⋮----
result = await storage.aget_similar_plans("search query")
⋮----
@pytest.mark.asyncio
async def test_async_agent_memory_methods() -> None
⋮----
output = LocalDataValue(
result = await storage.asave_plan_run_output(
⋮----
retrieved_output = await storage.aget_plan_run_output(
⋮----
@pytest.mark.asyncio
async def test_async_disk_file_storage_methods() -> None
⋮----
storage = DiskFileStorage(temp_dir)
⋮----
@pytest.mark.asyncio
async def test_async_error_handling() -> None
⋮----
@pytest.mark.asyncio
async def test_async_concurrent_operations() -> None
⋮----
plans = []
⋮----
tasks = [storage.asave_plan(plan) for plan in plans]
⋮----
tasks = [storage.aget_plan(plan.id) for plan in plans]
retrieved_plans = await asyncio.gather(*tasks)
⋮----
@pytest.mark.asyncio
async def test_async_portia_cloud_storage(httpx_mock: HTTPXMock) -> None
⋮----
tool_call = get_test_tool_call(plan_run)
end_user = EndUser(external_id="123")
⋮----
@pytest.mark.asyncio
async def test_async_portia_cloud_storage_errors(httpx_mock: HTTPXMock) -> None
⋮----
@pytest.mark.asyncio
async def test_async_portia_cloud_agent_memory(httpx_mock: HTTPXMock) -> None
⋮----
agent_memory = PortiaCloudStorage(config)
⋮----
output = LocalDataValue(value="test value", summary="test summary")
⋮----
result = await agent_memory.asave_plan_run_output("test_output", output, plan_run.id)
⋮----
put_request = httpx_mock.get_requests()[0]
⋮----
result = await agent_memory.aget_plan_run_output("test_output", plan_run.id)
# Verify the returned output
⋮----
# Test getting an output when it is not cached locally
# Mock the metadata response
⋮----
# Mock the value response
⋮----
result = await agent_memory.aget_plan_run_output("test_output2", plan_run.id)
# Verify that both HTTP requests were made
assert len(httpx_mock.get_requests()) >= 3  # Previous requests + 2 from get
# Verify the metadata request
metadata_request = httpx_mock.get_requests()[-2]
⋮----
# Verify the value request
value_request = httpx_mock.get_requests()[-1]
⋮----
# Verify that it wrote to the local cache
⋮----
@pytest.mark.asyncio
async def test_async_portia_cloud_agent_memory_errors(httpx_mock: HTTPXMock) -> None
⋮----
mock_exception = RuntimeError("An error occurred.")
# Test async save_plan_run_output error
⋮----
# Test async get_plan_run_output error
⋮----
# Check with an output that's too large
⋮----
@pytest.mark.asyncio
async def test_async_similar_plans(httpx_mock: HTTPXMock) -> None
⋮----
mock_id = "plan-00000000-0000-0000-0000-000000000000"
mock_response = {
endpoint = config.portia_api_endpoint
url = f"{endpoint}/api/v0/plans/embeddings/search/"
⋮----
plans = await storage.aget_similar_plans("Test query")
⋮----
@pytest.mark.asyncio
async def test_async_similar_plans_error(httpx_mock: HTTPXMock) -> None
⋮----
@pytest.mark.asyncio
async def test_async_plan_exists_portia_cloud_storage(httpx_mock: HTTPXMock) -> None
⋮----
different_plan_id = PlanUUID()
⋮----
exists = await storage.aplan_exists(different_plan_id)
⋮----
@pytest.mark.asyncio
async def test_async_get_plan_by_query_portia_cloud_storage(httpx_mock: HTTPXMock) -> None
⋮----
mock_plan_response = {
⋮----
found_plan = await storage.aget_plan_by_query("test query")
⋮----
@pytest.mark.asyncio
async def test_async_get_plan_by_query_portia_cloud_storage_error(httpx_mock: HTTPXMock) -> None
⋮----
def raise_connection_error(_: httpx.Request) -> httpx.Response
</file>

<file path="tests/unit/test_storage.py">
def test_storage_base_classes() -> None
⋮----
class MyStorage(RunStorage, PlanStorage, AdditionalStorage)
⋮----
def save_plan(self, plan: Plan) -> None
def get_plan(self, plan_id: PlanUUID) -> Plan
def get_plan_by_query(self, query: str) -> Plan
def plan_exists(self, plan_id: PlanUUID) -> bool
def save_plan_run(self, plan_run: PlanRun) -> None
def get_plan_run(self, plan_run_id: PlanRunUUID) -> PlanRun
⋮----
def save_tool_call(self, tool_call: ToolCallRecord) -> None
def save_end_user(self, end_user: EndUser) -> EndUser
def get_end_user(self, external_id: str) -> EndUser
storage = MyStorage()
plan = Plan(plan_context=PlanContext(query="", tool_ids=[]), steps=[])
plan_run = PlanRun(
tool_call = get_test_tool_call(plan_run)
end_user = EndUser(external_id="123")
⋮----
def test_in_memory_storage() -> None
⋮----
storage = InMemoryStorage()
⋮----
saved_output_1 = storage.save_plan_run_output(
⋮----
saved_output_2 = storage.save_plan_run_output(
⋮----
# This just logs, but check it doesn't cause any issues
⋮----
end_user = EndUser(
⋮----
user = storage.get_end_user("123")
⋮----
def test_disk_storage(tmp_path: Path) -> None
⋮----
storage = DiskFileStorage(storage_dir=str(tmp_path))
⋮----
all_runs = storage.get_plan_runs()
⋮----
def test_portia_cloud_storage() -> None
⋮----
config = get_test_config(portia_api_key="test_api_key")
storage = PortiaCloudStorage(config)
plan = Plan(
⋮----
mock_response = MagicMock()
⋮----
def test_portia_cloud_storage_errors() -> None
⋮----
mock_exception = RuntimeError("An error occurred.")
⋮----
def test_portia_cloud_agent_memory(httpx_mock: HTTPXMock) -> None
⋮----
agent_memory = PortiaCloudStorage(config)
⋮----
output = LocalDataValue(value="test value", summary="test summary")
⋮----
result = agent_memory.save_plan_run_output("test_output", output, plan_run.id)
⋮----
put_request = httpx_mock.get_requests()[0]
⋮----
result = agent_memory.get_plan_run_output("test_output", plan_run.id)
⋮----
# Verify the returned output
⋮----
# Test getting an output when it is not cached locally
# Mock the metadata response
⋮----
# Mock the value response
⋮----
# Verify that both HTTP requests were made
assert len(httpx_mock.get_requests()) == 3  # 1 from save + 2 from get
# Verify the metadata request
metadata_request = httpx_mock.get_requests()[1]
⋮----
# Verify the value request
value_request = httpx_mock.get_requests()[2]
⋮----
# Verify that it wrote to the local cache
⋮----
def test_portia_cloud_agent_memory_local_cache_expiry() -> None
⋮----
mock_success_response = MagicMock()
⋮----
# Write 21 outputs to the cache (cache size is 20)
⋮----
# Check that the cache only stores 20 entries
cache_files = list(Path(agent_memory.cache_dir).glob("**/*.json"))
⋮----
def test_portia_cloud_agent_memory_errors() -> None
⋮----
# Check with an output that's too large
⋮----
def test_similar_plans(httpx_mock: HTTPXMock) -> None
⋮----
mock_id = "plan-00000000-0000-0000-0000-000000000000"
mock_response = {
endpoint = config.portia_api_endpoint
url = f"{endpoint}/api/v0/plans/embeddings/search/"
⋮----
plans = storage.get_similar_plans("Test query")
⋮----
def test_similar_plans_error(httpx_mock: HTTPXMock) -> None
def test_plan_exists_in_memory_storage() -> None
⋮----
different_plan_id = PlanUUID()
⋮----
def test_plan_exists_disk_storage(tmp_path: Path) -> None
def test_plan_exists_portia_cloud_storage() -> None
⋮----
mock_failure_response = MagicMock()
⋮----
def test_get_plan_by_query_in_memory_storage() -> None
⋮----
plan1 = Plan(
plan2 = Plan(
plan3 = Plan(
⋮----
found_plan = storage.get_plan_by_query("test query 1")
⋮----
found_plan = storage.get_plan_by_query("test query 2")
⋮----
empty_storage = InMemoryStorage()
⋮----
def test_get_plan_by_query_disk_storage(tmp_path: Path) -> None
⋮----
empty_dir = tmp_path / "empty"
⋮----
empty_storage = DiskFileStorage(storage_dir=str(empty_dir))
⋮----
mixed_storage = DiskFileStorage(storage_dir=str(tmp_path / "mixed"))
⋮----
non_plan_file = tmp_path / "mixed" / "not_a_plan.txt"
⋮----
def test_get_plan_by_query_disk_storage_multiple_plans(tmp_path: Path) -> None
⋮----
found_plan = storage.get_plan_by_query("same query")
⋮----
def test_get_plan_by_query_portia_cloud_storage(httpx_mock: HTTPXMock) -> None
⋮----
mock_plan_response = {
⋮----
found_plan = storage.get_plan_by_query("test query")
⋮----
def test_get_plan_by_query_portia_cloud_storage_error(httpx_mock: HTTPXMock) -> None
⋮----
def raise_connection_error(_: httpx.Request) -> httpx.Response
⋮----
def test_get_plan_by_query_edge_cases() -> None
⋮----
found_plan = storage.get_plan_by_query("")
⋮----
long_query = "a" * 1000
⋮----
found_plan = storage.get_plan_by_query(long_query)
⋮----
special_query = "test query with !@
⋮----
found_plan = storage.get_plan_by_query(special_query)
</file>

<file path="tests/unit/test_token_check.py">
def test_estimate_tokens(text: str, expected_tokens: int) -> None
⋮----
actual_tokens = estimate_tokens(text)
⋮----
model = MagicMock()
</file>

<file path="tests/unit/test_tool_async.py">
class AdditionToolSchema(BaseModel)
⋮----
a: int = Field(..., description="The first number to add")
b: int = Field(..., description="The second number to add")
class AsyncAdditionTool(Tool)
⋮----
id: str = "add_tool"
name: str = "Add Tool"
description: str = "Use this tool to add two numbers together, it takes two numbers a + b"
args_schema: type[BaseModel] = AdditionToolSchema
output_schema: tuple[str, str] = ("int", "int: The value of the addition")
def run(self, _: ToolRunContext, a: int, b: int) -> int
⋮----
class AsyncClarificationTool(ClarificationTool)
class AsyncErrorTool(ErrorTool)
class SyncOnlyTool(AdditionTool)
⋮----
@pytest.fixture
def async_add_tool() -> AsyncAdditionTool
⋮----
@pytest.fixture
def async_clarification_tool() -> AsyncClarificationTool
⋮----
@pytest.fixture
def async_error_tool() -> AsyncErrorTool
⋮----
@pytest.fixture
def sync_only_tool() -> SyncOnlyTool
⋮----
@pytest.fixture
def tool_context() -> ToolRunContext
⋮----
result = await async_add_tool.arun(tool_context, 5, 3)
⋮----
result = await async_clarification_tool.arun(tool_context, "test")
⋮----
result = await async_clarification_tool.arun(tool_context, "no_clarification")
⋮----
result = await async_add_tool._arun(tool_context, 5, 3)
⋮----
result = await async_clarification_tool._arun(tool_context, "test")
⋮----
clarification = result.get_value()
⋮----
clarification = artifact.get_value()
⋮----
result = await sync_only_tool._arun(tool_context, 5, 3)
⋮----
langchain_tool = async_add_tool.to_langchain(tool_context, sync=False)
⋮----
def test_to_langchain_sync(async_add_tool: AsyncAdditionTool, tool_context: ToolRunContext) -> None
⋮----
langchain_tool = async_add_tool.to_langchain(tool_context, sync=True)
⋮----
langchain_tool = async_add_tool.to_langchain_with_artifact(tool_context, sync=False)
⋮----
langchain_tool = async_add_tool.to_langchain_with_artifact(tool_context, sync=True)
⋮----
result = await langchain_tool.coroutine(5, 3)
⋮----
class AdditionOutput(BaseModel)
⋮----
result: int
class StructuredAsyncAdditionTool(Tool)
⋮----
id: str = "structured_async_addition_tool"
name: str = "Structured Async Addition Tool"
description: str = "A tool that adds two numbers together and returns a structured output"
⋮----
output_schema: tuple[str, str] = (
structured_output_schema: type[BaseModel] | None = AdditionOutput
def run(self, _: ToolRunContext, a: int, b: int) -> AdditionOutput
⋮----
@pytest.fixture
def structured_async_tool() -> StructuredAsyncAdditionTool
⋮----
result = await structured_async_tool._arun(tool_context, 5, 3)
⋮----
output = result.get_value()
⋮----
@pytest.mark.asyncio
async def test_async_structured_output_coercion(tool_context: ToolRunContext) -> None
⋮----
class CoercionTestTool(Tool)
⋮----
id: str = "coercion_test_tool"
name: str = "Coercion Test Tool"
description: str = "A tool that returns a dict"
⋮----
output_schema: tuple[str, str] = ("dict", "dict: The result of the addition")
⋮----
def run(self, _: ToolRunContext, a: int, b: int) -> dict[str, int]
async def arun(self, _: ToolRunContext, a: int, b: int) -> dict[str, int]
test_tool = CoercionTestTool()
result = await test_tool._arun(tool_context, 5, 3)
⋮----
@pytest.mark.asyncio
async def test_async_tool_with_no_args(tool_context: ToolRunContext) -> None
⋮----
class NoArgsTool(Tool)
⋮----
id: str = "no_args_tool"
name: str = "No Args Tool"
description: str = "A tool with no arguments"
output_schema: tuple[str, str] = ("str", "A simple string")
def run(self, _: ToolRunContext) -> str
async def arun(self, _: ToolRunContext) -> str
tool = NoArgsTool()
result = await tool._arun(tool_context)
⋮----
@pytest.mark.asyncio
async def test_async_tool_with_complex_args(tool_context: ToolRunContext) -> None
⋮----
class ComplexArgsSchema(BaseModel)
⋮----
data: dict[str, list[int]]
flag: bool = False
class ComplexArgsTool(Tool)
⋮----
id: str = "complex_args_tool"
name: str = "Complex Args Tool"
description: str = "A tool with complex arguments"
args_schema: type[BaseModel] = ComplexArgsSchema
output_schema: tuple[str, str] = ("dict", "The processed data")
⋮----
tool = ComplexArgsTool()
test_data = {"a": [1, 2, 3], "b": [4, 5, 6]}
result = await tool._arun(tool_context, test_data, False)
⋮----
result = await tool._arun(tool_context, test_data, True)
⋮----
tasks = [async_add_tool._arun(tool_context, i, i + 1) for i in range(5)]
results = await asyncio.gather(*tasks)
⋮----
direct_result = await async_add_tool.arun(tool_context, 5, 3)
⋮----
internal_result = await async_add_tool._arun(tool_context, 5, 3)
⋮----
langchain_result = await langchain_tool.coroutine(5, 3)
⋮----
internal_result = await sync_only_tool._arun(tool_context, 5, 3)
⋮----
langchain_tool = sync_only_tool.to_langchain(tool_context, sync=False)
</file>

<file path="tests/unit/test_tool_call.py">
class DummyInput(BaseModel)
⋮----
value: str
class UnserializableOutput
⋮----
def __init__(self, data) -> None
def test_serializes_basemodel_input_and_output() -> None
⋮----
input_obj = DummyInput(value="test")
output_obj = DummyInput(value="result")
record = ToolCallRecord(
ser_input = record.serialize_input()
ser_output = record.serialize_output()
⋮----
def test_serializes_builtin_types_directly() -> None
def test_unserializable_objects_are_flagged() -> None
</file>

<file path="tests/unit/test_tool_decorator.py">
def test_basic_tool_decorator() -> None
⋮----
@tool
    def add_numbers(a: int, b: int) -> int
tool_instance = add_numbers()
⋮----
schema = tool_instance.args_schema
⋮----
ctx = get_test_tool_context()
result = tool_instance.run(ctx, a=3, b=4)
⋮----
def test_tool_with_optional_parameters() -> None
⋮----
@tool
    def greet_user(name: str, greeting: str = "Hello") -> str
tool_instance = greet_user()
⋮----
result = tool_instance.run(ctx, name="Alice")
⋮----
result = tool_instance.run(ctx, name="Bob", greeting="Hi")
⋮----
def test_tool_with_context_parameter() -> None
⋮----
@tool
    def get_user_id(ctx: ToolRunContext) -> str
tool_instance = get_user_id()
⋮----
result = tool_instance.run(ctx)
⋮----
def test_tool_with_context_named_context() -> None
⋮----
@tool
    def get_plan_id(context: ToolRunContext) -> str
tool_instance = get_plan_id()
⋮----
def test_tool_with_mixed_parameters() -> None
⋮----
@tool
    def personalized_message(message: str, ctx: ToolRunContext, prefix: str = "Message") -> str
tool_instance = personalized_message()
⋮----
result = tool_instance.run(ctx, message="Hello World")
⋮----
result = tool_instance.run(ctx, message="Test", prefix="Alert")
⋮----
def test_tool_with_complex_types() -> None
⋮----
@tool
    def process_data(items: list[str], count: int | None = None) -> dict[str, int]
⋮----
count = len(items)
⋮----
tool_instance = process_data()
⋮----
result = tool_instance.run(ctx, items=["a", "b", "c"])
⋮----
result = tool_instance.run(ctx, items=["x", "y"], count=5)
⋮----
def test_tool_raises_errors() -> None
⋮----
@tool
    def failing_tool(should_fail: bool, error_type: str = "soft") -> str
tool_instance = failing_tool()
⋮----
result = tool_instance.run(ctx, should_fail=False)
⋮----
def test_weather_tool_example(monkeypatch: pytest.MonkeyPatch) -> None
⋮----
@tool
    def weather_tool(city: str) -> str
⋮----
api_key = os.getenv("OPENWEATHERMAP_API_KEY")
⋮----
tool_instance = weather_tool()
⋮----
city_field = schema.model_fields["city"]
⋮----
def test_tool_class_naming() -> None
⋮----
@tool
    def my_custom_tool(value: str) -> str
tool_instance = my_custom_tool()
⋮----
def test_tool_validation_missing_return_type() -> None
⋮----
def invalid_tool_func(a: int, b: int)
⋮----
def test_tool_validation_non_callable() -> None
def test_tool_args_schema_generation() -> None
⋮----
tool_instance = complex_tool()
⋮----
schema_instance = schema(
⋮----
def test_tool_to_langchain() -> None
⋮----
@tool
    def simple_tool(text: str) -> str
tool_instance = simple_tool()
⋮----
lc_tool = tool_instance.to_langchain(ctx)
⋮----
def test_tool_serialization() -> None
⋮----
@tool
    def serializable_tool(data: str) -> str
tool_instance = serializable_tool()
str_repr = str(tool_instance)
⋮----
json_data = tool_instance.model_dump_json()
⋮----
def test_annotated_string_description() -> None
⋮----
tool_instance = say_hello()
⋮----
name_field = schema.model_fields["name"]
⋮----
def test_annotated_field_description() -> None
⋮----
tool_instance = calculate_area()
⋮----
length_field = schema.model_fields["length"]
width_field = schema.model_fields["width"]
⋮----
result = tool_instance.run(ctx, length=5.0, width=3.0)
⋮----
def test_mixed_annotation_patterns() -> None
⋮----
tool_instance = mixed_function()
⋮----
fields = schema.model_fields
⋮----
description = fields["required_regular"].description
⋮----
optional_field = fields["optional_annotated"]
⋮----
min_len_constraint = None
⋮----
min_len_constraint = meta
⋮----
result = tool_instance.run(ctx, required_annotated="test", required_regular=42)
⋮----
def test_get_type_hints_exception_handling() -> None
⋮----
sig = inspect.signature(problematic_function)
schema = _create_args_schema(sig, "test_func", problematic_function)
⋮----
def test_empty_parameter_annotation_fallback() -> None
⋮----
def test_func(param) -> str
sig = inspect.signature(test_func)
schema = _create_args_schema(sig, "test_func", test_func)
⋮----
param_field = schema.model_fields["param"]
⋮----
def test_malformed_annotated_type() -> None
⋮----
class MockAnnotated
⋮----
param = inspect.Parameter("test_param", inspect.Parameter.POSITIONAL_OR_KEYWORD)
⋮----
def test_field_with_custom_default() -> None
⋮----
tool_instance = tool_with_field_default()
⋮----
def test_tool_with_invalid_annotation_metadata() -> None
def test_tool_description_length_validation() -> None
⋮----
def tool_with_long_description() -> str
⋮----
tool_class = tool(tool_with_long_description)
⋮----
def test_tool_with_context_parameter_name_invalid() -> None
</file>

<file path="tests/unit/test_tool_registry.py">
MOCK_TOOL_ID = "mock_tool"
OTHER_MOCK_TOOL_ID = "other_mock_tool"
def test_tool_registry_register_tool() -> None
⋮----
tool_registry = ToolRegistry()
⋮----
tool1 = tool_registry.get_tool(MOCK_TOOL_ID)
⋮----
tool2 = tool_registry.get_tool(MOCK_TOOL_ID)
⋮----
def test_tool_registry_get_and_plan_run() -> None
⋮----
ctx = get_test_tool_context()
⋮----
def test_tool_registry_get_tools() -> None
⋮----
tool_registry = ToolRegistry(
tools = tool_registry.get_tools()
⋮----
def test_tool_registry_contains() -> None
def test_tool_registry_iter() -> None
def test_tool_registry_len() -> None
def test_tool_registry_match_tools() -> None
⋮----
matched_tools = tool_registry.match_tools(tool_ids=[MOCK_TOOL_ID])
⋮----
matched_tools = tool_registry.match_tools(
⋮----
matched_tools = tool_registry.match_tools(tool_ids=["non_existent_tool"])
⋮----
matched_tools = tool_registry.match_tools()
⋮----
def test_combined_tool_registry_duplicate_tool() -> None
⋮----
tool_registry = ToolRegistry([MockTool(id=MOCK_TOOL_ID)])
other_tool_registry = ToolRegistry(
combined_tool_registry = tool_registry + other_tool_registry
tool1 = combined_tool_registry.get_tool(MOCK_TOOL_ID)
⋮----
def test_combined_tool_registry_get_tool() -> None
def test_combined_tool_registry_get_tools() -> None
⋮----
tools = combined_tool_registry.get_tools()
⋮----
def test_combined_tool_registry_match_tools() -> None
⋮----
matched_tools = combined_tool_registry.match_tools(tool_ids=[MOCK_TOOL_ID])
⋮----
matched_tools = combined_tool_registry.match_tools(
⋮----
matched_tools = combined_tool_registry.match_tools(tool_ids=["non_existent_tool"])
⋮----
def test_tool_registry_add_operators(mocker: MockerFixture) -> None
⋮----
mock_logger = mocker.Mock()
⋮----
registry1 = ToolRegistry([MockTool(id=MOCK_TOOL_ID)])
registry2 = ToolRegistry([MockTool(id=OTHER_MOCK_TOOL_ID)])
tool_list = [MockTool(id="tool3")]
combined = registry1 + registry2
⋮----
combined = registry1 + tool_list
⋮----
combined = tool_list + registry1
⋮----
duplicate_registry = ToolRegistry([MockTool(id=MOCK_TOOL_ID)])
combined = registry1 + duplicate_registry
⋮----
def test_in_memory_tool_registry_from_local_tools() -> None
⋮----
tool_registry = InMemoryToolRegistry.from_local_tools([MockTool(id=MOCK_TOOL_ID)])
⋮----
def test_tool_registry_filter_tools() -> None
⋮----
tool_registry = ToolRegistry([MockTool(id=MOCK_TOOL_ID), MockTool(id=OTHER_MOCK_TOOL_ID)])
filtered_registry = tool_registry.filter_tools(lambda tool: tool.id == MOCK_TOOL_ID)
filtered_tools = filtered_registry.get_tools()
⋮----
def test_portia_tool_registry_missing_required_args() -> None
def test_portia_tool_registry_load_tools(httpx_mock: HTTPXMock) -> None
⋮----
mock_response_data = {
⋮----
client = httpx.Client(base_url="https://api.example.com")
tools = PortiaToolRegistry(client=client)
⋮----
tool1 = tools.get_tool("test_tool_1")
⋮----
tool2 = tools.get_tool("test_tool_2")
⋮----
def test_portia_tool_registry_load_tools_fallback_v1(httpx_mock: HTTPXMock) -> None
⋮----
mock_response_data = [
⋮----
expected_calls = [
⋮----
def test_portia_tool_registry_load_tools_http_error(httpx_mock: HTTPXMock) -> None
def test_tool_registry_with_tool_description() -> None
⋮----
mock_tool_1 = MockTool(id=MOCK_TOOL_ID, description="mock tool 1")
mock_tool_2 = MockTool(id=OTHER_MOCK_TOOL_ID, description="mock tool 2")
tool_registry = ToolRegistry([mock_tool_1, mock_tool_2])
⋮----
def test_tool_registry_with_tool_description_overwrite() -> None
def test_tool_registry_with_tool_description_tool_id_not_found(mocker: MockerFixture) -> None
def test_tool_registry_reconfigure_llm_tool() -> None
⋮----
registry = ToolRegistry(open_source_tool_registry.get_tools())
llm_tool = registry.get_tool("llm_tool")
⋮----
@pytest.fixture
def mock_get_mcp_session() -> Iterator[None]
⋮----
mock_session = MagicMock(spec=ClientSession)
⋮----
@pytest.fixture
def mcp_tool_registry(mock_get_mcp_session: None) -> McpToolRegistry
⋮----
@pytest.mark.usefixtures("mock_get_mcp_session")
def test_mcp_tool_registry_from_sse_connection() -> None
⋮----
mcp_registry_sse = McpToolRegistry.from_sse_connection(
⋮----
@pytest.mark.usefixtures("mock_get_mcp_session")
async def test_mcp_tool_registry_from_sse_connection_async() -> None
⋮----
mcp_registry_sse = await McpToolRegistry.from_sse_connection_async(
⋮----
@pytest.mark.usefixtures("mock_get_mcp_session")
async def test_mcp_tool_registry_get_tools_async() -> None
⋮----
mcp_registry_stdio = await McpToolRegistry.from_stdio_connection_async(
⋮----
def test_mcp_tool_registry_get_tools(mcp_tool_registry: McpToolRegistry) -> None
⋮----
tools = mcp_tool_registry.get_tools()
⋮----
def test_mcp_tool_registry_get_tool(mcp_tool_registry: McpToolRegistry) -> None
⋮----
tool = mcp_tool_registry.get_tool("mcp:mock_mcp:test_tool")
⋮----
def test_mcp_tool_registry_filters_bad_tools() -> None
⋮----
registry = McpToolRegistry.from_stdio_connection(
⋮----
def test_generate_pydantic_model_from_json_schema() -> None
⋮----
json_schema = {
model = generate_pydantic_model_from_json_schema("TestModel", json_schema)
⋮----
address_type = model.model_fields["address"].annotation
⋮----
def test_generate_pydantic_model_from_json_schema_min_max_length() -> None
⋮----
model = generate_pydantic_model_from_json_schema("TestMinMaxModel", json_schema)
⋮----
def test_generate_pydantic_model_from_json_schema_union_types() -> None
⋮----
model = generate_pydantic_model_from_json_schema("TestUnionModel", json_schema)
⋮----
def test_generate_pydantic_model_from_json_schema_doesnt_handle_none_for_non_union_fields() -> None
⋮----
model = generate_pydantic_model_from_json_schema("TestNullSchema", json_schema)
⋮----
def test_generate_pydantic_model_from_json_schema_not_single_type_or_union_field() -> None
def test_generate_pydantic_model_from_json_schema_handles_omissible_fields() -> None
⋮----
model = generate_pydantic_model_from_json_schema("TestOmissibleFields", json_schema)
⋮----
deserialized = model.model_validate({"name": "John"})
⋮----
model_1 = generate_pydantic_model_from_json_schema(
model_2 = generate_pydantic_model_from_json_schema(
⋮----
def test_generate_pydantic_model_from_json_schema_handles_number_type() -> None
⋮----
model = generate_pydantic_model_from_json_schema("TestNumberType", json_schema)
⋮----
int_object = model.model_validate({"number": 1})
⋮----
float_object = model.model_validate({"number": 1.23})
⋮----
def test_generate_pydantic_model_from_json_schema_handles_empty_enum() -> None
⋮----
model = generate_pydantic_model_from_json_schema("TestEmptyEnum", json_schema)
⋮----
@pytest.fixture
def additional_properties_dict_json_schema() -> dict[str, Any]
⋮----
json_schema = additional_properties_dict_json_schema
model = generate_pydantic_model_from_json_schema("TestAdditionalPropertiesDict", json_schema)
deserialized = model.model_validate(input_json_object)
⋮----
@pytest.mark.usefixtures("mock_get_mcp_session")
def test_mcp_tool_registry_from_streamable_http_connection() -> None
⋮----
mcp_registry_streamable_http = McpToolRegistry.from_streamable_http_connection(
⋮----
@pytest.mark.usefixtures("mock_get_mcp_session")
async def test_mcp_tool_registry_from_streamable_http_connection_async() -> None
⋮----
mcp_registry_streamable_http = await McpToolRegistry.from_streamable_http_connection_async(
⋮----
def test_mcp_tool_registry_load_tools_error_in_async() -> None
⋮----
class CustomError(Exception)
⋮----
def test_mcp_tool_registry_loads_from_string() -> None
⋮----
config_str = """{
⋮----
registry = McpToolRegistry.from_stdio_connection_raw(config_str)
⋮----
tool = next(iter(registry))
⋮----
config_dict = {
⋮----
registry = McpToolRegistry.from_stdio_connection_raw(config_dict)
⋮----
broken_config_str = "{"
⋮----
invalid_config_str = """{}"""
</file>

<file path="tests/unit/test_tool_wrapper.py">
class MockStorage(AdditionalStorage)
⋮----
def __init__(self) -> None
def save_tool_call(self, tool_call: ToolCallRecord) -> None
def save_end_user(self, end_user: EndUser) -> EndUser
def get_end_user(self, external_id: str) -> EndUser
⋮----
end_user = EndUser(external_id=external_id)
⋮----
@pytest.fixture
def mock_tool() -> Tool
⋮----
@pytest.fixture
def mock_storage() -> MockStorage
def test_tool_call_wrapper_initialization(mock_tool: Tool, mock_storage: MockStorage) -> None
⋮----
wrapper = ToolCallWrapper(child_tool=mock_tool, storage=mock_storage, plan_run=plan_run)
⋮----
def test_tool_call_wrapper_run_success(mock_tool: Tool, mock_storage: MockStorage) -> None
⋮----
wrapper = ToolCallWrapper(mock_tool, mock_storage, plan_run)
ctx = get_test_tool_context()
result = wrapper.run(ctx, 1, 2)
⋮----
tool = ErrorTool()
⋮----
wrapper = ToolCallWrapper(tool, mock_storage, plan_run)
⋮----
tool = ClarificationTool()
⋮----
result = wrapper.run(ctx, "new clarification")
⋮----
def test_tool_call_wrapper_run_records_latency(mock_tool: Tool, mock_storage: MockStorage) -> None
def test_tool_call_wrapper_run_returns_none(mock_storage: MockStorage) -> None
⋮----
wrapper = ToolCallWrapper(NoneTool(), mock_storage, plan_run)
</file>

<file path="tests/unit/test_tool.py">
@pytest.fixture
def add_tool() -> AdditionTool
⋮----
@pytest.fixture
def clarification_tool() -> ClarificationTool
def test_tool_initialization(add_tool: AdditionTool) -> None
def test_tool_initialization_long_description() -> None
⋮----
class FakeAdditionTool(AdditionTool)
⋮----
description: str = "this is a description" * 1000
⋮----
def test_run_signature_validation() -> None
⋮----
class TestArgSchema(BaseModel)
⋮----
foo: str
class BadTool(Tool)
⋮----
id: str = "bad_tool"
name: str = "Bad Tool"
description: str = "bad"
args_schema: type[BaseModel] = TestArgSchema
output_schema: tuple[str, str] = ("str", "out")
def run(self, ctx: ToolRunContext, foo: int) -> str
⋮----
def test_run_signature_validation_complex_type() -> None
⋮----
class ComplexSchema(BaseModel)
⋮----
foo: dict[str, list[int]]
⋮----
id: str = "bad_tool_complex"
name: str = "Bad Tool Complex"
⋮----
args_schema: type[BaseModel] = ComplexSchema
⋮----
def run(self, ctx: ToolRunContext, foo: list[str]) -> str
⋮----
def test_run_signature_context_type_required() -> None
⋮----
id: str = "bad_tool_ctx"
name: str = "Bad Tool Context"
⋮----
def run(self, ctx: int, foo: str) -> str
⋮----
def test_run_signature_validation_no_args() -> None
⋮----
class TestTool(Tool)
⋮----
id: str = "test_tool"
name: str = "Test Tool"
description: str = "test"
⋮----
def run(self) -> str
⋮----
def test_tool_to_langchain() -> None
⋮----
tool = AdditionTool()
⋮----
def test_run_method(add_tool: AdditionTool) -> None
⋮----
ctx = get_test_tool_context()
result = add_tool.run(ctx, a, b)
⋮----
def test_handle(add_tool: AdditionTool) -> None
def test_run_method_with_uncaught_error() -> None
⋮----
tool = ErrorTool()
⋮----
def test_ready() -> None
def test_tool_serialization() -> None
def test_remote_tool_run_with_pydantic_model(httpx_mock: HTTPXMock) -> None
⋮----
endpoint = "https://api.fake-portia.test"
⋮----
class DoubleNestedSchema(BaseModel)
⋮----
d: int
class NestedSchema(BaseModel)
⋮----
c: int
double_nested: DoubleNestedSchema
⋮----
a: int
b: int
sub: NestedSchema
tool = PortiaRemoteTool(
res = tool.run(
⋮----
def test_remote_tool_hard_error_from_server(httpx_mock: HTTPXMock) -> None
⋮----
content = {
⋮----
def test_remote_tool_run_with_unserializable_object() -> None
⋮----
class UnserializableObject
⋮----
def test_remote_tool_soft_error(httpx_mock: HTTPXMock) -> None
def test_remote_tool_bad_response(httpx_mock: HTTPXMock) -> None
def test_remote_tool_run_unhandled_error(httpx_mock: HTTPXMock) -> None
⋮----
def test_remote_tool_ready(httpx_mock: HTTPXMock, response_json: dict, is_ready: bool) -> None
⋮----
def test_remote_tool_ready_error(httpx_mock: HTTPXMock, status_code: int, is_ready: bool) -> None
def test_remote_tool_action_clarifications(httpx_mock: HTTPXMock) -> None
⋮----
output = tool.run(ctx)
⋮----
def test_remote_tool_input_clarifications(httpx_mock: HTTPXMock) -> None
def test_remote_tool_mc_clarifications(httpx_mock: HTTPXMock) -> None
def test_remote_tool_value_confirm_clarifications(httpx_mock: HTTPXMock) -> None
def test_portia_mcp_tool_call() -> None
⋮----
mock_session = MagicMock(spec=ClientSession)
⋮----
class MyEnum(str, Enum)
⋮----
A = "A"
⋮----
a: MyEnum
⋮----
tool = PortiaMcpTool(
expected = (
⋮----
tool_result = tool.run(get_test_tool_context(), a=1, b=2)
⋮----
def test_portia_mcp_tool_call_with_error() -> None
⋮----
@pytest.mark.asyncio
async def test_portia_mcp_tool_call_with_timeout() -> None
⋮----
mock_mcp_session = MockMcpSessionWrapper(
⋮----
@pytest.mark.asyncio
async def test_portia_mcp_tool_call_with_tool_call_timeout() -> None
⋮----
async def sleep_task(*args: Any, **kwargs: Any) -> None
⋮----
@pytest.mark.asyncio
async def test_portia_mcp_tool_call_with_other_mcp_error() -> None
def test_flatten_exceptions() -> None
⋮----
value_error_1 = ValueError("test1")
value_error_2 = ValueError("test3")
type_error_1 = TypeError("test2")
eg = ExceptionGroup(
⋮----
@pytest.mark.asyncio
async def test_portia_mcp_tool_async_call() -> None
⋮----
tool_result = await tool.arun(get_test_tool_context(), a=1, b=2)
⋮----
@pytest.mark.asyncio
async def test_portia_mcp_tool_async_call_with_error() -> None
⋮----
@pytest.mark.asyncio
async def test_portia_mcp_tool_async_call_with_complex_response() -> None
⋮----
query: str
⋮----
tool_result = await tool.arun(get_test_tool_context(), query="test query")
⋮----
@pytest.mark.asyncio
async def test_portia_mcp_tool_async_call_with_no_args() -> None
⋮----
class EmptyArgSchema(BaseModel)
⋮----
tool_result = await tool.arun(get_test_tool_context())
⋮----
def test_remote_tool_batch_ready_check(httpx_mock: HTTPXMock) -> None
⋮----
config = get_test_config()
mock_client = httpx.Client(base_url=endpoint)
⋮----
response = PortiaRemoteTool.batch_ready_check(
⋮----
request = httpx_mock.get_request(
⋮----
json_data = request.read().decode()
request_body = json.loads(json_data)
⋮----
def test_remote_tool_batch_ready_check_not_ready(httpx_mock: HTTPXMock) -> None
⋮----
clarification = ActionClarification(
⋮----
def test_remote_tool_batch_ready_check_404_fallback(httpx_mock: HTTPXMock) -> None
def test_structured_output_schema(add_tool: AdditionTool) -> None
⋮----
class AdditionOutput(BaseModel)
⋮----
result: int
class StructuredAdditionTool(AdditionTool)
⋮----
structured_output_schema: type[BaseModel] | None = AdditionOutput
def run(self, _: ToolRunContext, a: int, b: int) -> int
structured_add_tool = StructuredAdditionTool()
⋮----
output = structured_add_tool._run(get_test_tool_context(), a=1, b=2)
⋮----
def test_structured_output_schema_coercion(add_tool: AdditionTool) -> None
def test_structured_output_schema_coercion_error(add_tool: AdditionTool) -> None
</file>

<file path="tests/unit/test_version.py">
def test_get_version_installed_package() -> None
def test_get_version_from_pyproject_toml() -> None
def test_get_version_from_pyproject_toml_with_quotes() -> None
def test_get_version_from_pyproject_toml_no_version_line() -> None
def test_get_version_pyproject_toml_not_found() -> None
def test_get_version_file_read_error() -> None
def test_get_version_path_resolution() -> None
⋮----
result = get_version()
</file>

<file path="tests/conftest.py">
def pytest_sessionstart(session: pytest.Session) -> None
</file>

<file path="tests/utils.py">
plan = get_test_plan_run()[0]
⋮----
def get_test_plan_run() -> tuple[Plan, PlanRun]
⋮----
step1 = Step(
plan = Plan(
plan_run = PlanRun(plan_id=plan.id, current_step_index=0, end_user_id="test")
⋮----
def get_test_tool_call(plan_run: PlanRun) -> ToolCallRecord
def get_test_config(**kwargs) -> Config
class AdditionToolSchema(BaseModel)
⋮----
a: int = Field(..., description="The first number to add")
b: int = Field(..., description="The second number to add")
class AdditionTool(Tool)
⋮----
id: str = "add_tool"
name: str = "Add Tool"
description: str = "Use this tool to add two numbers together, it takes two numbers a + b"
args_schema: type[BaseModel] = AdditionToolSchema
output_schema: tuple[str, str] = ("int", "int: The value of the addition")
def run(self, _: ToolRunContext, a: int, b: int) -> int
class EndUserUpdateToolSchema(BaseModel)
⋮----
name: str | None = Field(default=None, description="The new name for the end user.")
class EndUserUpdateTool(Tool)
⋮----
id: str = "end_user_update"
name: str = "End User Update Tool"
description: str = "Updates the name of the end user"
args_schema: type[BaseModel] = EndUserUpdateToolSchema
output_schema: tuple[str, str] = ("str", "str: The new name")
def run(self, ctx: ToolRunContext, name: str) -> str
class ClarificationToolSchema(BaseModel)
⋮----
user_guidance: str = Field(..., description="The user guidance for the clarification")
class ClarificationTool(Tool)
⋮----
id: str = "clarification_tool"
name: str = "Clarification Tool"
description: str = "Returns a clarification"
args_schema: type[BaseModel] = ClarificationToolSchema
output_schema: tuple[str, str] = (
⋮----
class MockToolSchema(BaseModel)
class MockTool(Tool)
⋮----
name: str = "Mock Tool"
description: str = "do nothing"
args_schema: type[BaseModel] = MockToolSchema
output_schema: tuple[str, str] = ("None", "None: returns nothing")
⋮----
class ErrorToolSchema(BaseModel)
⋮----
error_str: str
return_soft_error: bool
return_uncaught_error: bool
class ErrorTool(Tool)
⋮----
id: str = "error_tool"
name: str = "Error Tool"
description: str = "Returns a error"
args_schema: type[BaseModel] = ErrorToolSchema
⋮----
class NoneTool(Tool)
⋮----
id: str = "none_tool"
name: str = "None Tool"
description: str = "returns None"
output_schema: tuple[str, str] = ("None", "None: nothing")
def run(self, _: ToolRunContext) -> None
class TestClarificationHandler(ClarificationHandler)
⋮----
received_clarification: Clarification | None = None
clarification_response: object = "Test"
⋮----
def reset(self) -> None
class MockMcpSessionWrapper
⋮----
def __init__(self, session: MagicMock, exit_error: Exception | None = None) -> None
⋮----
@asynccontextmanager
    async def mock_mcp_session(self, _: McpClientConfig) -> AsyncIterator[ClientSession]
async def __aenter__(self) -> ClientSession
async def __aexit__(self, *args: Any, **kwargs: Any) -> None
def __call__(self, *args: Any, **kwargs: Any) -> MockMcpSessionWrapper
⋮----
model = MagicMock(spec=BaseChatModel)
def invoke(*_: Any, **__: Any) -> BaseMessage
async def ainvoke(*_: Any, **__: Any) -> BaseMessage
def with_structured_output(_: BaseModel, *__: Any, **___: Any) -> BaseChatModel
def bind_tools(_: Sequence[BaseTool], *__: Any, **___: Any) -> BaseChatModel
⋮----
def get_mock_generative_model(response: Any = None) -> GenerativeModel
⋮----
dict1 = clarification1.model_dump()
dict2 = clarification2.model_dump()
</file>

<file path=".env.example">
OPENAI_API_KEY="sk-proj-YOUR_API_KEY"
PORTIA_API_KEY=""
ANTHROPIC_API_KEY=""
MISTRAL_API_KEY=""
GOOGLE_API_KEY=""
AZURE_OPENAI_API_KEY=""
AZURE_OPENAI_ENDPOINT=""
</file>

<file path=".gitignore">
dist
__pycache__
.coverage
.coverage.*
test_harness.py
htmlcov
docs
.venv
*.env
.vscode
.portia
scratch
.DS_Store
</file>

<file path=".pre-commit-config.yaml">
repos:
-   repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.8.0
    hooks:
      - id: ruff
        args: ["--fix"]
      - id: ruff-format
-   repo: local
    hooks:
      - id: pyright
        name: Pyright
        entry: uv run pyright
        language: system
        pass_filenames: false
        verbose: true
</file>

<file path="CODE_OF_CONDUCT.md">
# Contributor Covenant Code of Conduct

## Our Pledge

In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.

## Our Standards

Examples of behavior that contributes to creating a positive environment include:

* Using welcoming and inclusive language
* Being respectful of differing viewpoints and experiences
* Gracefully accepting constructive criticism
* Focusing on what is best for the community
* Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

* The use of sexualized language or imagery and unwelcome sexual attention or advances
* Trolling, insulting/derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or electronic address, without explicit permission
* Other conduct which could reasonably be considered inappropriate in a professional setting

## Our Responsibilities

Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.

## Scope

This Code of Conduct applies within all project spaces, and it also applies when an individual is representing the project or its community in public spaces.
Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team directly on complaints@portialabs.ai. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. 
Further details of specific enforcement policies may be posted separately.

Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq
</file>

<file path="CONTRIBUTING.md">
# Contributing to Portia SDK 🏗️

Thank you for your interest in contributing to Portia SDK! We welcome contributions that improve the library and help us build a great experience for the community.

## What to contribute
* **Paid issue contributions** We post paid contributions to our issues list for which we will remunerate contributions. Please see the guidelines [below](https://github.com/portiaAI/portia-sdk-python/blob/main/CONTRIBUTING.md#paid-contributions).
* **Documentation** Tutorials, how-to guides and revisions to our existing docs go a long way in making our repo easier to setup and use
* **Examples** Show our community what you can do with our SDK. We particularly encourage end-to-end, real-world applications
* **Bug reports** Please include a detailed method to reproduce any bugs you spot. We would be grateful if you give the [Issue Tracker](https://github.com/portiaAI/portia-sdk-python/issues) a quick skim first to avoid duplicates 🙌
* **Bug fixes** Those are our favourites! Please avoid breaking changes. The next section has some helpful tips for that.
* **Feedback** Help us be better. Come chat to us on [Discord](https://discord.gg/DvAJz9ffaR) about your experience using the SDK 🫶

⚠️ **A note on new features** If you have something in mind, please give us a shout on our Discord channel. Features like new core abstractions, changes to infra or to dependencies will require careful consideration before we can move forward with them.

## Paid contributions
Paid contributions are shown in the [Issue](https://github.com/portiaAI/portia-sdk-python/issues) list with the monetary amount shown as follows [£20]. If an issue does not include a monetary amount, it indicates that fixing it will not be remunerated (though we appreciate it greatly!). Please bear in mind the following rules for paid contributions:
* If you wish to work on a paid contribution you should comment on the issue indicating that you want to work on it and we'll assign it to you. You then have a week to submit the code for the issue or else it will be unassigned from you. If the code is not submitted in this time, and you are unassigned, you are not permitted to request that you work on that issue again.
* Contributors can have only a single assigned issue at a time.
* We expect paid contributions to require no more than 1 major review (i.e broader suggestions on direction), and 4 minor reviews. If more than this is required, the contribution will not be remunerated.
* Once your feature is ready for review, please email code-submission@portialabs.ai with a link to the PR and we will review it.
* If you submit code for an issue which you were not assigned to, you will not be remunerated.

### Getting paid
* Once you have contributed and submitted a paid contribution, please email code-submission@portialabs.ai including a link to the PR you made and a screenshot of the Github accounts profile page that authored the PR to prove your identiy. If you are using a different currency we will provide the remuneration at the local exchange rate.

## How to contribute

1. **Fork the Repository**: Start by forking the repository and cloning it locally.
2. **Create a Branch**: Create a branch for your feature or bug fix. Use a descriptive name for your branch (e.g., `fix-typo`, `add-feature-x`).
3. **Install the dependencies** We use uv to manage dependencies. Run ``uv sync --all-extras``
4. **Make Your Changes**: Implement your changes in small, focused commits. Be sure to follow our linting rules and style guide.
5. **Run Tests**: If your changes affect functionality, please test thoroughly 🌡️ Details on how run tests are in the **Tests** section below.
6. **Lint Your Code**: We use [ruff](https://github.com/charliermarsh/ruff) for linting. Please ensure your code passes all linting checks. We prefer per-line disables for rules rather than global ignores, and please leave comments explaining why you disable any rules.
7. **Open a Pull Request**: Once you're happy with your changes, open a pull request. Ensure that your PR description clearly explains the changes and the problem it addresses. The **Release** section below has some useful tips on this process.
8. **Code Review**: Your PR will be reviewed by the maintainers. They may suggest improvements or request changes. We will do our best to review your PRs promptly but we're still a tiny team with limited resource. Please bear with us 🙏
10. **Merge Your PR**: Once approved, the author of the PR can merge the changes. 🚀

## Linting

We lint our code using [Ruff](https://github.com/astral-sh/ruff). We also have [pre-commit](https://pre-commit.com/) setup to allow running this easily locally.

## Tests

We write two types of tests:
- Unit tests should mock out the LLM providers, and aim to give quick feedback. They should mock out LLM providers.
- Integration tests actually call LLM providers, are much slower but test the system works fully.

To run tests:
- Run all tests with `uv run pytest`.
- Run unit tests with `uv run pytest tests/unit`.
- Run integration tests with `uv run pytest tests/integration`.

We utilize [pytest-parallel](https://pypi.org/project/pytest-parallel/) to execute tests in parallel. You can add the `--workers=4` argument to the commands above to run in parallel. If you run into issues running this try setting `export NO_PROXY=true` first.

## Release

Releases are controlled via Github Actions and the version field of the `pyproject.toml`. To release:

1. Create a PR that updates the version field in the `pyproject.toml`.
2. Merge the PR to main.
3. Github Actions will create a new tag and push the new version to PyPi.

## Contributor License Agreement (CLA)

By submitting a pull request, you agree to sign our Contributor License Agreement (CLA), which ensures that contributions can be included in the project under the terms of our current [license](https://github.com/portiaAI/portia-sdk-python/edit/main/CONTRIBUTING.md#:~:text=CONTRIBUTING.md-,LICENSE,-Logo_Portia_Stacked_Black.png). We will ask you to sign this CLA when submitting your first contribution.

## Thank you

Thank you for contributing to Portia SDK Python!
</file>

<file path="example_builder.py">
class CommodityPriceWithCurrency(BaseModel)
⋮----
price: float
currency: str
class FinalOutput(BaseModel)
⋮----
poem: str
email_address: str
portia = Portia(execution_hooks=CLIExecutionHooks())
plan = (
plan_run = portia.run_plan(plan, plan_run_inputs={"purchase_quantity": 100})
</file>

<file path="example.py">
portia = Portia(
plan_run = portia.run(
⋮----
plan_run = portia.storage.get_plan_run(plan_run.id)
⋮----
new_value = "Sydney"
plan_run = portia.resolve_clarification(
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2025 Portia AI

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="pyproject.toml">
[project]
name = "portia-sdk-python"
version = "0.7.0"
description = "Portia Labs Python SDK for building agentic workflows."
authors = [{ name = "Hello", email = "hello@portialabs.ai" }]
requires-python = ">=3.11"
readme = "README.md"
license = { file = "LICENSE" }
keywords = [
    "LLM",
    "agentic",
    "workflow",
]
classifiers = ["Development Status :: 3 - Alpha"]
dependencies = [
    "pydantic>=2.11.5,<3",
    "jinja2>=3.1.4,<4",
    "instructor>=1.9.0,<2 ; python_version >= '3.11' and python_version < '4.0'",
    "anthropic>=0.41.0,<=0.55.0",
    # We fixate langchain-anthropic to this version because of browser-use version conflicts.
    # Once we upgrade browser-use, we should upgrade this dependency to latest.
    "langchain-anthropic==0.3.3 ; python_version >= '3.11' and python_version < '4.0'", #
    "langchain-core>=0.3.25,<0.4 ; python_version >= '3.11' and python_version < '4.0'",
    "langchain-openai>=0.3,<0.4 ; python_version >= '3.11' and python_version < '4.0'",
    "langchain>=0.3.17,<0.4 ; python_version >= '3.11' and python_version < '4.0'",
    "langgraph>=0.2.59,<0.3 ; python_version >= '3.11' and python_version < '4.0'",
    "click>=8.1.7,<9",
    "loguru>=0.7.3,<0.8 ; python_version >= '3.11' and python_version < '4.0'",
    "python-dotenv>=1.0.1,<2",
    "pandas>=2.2.3,<3",
    "pytest-mock>=3.14.0,<4",
    "openpyxl>=3.1.5,<4",
    "mcp>=1.9.2,<2",
    "langsmith>=0.3.15,<0.4 ; python_version >= '3.11' and python_version < '4.0'",
    "jsonref>=1.1.0,<2",
    "posthog~=6.0",
    "testcontainers[redis]>=4.10.0 ; python_version >= '3.9' and python_version < '4.0'",
    "instructor>=1.9.0 ; python_version >= '3.9' and python_version < '4.0'",
    "litellm>=1.74.9.post1",
    "openai<1.99"
]

[project.optional-dependencies]
mistral = [
    "langchain-mistralai>=0.2.3,<0.3 ; python_version >= '3.11' and python_version < '4.0'",
    "mistralai>=1.2.5,<2",
]
mistralai = [
    "langchain-mistralai>=0.2.3,<0.3 ; python_version >= '3.11' and python_version < '4.0'",
    "mistralai>=1.2.5,<2",
]
google = [
    "langchain-google-genai>=2.1.8,<3 ; python_version >= '3.11' and python_version < '4.0'",
    "google-genai>=1.18.0",
]
amazon = [
    "langchain-aws>=0.2.28 ; python_version >= '3.11' and python_version < '4.0'",
    "boto3>=1.39.9"
]
ollama = ["langchain-ollama>=0.2.2,<0.3 ; python_version >= '3.11' and python_version < '4.0'"]
tools-browser-local = [
    "playwright>=1.49.0,<2",
    "browser-use==0.1.40 ; python_version >= '3.11' and python_version < '4.0'",
]
tools-browser-browserbase = [
    "playwright>=1.49.0,<2",
    "browser-use==0.1.40 ; python_version >= '3.11' and python_version < '4.0'",
    "browserbase>=1.2.0,<2",
]
tools-pdf-reader = ["mistralai>=1.2.5,<2"]
cache = ["redis>=5.2.1,<6", "langchain-redis>=0.1.2" ]
all = [
    "langchain-mistralai>=0.2.3,<0.3 ; python_version >= '3.11' and python_version < '4.0'",
    "mistralai>=1.2.5,<2",
    "langchain-google-genai>=2.1.8,<3 ; python_version >= '3.11' and python_version < '4.0'",
    "google-genai>=1.18.0",
    "langchain-ollama>=0.2.2,<0.3 ; python_version >= '3.11' and python_version < '4.0'",
    "playwright>=1.49.0,<2",
    "browser-use==0.1.40 ; python_version >= '3.11' and python_version < '4.0'",
    "browserbase>=1.2.0,<2",
    "redis>=5.2.1,<6",
    "langchain-redis>=0.1.2 ; python_version >= '3.11' and python_version < '3.14'",
    "langchain-aws>=0.2.28 ; python_version >= '3.11' and python_version < '4.0'",
    "boto3>=1.39.9 ; python_version >= '3.9' and python_version < '4.0'"
]

[project.urls]
Homepage = "https://www.portialabs.ai/"
Repository = "https://github.com/portiaAI/portia-sdk-python"
Documentation = "https://docs.portialabs.ai"

[project.scripts]
portia-cli = "portia.cli:cli"

[dependency-groups]
dev = [
    "pre-commit>=4.2.0,<5",
    "ruff>=0.8.0,<0.9",
    "pytest>=8.3.3",
    "pytest-rerunfailures~=14.0",
    "pytest-cov>=5.0.0,<6",
    "pyright>=1.1.382,<2",
    "pytest-xdist[psutil]>=3.6.1,<4",
    "pytest-asyncio>=1.1.0",
    "pytest-httpx>=0.35.0",
]

[tool.hatch.build.targets.sdist]
include = ["portia"]

[tool.hatch.build.targets.wheel]
include = ["portia"]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.pyright]
ignore = ["scratch/*"]

[tool.ruff]
line-length=100

[tool.ruff.lint]
select = ["ALL"]
ignore = [
  "COM812",  # Disables checks for trailing commas as they are fixed by the formatted and running both is not recommended.
  "D203",    # Disables checks for having a blank line before a class docstring. We instead have no-blank-line-before-class (D211) enabled.
  "D213",    # Disables checks for multi-line docstrings not starting on the first line. We instead have multi-line-summary-first-line (D212) enabled.
  "EM101",   # Disables checks for missing exception message arguments. We prefer single-line exception statements for simplicity and terseness.
  "EM102",   # Disables checks for f-string usage in exception messages. We prefer single-line exception statements with f-strings for simplicity and terseness.
  "TRY003",  # Disables checks for long error messages. We prefer to provide as much context to users as possible but want to avoid a proliferation of error classes.
  "FBT001",  # Disables checks for unused type parameters. We prefer booleans for simple parameters rather than unnecessary enums.
  "FBT002",  # Disables checks for unused type parameters. We prefer booleans for simple parameters rather than unnecessary enums.
]

[tool.ruff.lint.per-file-ignores]
"**/tests/*" = [
  "S101",    # Disables check for asserts. Asserts in test cases can be useful.
  "PLR2004", # Disables magic number checks. Its normal to assert using magic numbers for things like array length.
  "INP001",  # Disables checks for implicit namespace packages. Tests are not part of the package.
  "SLF001",  # Disables checks for private member access. We call private methods in tests.
  "C901",    # Disables checks for too many lines in function. Tests are allowed to be longer.
]

[tool.ruff.lint.flake8-type-checking]
runtime-evaluated-base-classes = [
  "pydantic.BaseModel", # Tells ruff that BaseModel instances need to be evaluated at runtime.
]

[tool.ruff.lint.flake8-annotations]
allow-star-arg-any = true  # Allows **kwargs: Any in type signatures.

[tool.ruff.lint.pylint]
max-args = 10

[tool.setuptools.package-data]
portia = ["templates/**/*.jinja"]

[tool.pytest.ini_options]
filterwarnings = [
    "ignore:Failing to pass a value to the 'type_params' parameter of 'typing.ForwardRef._evaluate':DeprecationWarning",  # this comes from LangChain
    "ignore::DeprecationWarning:langchain_core.load.serializable",  # Pydantic via LangChain
]
addopts = [
  "--cov",
  "--cov-report=term-missing",
  "--cov-report=html",
  "--import-mode=importlib"
]
asyncio_mode = "auto"
markers = [
    "daily: marks tests as expensive daily tests (deselect with '-m \"not daily\"')",
]

[tool.coverage.run]
omit = [
    "*/tests/*", # Don't cover test files themselves
    "example.py", # Don't cover example
    "*/_unstable/**",  # Don't check _unstable files
    "portia/cli.py",  # Best effort test coverage
    "portia/portia.py", # TODO: Remove this once testing has been added for new PlanV2
    "portia/builder/step_v2.py", # TODO: Remove this once testing has been added for string templating
    "portia/builder/plan_builder_v2.py", # TODO: Remove this once testing has been added for string templating
    "portia/execution_agents/conditional_evaluation_agent.py", # TODO: Remove after tests added
]

[tool.coverage.report]
exclude_lines = [
    "if TYPE_CHECKING:",
    "pragma: no cover",
]

[[tool.pydoc-markdown.loaders]]
type = "python"

[[tool.pydoc-markdown.processors]]
type = "filter"
expression = "not 'test' in name and not 'cli' in name and not 'prefixed_uuid' in name and not 'common' in name and not 'templates' in name and not '_unstable' in name and default()"
skip_empty_modules = true

[[tool.pydoc-markdown.processors]]
type = "smart"

[[tool.pydoc-markdown.processors]]
type = "crossref"

[tool.pydoc-markdown.renderer]
type = "docusaurus"
relative_output_path="SDK"
sidebar_top_level_label=""

[tool.licensecheck]
using = "uv"
ignore_packages = [
  "mistralai", # MistralAI is Apache 2.0 licensed: https://github.com/mistralai/client-python?tab=Apache-2.0-1-ov-file
]
</file>

<file path="README.md">
<p align="center">
    <a href="https://docs.portialabs.ai/steel-thread-intro">
       <img alt="SteelThread banner" src="assets/steelthread_banner.png" width="75%">
    </a></br>
<strong>Our flexible evaluation framework is out!</strong></br>
  🧪 Ingest test cases directly from your agent runs rather than building datasets from scratch.</br>
  🛰️ Monitor in real-time or with offline evals.</br>
  📊 Use custom metric definitions including both deterministic and LLM-based judging.</br></br>
</p>

---

<p align="center">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="assets/Logo_Portia_Line_White.png">
    <source media="(prefers-color-scheme: light)" srcset="assets/Logo_Portia_Line_Black.png">
    <img alt="Portia AI logo" src="https://raw.githubusercontent.com/portiaAI/portia-sdk-python/main/assets/Logo_Portia_Line_Black.png"  width="50%">
  </picture>
</p>
<p align="center">
  <a href="https://github.com/mahseema/awesome-ai-tools">
    <img src="https://awesome.re/mentioned-badge-flat.svg" alt="Mentioned in Awesome">
  </a>
</p>

<p align="center">
  <a href="https://www.youtube.com/@PortiaAI">
    <img src="https://img.shields.io/badge/YouTube-FF0000?logo=youtube&logoColor=white">
  </a>
  <a href="https://discord.gg/DvAJz9ffaR">
    <img src="https://img.shields.io/badge/Discord-5865F2?logo=discord&logoColor=white">
  </a>
  <a href="https://x.com/RealPortiaAI">
    <img src="https://img.shields.io/badge/  X  -000000?logo=twitter&logoColor=white">
  </a>
  <a href="https://www.reddit.com/r/PortiaAI/">
    <img src="https://img.shields.io/badge/Reddit-FF4500?logo=reddit&logoColor=white">
  </a>
  <a href="https://www.linkedin.com/company/portiaai">
    <img src="https://img.shields.io/badge/LinkedIn-0A66C2?logo=linkedin&logoColor=white">
  </a>
  <br>
  <a href="https://app.portialabs.ai">
    <img src="https://img.shields.io/badge/Dashboard-059669">
  </a>
  <a href="https://docs.portialabs.ai">
    <img src="https://img.shields.io/badge/Docs-38BDF8">
  </a>
  <a href="https://blog.portialabs.ai">
    <img src="https://img.shields.io/badge/Blog-2D3748">
  </a>
</p>

<p align="center">
  <a href="https://www.producthunt.com/products/portia-ai?embed=true&utm_source=badge-top-post-badge&utm_medium=badge&utm_source=badge-portia&#0045;ai&#0045;2" target="_blank">
    <img src="https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=983741&theme=light&period=daily&t=1751531459759" alt="Portia&#0032;AI - Secure&#0032;AI&#0032;agents&#0032;with&#0032;tools&#0044;&#0032;auth&#0044;&#0032;and&#0032;smart&#0032;control | Product Hunt" style="width: 200px; height: 43.2px;" width="200" height="43.2"/>
  </a>
</p>

# Portia SDK Python

Portia AI is an open source developer framework for predictable, stateful, authenticated agentic workflows. We allow developers to have as much or as little oversight as they’d like over their multi-agent deployments and we are obsessively focused on production readiness.
Play around, break things and tell us how you're getting on in our <a href="https://discord.gg/DvAJz9ffaR" target="_blank">**Discord channel (↗)**</a>. Most importantly please be kind to your fellow humans (<a href="https://github.com/portiaAI/portia-sdk-python/blob/main/CODE_OF_CONDUCT.md" target="_blank" rel="noopener noreferrer">**Code of Conduct (↗)**</a>).

If you want to dive straight in with an example, dive into one of our examples in our **[Examples Repo (↗)](https://github.com/portiaAI/portia-agent-examples)**.

## Key features

**Iterate on agents’ reasoning and intervene in their execution**</br>
🧠 Create your multi-agent [`Plan`](https://docs.portialabs.ai/generate-plan) conversationally, or build them with our [`PlanBuilder`](https://docs.portialabs.ai/generate-plan#build-a-plan-manually).</br>
📝 Enrich a [`PlanRunState`](https://docs.portialabs.ai/run-plan) during execution to track progress.</br>
🚧 Define inputs and output structures for enhanced predictability.</br>
✋🏼 Add deterministic tasks through an [`ExecutionHook`](https://docs.portialabs.ai/execution-hooks). Use a [`clarification`](https://docs.portialabs.ai/understand-clarifications) for human:agent interactions.</br>

**Extensive tool support including MCP support**</br>
🔧 Connect [tool registries](https://docs.portialabs.ai/extend-run-tools) from any MCP server, local tools or another AI tool provider (e.g. ACI.dev).</br>
🫆 Leverage Portia cloud's prebuilt [1000+ cloud and MCP tools](https://docs.portialabs.ai/cloud-tool-registry) with out-of-the-box authentication.</br>
🌐 Navigate the web and cope with captchas and logins using our [open source browser tool](https://docs.portialabs.ai/browser-tools).</br>

**Authentication for API and web agents**</br>
🔑 Handle user credentials seamlessly, for both API tools and browser sessions, with our `clarification` interface.</br>

**Production ready**</br>
👤 Attribute multi-agent runs and auth at an [`EndUser`](https://docs.portialabs.ai/manage-end-users) level.</br>
💾 Large inputs and outputs are automatically stored / retrieved in [Agent memory](https://docs.portialabs.ai/agent-memory) at runtime.</br>
🔗 Connect [any LLM](https://docs.portialabs.ai/manage-config#configure-llm-options) including local ones, and use your own Redis server for [caching](https://docs.portialabs.ai/manage-config#manage-caching).</br>

<p align="center"><strong>🌟 Star Portia AI to stay updated on new releases!</strong></p>

## Demo
To clickthrough at your own pace, please follow this [link](https://snappify.com/view/3d721d6c-c5ff-4e84-b770-83e93bd1a8f1)</br>
![Feature run-through](https://github.com/user-attachments/assets/1cd66940-ee78-42a6-beb4-7533835de7e9)

## Quickstart

### Installation in 3 quick steps

Ensure you have python 3.11 or higher installed using `python --version`. If you need to update your python version please visit their [docs](https://www.python.org/downloads/). Note that the example below uses OpenAI but we support other models as well. For instructions on linking other models, refer to our [docs](https://docs.portialabs.ai/manage-config).</br>

**Step 1:** Install the Portia Python SDK
```bash
pip install portia-sdk-python 
```

**Step 2:** Ensure you have an LLM API key set up
```bash
export OPENAI_API_KEY='your-api-key-here'
```
**Step 3:** Validate your installation by submitting a simple maths prompt from the command line
```
portia-cli run "add 1 + 2"
```

**All set? Now let's explore some basic usage of the product 🚀**

### E2E example
You will need a Portia API key* for this one because we use one of our cloud tools to schedule a calendar event and send an email. 
<br>**🙏🏼 *We have a free tier so you do not need to share payment details to get started 🙏🏼.**<br>
Head over to <a href="https://app.portialabs.ai" target="_blank">**app.portialabs.ai (↗)**</a> and get your Portia API key. You will then need to set it as the env variable `PORTIA_API_KEY`.<br/>

The example below introduces **some** of the config options available with Portia AI (check out our <a href="https://docs.portialabs.ai/manage-config" target="_blank">**docs (↗)**</a> for more):
- The `storage_class` is set using the `StorageClass.CLOUD` ENUM. So long as your `PORTIA_API_KEY` is set, runs and tool calls will be logged and appear automatically in your Portia dashboard at <a href="https://app.portialabs.ai" target="_blank">**app.portialabs.ai (↗)**</a>.
- The `default_log_level` is set using the `LogLevel.DEBUG` ENUM to `DEBUG` so you can get some insight into the sausage factory in your terminal, including plan generation, run states, tool calls and outputs at every step 😅
- The `llm_provider` and `xxx_api_key` (varies depending on model provider chosen) are used to choose the specific LLM provider. In the example below we're using GPT 4o, but you can use Anthropic, Gemini and others!

Finally we also introduce the concept of a `tool_registry`, which is a flexible grouping of tools.

```python
from dotenv import load_dotenv
from portia import Config, Portia, DefaultToolRegistry
from portia.cli import CLIExecutionHooks

load_dotenv(override=True)

recipient_email = input("Please enter the email address of the person you want to schedule a meeting with:\n")
task = f"""
Please help me accomplish the following tasks:
- Get my availability from Google Calendar tomorrow between 8:00 and 8:30
- If I am available, schedule a 30 minute meeting with {recipient_email} at a time that works for me with the title "Portia AI Demo" and a description of the meeting as "Test demo".
"""

config = Config.from_default()
portia = Portia(
   config=config,
   tools=DefaultToolRegistry(config=config),
   execution_hooks=CLIExecutionHooks(),
)

plan = portia.run(task)
```

### Advanced examples on YouTube
Here is an example where we build a customer refund agent using Stripe's MCP server. It leverages execution hooks and clarifications to confirm human approval before moving money.</br>
[![Customer refund agent with Stripe MCP](assets/stripemcp.jpg)](https://youtu.be/DB-FDEM_7_Y?si=IqVq14eskvLIKmvv)

Here is another example where we use our open browser tool. It uses clarifications when it encounters a login page to allow a human to enter their credentials directly into the session and allow it to progress.</br>
[![Manage Linkedin connections](assets/linkedinbrowsertool.jpg)](https://youtu.be/hSq8Ww-hagg?si=8oQaXcTcAyrzEQty)

## Learn more
- Head over to our docs at <a href="https://docs.portialabs.ai" target="_blank">**docs.portialabs.ai (↗)**</a>.
- Join the conversation on our <a href="https://discord.gg/DvAJz9ffaR" target="_blank">**Discord channel (↗)**</a>.
- Watch us embarrass ourselves on our <a href="https://www.youtube.com/@PortiaAI" target="_blank">**YouTube channel (↗)**</a>.
- Follow us on <a href="https://www.producthunt.com/posts/portia-ai" target="_blank">**Product Hunt (↗)**</a>.

## Paid contributions & contribution guidelines
Head on over to our <a href="https://github.com/portiaAI/portia-sdk-python/blob/main/CONTRIBUTING.md" target="_blank">**contribution guide (↗)**</a> for details.

Portia offers a **PAID** contribution program by fixing issues on our 'Issues' list. You can read all about this in the <a href="https://github.com/portiaAI/portia-sdk-python/blob/main/CONTRIBUTING.md" target="_blank">**contribution guide (↗)**</a>.

# ⭐ Support
You can support our work best by leaving a star!
![star](https://github.com/user-attachments/assets/8df5e1d9-a0d4-40b4-9c51-945841744050)

We love feedback and suggestions. Please join our <a href="https://discord.gg/DvAJz9ffaR" target="_blank">**Discord channel (↗)**</a> to chat with us.
</file>

</files>
